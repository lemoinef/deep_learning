{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Anna KaRNNa\n",
    "\n",
    "In this notebook, I'll build a character-wise RNN trained on Anna Karenina, one of my all-time favorite books. It'll be able to generate new text based on the text from the book.\n",
    "\n",
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). Also, some information [here at r2rt](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html) and from [Sherjil Ozair](https://github.com/sherjilozair/char-rnn-tensorflow) on GitHub. Below is the general architecture of the character-wise RNN.\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First we'll load the text file and convert it into integers for our network to use. Here I'm creating a couple dictionaries to convert the characters to and from integers. Encoding the characters as integers makes it easier to use as input in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = set(text)\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "chars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's check out the first 100 characters, make sure everything is peachy. According to the [American Book Review](http://americanbookreview.org/100bestlines.asp), this is the 6th best first line of a book ever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "And we can see the characters encoded as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([49,  0,  1, 35, 19, 12, 13, 25, 62, 81, 81, 81, 24,  1, 35, 35, 40,\n",
       "       25, 39,  1, 50, 38, 75, 38, 12, 17, 25,  1, 13, 12, 25,  1, 75, 75,\n",
       "       25,  1, 75, 38,  5, 12, 65, 25, 12, 78, 12, 13, 40, 25, 72, 73,  0,\n",
       "        1, 35, 35, 40, 25, 39,  1, 50, 38, 75, 40, 25, 38, 17, 25, 72, 73,\n",
       "        0,  1, 35, 35, 40, 25, 38, 73, 25, 38, 19, 17, 25, 60,  6, 73, 81,\n",
       "        6,  1, 40, 68, 81, 81, 21, 78, 12, 13, 40, 19,  0, 38, 73], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Since the network is working with individual characters, it's similar to a classification problem in which we are trying to predict the next character from the previous text.  Here's how many 'classes' our network has to pick from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(chars)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Making training and validation batches\n",
    "\n",
    "Now I need to split up the data into batches, and into training and validation sets. I should be making a test set here, but I'm not going to worry about that. My test will be if the network can generate new text.\n",
    "\n",
    "Here I'll make both input and target arrays. The targets are the same as the inputs, except shifted one character over. I'll also drop the last bit of data so that I'll only have completely full batches.\n",
    "\n",
    "The idea here is to make a 2D matrix where the number of rows is equal to the batch size. Each row will be one long concatenated string from the character data. We'll split this data into a training set and validation set using the `split_frac` keyword. This will keep 90% of the batches in the training set, the other 10% in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def split_data(chars, batch_size, num_steps, split_frac=0.9):\n",
    "    \"\"\" \n",
    "    Split character data into training and validation sets, inputs and targets for each set.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    chars: character array\n",
    "    batch_size: Size of examples in each of batch\n",
    "    num_steps: Number of sequence steps to keep in the input and pass to the network\n",
    "    split_frac: Fraction of batches to keep in the training set\n",
    "    \n",
    "    \n",
    "    Returns train_x, train_y, val_x, val_y\n",
    "    \"\"\"\n",
    "    \n",
    "    slice_size = batch_size * num_steps\n",
    "    n_batches = int(len(chars) / slice_size)\n",
    "    \n",
    "    # Drop the last few characters to make only full batches\n",
    "    x = chars[: n_batches*slice_size]\n",
    "    y = chars[1: n_batches*slice_size + 1]\n",
    "    \n",
    "    # Split the data into batch_size slices, then stack them into a 2D matrix \n",
    "    x = np.stack(np.split(x, batch_size))\n",
    "    y = np.stack(np.split(y, batch_size))\n",
    "    \n",
    "    # Now x and y are arrays with dimensions batch_size x n_batches*num_steps\n",
    "    \n",
    "    # Split into training and validation sets, keep the first split_frac batches for training\n",
    "    split_idx = int(n_batches*split_frac)\n",
    "    train_x, train_y= x[:, :split_idx*num_steps], y[:, :split_idx*num_steps]\n",
    "    val_x, val_y = x[:, split_idx*num_steps:], y[:, split_idx*num_steps:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now I'll make my data sets and we can check out what's going on here. Here I'm going to use a batch size of 10 and 50 sequence steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_x, train_y, val_x, val_y = split_data(chars, 10, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 178650)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Looking at the size of this array, we see that we have rows equal to the batch size. When we want to get a batch out of here, we can grab a subset of this array that contains all the rows but has a width equal to the number of steps in the sequence. The first batch looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[49,  0,  1, 35, 19, 12, 13, 25, 62, 81, 81, 81, 24,  1, 35, 35, 40,\n",
       "        25, 39,  1, 50, 38, 75, 38, 12, 17, 25,  1, 13, 12, 25,  1, 75, 75,\n",
       "        25,  1, 75, 38,  5, 12, 65, 25, 12, 78, 12, 13, 40, 25, 72, 73],\n",
       "       [25,  1, 50, 25, 73, 60, 19, 25, 44, 60, 38, 73, 44, 25, 19, 60, 25,\n",
       "        17, 19,  1, 40, 53, 41, 25,  1, 73, 17,  6, 12, 13, 12, 51, 25, 74,\n",
       "        73, 73,  1, 53, 25, 17, 50, 38, 75, 38, 73, 44, 53, 25, 82, 72],\n",
       "       [78, 38, 73, 68, 81, 81, 41,  9, 12, 17, 53, 25, 38, 19, 56, 17, 25,\n",
       "        17, 12, 19, 19, 75, 12, 51, 68, 25, 59,  0, 12, 25, 35, 13, 38, 58,\n",
       "        12, 25, 38, 17, 25, 50,  1, 44, 73, 38, 39, 38, 58, 12, 73, 19],\n",
       "       [73, 25, 51, 72, 13, 38, 73, 44, 25,  0, 38, 17, 25, 58, 60, 73, 78,\n",
       "        12, 13, 17,  1, 19, 38, 60, 73, 25,  6, 38, 19,  0, 25,  0, 38, 17,\n",
       "        81, 82, 13, 60, 19,  0, 12, 13, 25,  6,  1, 17, 25, 19,  0, 38],\n",
       "       [25, 38, 19, 25, 38, 17, 53, 25, 17, 38, 13, 42, 41, 25, 17,  1, 38,\n",
       "        51, 25, 19,  0, 12, 25, 60, 75, 51, 25, 50,  1, 73, 53, 25, 44, 12,\n",
       "        19, 19, 38, 73, 44, 25, 72, 35, 53, 25,  1, 73, 51, 81, 58, 13],\n",
       "       [25, 31, 19, 25,  6,  1, 17, 81, 60, 73, 75, 40, 25,  6,  0, 12, 73,\n",
       "        25, 19,  0, 12, 25, 17,  1, 50, 12, 25, 12, 78, 12, 73, 38, 73, 44,\n",
       "        25,  0, 12, 25, 58,  1, 50, 12, 25, 19, 60, 25, 19,  0, 12, 38],\n",
       "       [ 0, 12, 73, 25, 58, 60, 50, 12, 25, 39, 60, 13, 25, 50, 12, 53, 41,\n",
       "        25, 17,  0, 12, 25, 17,  1, 38, 51, 53, 25,  1, 73, 51, 25,  6, 12,\n",
       "        73, 19, 25, 82,  1, 58,  5, 25, 38, 73, 19, 60, 25, 19,  0, 12],\n",
       "       [65, 25, 82, 72, 19, 25, 73, 60,  6, 25, 17,  0, 12, 25,  6, 60, 72,\n",
       "        75, 51, 25, 13, 12,  1, 51, 38, 75, 40, 25,  0,  1, 78, 12, 25, 17,\n",
       "         1, 58, 13, 38, 39, 38, 58, 12, 51, 53, 25, 73, 60, 19, 25, 50],\n",
       "       [19, 25, 38, 17, 73, 56, 19, 68, 25, 59,  0, 12, 40, 56, 13, 12, 25,\n",
       "        35, 13, 60, 35, 13, 38, 12, 19, 60, 13, 17, 25, 60, 39, 25,  1, 25,\n",
       "        17, 60, 13, 19, 53, 81, 82, 72, 19, 25,  6, 12, 56, 13, 12, 25],\n",
       "       [25, 17,  1, 38, 51, 25, 19, 60, 25,  0, 12, 13, 17, 12, 75, 39, 53,\n",
       "        25,  1, 73, 51, 25, 82, 12, 44,  1, 73, 25,  1, 44,  1, 38, 73, 25,\n",
       "        39, 13, 60, 50, 25, 19,  0, 12, 25, 82, 12, 44, 38, 73, 73, 38]], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[:,:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "I'll write another function to grab batches out of the arrays made by `split_data`. Here each batch will be a sliding window on these arrays with size `batch_size X num_steps`. For example, if we want our network to train on a sequence of 100 characters, `num_steps = 100`. For the next batch, we'll shift this window the next sequence of `num_steps` characters. In this way we can feed batches to the network and the cell states will continue through on each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_batch(arrs, num_steps):\n",
    "    batch_size, slice_size = arrs[0].shape\n",
    "    \n",
    "    n_batches = int(slice_size/num_steps)\n",
    "    for b in range(n_batches):\n",
    "        yield [x[:, b*num_steps: (b+1)*num_steps] for x in arrs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Building the model\n",
    "\n",
    "Below is a function where I build the graph for the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(num_classes, batch_size=50, num_steps=50, lstm_size=128, num_layers=2,\n",
    "              learning_rate=0.001, grad_clip=5, sampling=False):\n",
    "    \n",
    "    # When we're using this network for sampling later, we'll be passing in\n",
    "    # one character at a time, so providing an option for that\n",
    "    if sampling == True:\n",
    "        batch_size, num_steps = 1, 1\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "    \n",
    "    # Keep probability placeholder for drop out layers\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    # One-hot encoding the input and target characters\n",
    "    x_one_hot = tf.one_hot(inputs, num_classes)\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "\n",
    "    ### Build the RNN layers\n",
    "    # Use a basic LSTM cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "    # Add dropout to the cell\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    ### Run the data through the RNN layers\n",
    "    # This makes a list where each element is on step in the sequence\n",
    "    rnn_inputs = [tf.squeeze(i, squeeze_dims=[1]) for i in tf.split(x_one_hot, num_steps, 1)]\n",
    "    \n",
    "    # Run each sequence step through the RNN and collect the outputs\n",
    "    outputs, state = tf.contrib.rnn.static_rnn(cell, rnn_inputs, initial_state=initial_state)\n",
    "    final_state = state\n",
    "    \n",
    "    # Reshape output so it's a bunch of rows, one output row for each step for each batch\n",
    "    seq_output = tf.concat(outputs, axis=1)\n",
    "    output = tf.reshape(seq_output, [-1, lstm_size])\n",
    "    \n",
    "    # Now connect the RNN outputs to a softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(num_classes))\n",
    "    \n",
    "    # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "    # of rows of logit outputs, one for each step and batch\n",
    "    logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "    \n",
    "    # Use softmax to get the probabilities for predicted characters\n",
    "    preds = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    # Reshape the targets to match the logits\n",
    "    y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "\n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    # Export the nodes\n",
    "    # NOTE: I'm using a namedtuple here because I think they are cool\n",
    "    export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
    "                    'keep_prob', 'cost', 'preds', 'optimizer']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here I'm defining the hyperparameters for the network. \n",
    "\n",
    "* `batch_size` - Number of sequences running through the network in one pass.\n",
    "* `num_steps` - Number of characters in the sequence the network is trained on. Larger is better typically, the network will learn more long range dependencies. But it takes longer to train. 100 is typically a good number here.\n",
    "* `lstm_size` - The number of units in the hidden layers.\n",
    "* `num_layers` - Number of hidden LSTM layers to use\n",
    "* `learning_rate` - Learning rate for training\n",
    "* `keep_prob` - The dropout keep probability when training. If you're network is overfitting, try decreasing this.\n",
    "\n",
    "Here's some good advice from Andrej Karpathy on training the network. I'm going to write it in here for your benefit, but also link to [where it originally came from](https://github.com/karpathy/char-rnn#tips-and-tricks).\n",
    "\n",
    "> ## Tips and Tricks\n",
    "\n",
    ">### Monitoring Validation Loss vs. Training Loss\n",
    ">If you're somewhat new to Machine Learning or Neural Networks it can take a bit of expertise to get good models. The most important quantity to keep track of is the difference between your training loss (printed during training) and the validation loss (printed once in a while when the RNN is run on the validation data (by default every 1000 iterations)). In particular:\n",
    "\n",
    "> - If your training loss is much lower than validation loss then this means the network might be **overfitting**. Solutions to this are to decrease your network size, or to increase dropout. For example you could try dropout of 0.5 and so on.\n",
    "> - If your training/validation loss are about equal then your model is **underfitting**. Increase the size of your model (either number of layers or the raw number of neurons per layer)\n",
    "\n",
    "> ### Approximate number of parameters\n",
    "\n",
    "> The two most important parameters that control the model are `lstm_size` and `num_layers`. I would advise that you always use `num_layers` of either 2/3. The `lstm_size` can be adjusted based on how much data you have. The two important quantities to keep track of here are:\n",
    "\n",
    "> - The number of parameters in your model. This is printed when you start training.\n",
    "> - The size of your dataset. 1MB file is approximately 1 million characters.\n",
    "\n",
    ">These two should be about the same order of magnitude. It's a little tricky to tell. Here are some examples:\n",
    "\n",
    "> - I have a 100MB dataset and I'm using the default parameter settings (which currently print 150K parameters). My data size is significantly larger (100 mil >> 0.15 mil), so I expect to heavily underfit. I am thinking I can comfortably afford to make `lstm_size` larger.\n",
    "> - I have a 10MB dataset and running a 10 million parameter model. I'm slightly nervous and I'm carefully monitoring my validation loss. If it's larger than my training loss then I may want to try to increase dropout a bit and see if that helps the validation loss.\n",
    "\n",
    "> ### Best models strategy\n",
    "\n",
    ">The winning strategy to obtaining very good models (if you have the compute time) is to always err on making the network larger (as large as you're willing to wait for it to compute) and then try different dropout values (between 0,1). Whatever model has the best validation performance (the loss, written in the checkpoint filename, low is good) is the one you should use in the end.\n",
    "\n",
    ">It is very common in deep learning to run many different models with many different hyperparameter settings, and in the end take whatever checkpoint gave the best validation performance.\n",
    "\n",
    ">By the way, the size of your training and validation splits are also parameters. Make sure you have a decent amount of data in your validation set or otherwise the validation performance will be noisy and not very informative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_steps = 100 \n",
    "lstm_size = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "keep_prob = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training\n",
    "\n",
    "Time for training which is pretty straightforward. Here I pass in some data, and get an LSTM state back. Then I pass that state back in to the network so the next batch can continue the state from the previous batch. And every so often (set by `save_every_n`) I calculate the validation loss and save a checkpoint.\n",
    "\n",
    "Here I'm saving checkpoints with the format\n",
    "\n",
    "`i{iteration number}_l{# hidden layer units}_v{validation loss}.ckpt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20  Iteration 1/3560 Training loss: 4.4141 108.5775 sec/batch\n",
      "Epoch 1/20  Iteration 2/3560 Training loss: 4.3654 37.9067 sec/batch\n",
      "Epoch 1/20  Iteration 3/3560 Training loss: 4.1658 28.0740 sec/batch\n",
      "Epoch 1/20  Iteration 4/3560 Training loss: 4.3214 19.0596 sec/batch\n",
      "Epoch 1/20  Iteration 5/3560 Training loss: 4.2428 19.4030 sec/batch\n",
      "Epoch 1/20  Iteration 6/3560 Training loss: 4.1778 17.7936 sec/batch\n",
      "Epoch 1/20  Iteration 7/3560 Training loss: 4.1015 19.6730 sec/batch\n",
      "Epoch 1/20  Iteration 8/3560 Training loss: 4.0270 21.1648 sec/batch\n",
      "Epoch 1/20  Iteration 9/3560 Training loss: 3.9606 19.0740 sec/batch\n",
      "Epoch 1/20  Iteration 10/3560 Training loss: 3.9060 19.1302 sec/batch\n",
      "Epoch 1/20  Iteration 11/3560 Training loss: 3.8563 19.3620 sec/batch\n",
      "Epoch 1/20  Iteration 12/3560 Training loss: 3.8149 22.6945 sec/batch\n",
      "Epoch 1/20  Iteration 13/3560 Training loss: 3.7785 18.9435 sec/batch\n",
      "Epoch 1/20  Iteration 14/3560 Training loss: 3.7474 18.3009 sec/batch\n",
      "Epoch 1/20  Iteration 15/3560 Training loss: 3.7179 18.3902 sec/batch\n",
      "Epoch 1/20  Iteration 16/3560 Training loss: 3.6916 18.8212 sec/batch\n",
      "Epoch 1/20  Iteration 17/3560 Training loss: 3.6669 19.9664 sec/batch\n",
      "Epoch 1/20  Iteration 18/3560 Training loss: 3.6463 20.3520 sec/batch\n",
      "Epoch 1/20  Iteration 19/3560 Training loss: 3.6270 20.0435 sec/batch\n",
      "Epoch 1/20  Iteration 20/3560 Training loss: 3.6069 18.8225 sec/batch\n",
      "Epoch 1/20  Iteration 21/3560 Training loss: 3.5902 18.5590 sec/batch\n",
      "Epoch 1/20  Iteration 22/3560 Training loss: 3.5744 19.1245 sec/batch\n",
      "Epoch 1/20  Iteration 23/3560 Training loss: 3.5596 18.5658 sec/batch\n",
      "Epoch 1/20  Iteration 24/3560 Training loss: 3.5456 19.5358 sec/batch\n",
      "Epoch 1/20  Iteration 25/3560 Training loss: 3.5325 19.8490 sec/batch\n",
      "Epoch 1/20  Iteration 26/3560 Training loss: 3.5207 18.4524 sec/batch\n",
      "Epoch 1/20  Iteration 27/3560 Training loss: 3.5100 17.9548 sec/batch\n",
      "Epoch 1/20  Iteration 28/3560 Training loss: 3.4989 18.4576 sec/batch\n",
      "Epoch 1/20  Iteration 29/3560 Training loss: 3.4888 18.3573 sec/batch\n",
      "Epoch 1/20  Iteration 30/3560 Training loss: 3.4795 18.4478 sec/batch\n",
      "Epoch 1/20  Iteration 31/3560 Training loss: 3.4712 18.2563 sec/batch\n",
      "Epoch 1/20  Iteration 32/3560 Training loss: 3.4624 17.6468 sec/batch\n",
      "Epoch 1/20  Iteration 33/3560 Training loss: 3.4539 18.4701 sec/batch\n",
      "Epoch 1/20  Iteration 34/3560 Training loss: 3.4464 18.9049 sec/batch\n",
      "Epoch 1/20  Iteration 35/3560 Training loss: 3.4388 18.9296 sec/batch\n",
      "Epoch 1/20  Iteration 36/3560 Training loss: 3.4320 18.4269 sec/batch\n",
      "Epoch 1/20  Iteration 37/3560 Training loss: 3.4247 18.2182 sec/batch\n",
      "Epoch 1/20  Iteration 38/3560 Training loss: 3.4180 18.0733 sec/batch\n",
      "Epoch 1/20  Iteration 39/3560 Training loss: 3.4115 19.5267 sec/batch\n",
      "Epoch 1/20  Iteration 40/3560 Training loss: 3.4055 20.1156 sec/batch\n",
      "Epoch 1/20  Iteration 41/3560 Training loss: 3.3993 18.7399 sec/batch\n",
      "Epoch 1/20  Iteration 42/3560 Training loss: 3.3937 18.5604 sec/batch\n",
      "Epoch 1/20  Iteration 43/3560 Training loss: 3.3882 18.4907 sec/batch\n",
      "Epoch 1/20  Iteration 44/3560 Training loss: 3.3830 18.3987 sec/batch\n",
      "Epoch 1/20  Iteration 45/3560 Training loss: 3.3778 18.2993 sec/batch\n",
      "Epoch 1/20  Iteration 46/3560 Training loss: 3.3732 17.9857 sec/batch\n",
      "Epoch 1/20  Iteration 47/3560 Training loss: 3.3689 18.4955 sec/batch\n",
      "Epoch 1/20  Iteration 48/3560 Training loss: 3.3646 18.8604 sec/batch\n",
      "Epoch 1/20  Iteration 49/3560 Training loss: 3.3606 18.7386 sec/batch\n",
      "Epoch 1/20  Iteration 50/3560 Training loss: 3.3566 18.8064 sec/batch\n",
      "Epoch 1/20  Iteration 51/3560 Training loss: 3.3525 19.0131 sec/batch\n",
      "Epoch 1/20  Iteration 52/3560 Training loss: 3.3485 18.5476 sec/batch\n",
      "Epoch 1/20  Iteration 53/3560 Training loss: 3.3448 18.5331 sec/batch\n",
      "Epoch 1/20  Iteration 54/3560 Training loss: 3.3410 18.8176 sec/batch\n",
      "Epoch 1/20  Iteration 55/3560 Training loss: 3.3375 20.6718 sec/batch\n",
      "Epoch 1/20  Iteration 56/3560 Training loss: 3.3338 19.2134 sec/batch\n",
      "Epoch 1/20  Iteration 57/3560 Training loss: 3.3303 19.0317 sec/batch\n",
      "Epoch 1/20  Iteration 58/3560 Training loss: 3.3271 18.4951 sec/batch\n",
      "Epoch 1/20  Iteration 59/3560 Training loss: 3.3236 18.6740 sec/batch\n",
      "Epoch 1/20  Iteration 60/3560 Training loss: 3.3206 18.5897 sec/batch\n",
      "Epoch 1/20  Iteration 61/3560 Training loss: 3.3176 19.4509 sec/batch\n",
      "Epoch 1/20  Iteration 62/3560 Training loss: 3.3150 18.9838 sec/batch\n",
      "Epoch 1/20  Iteration 63/3560 Training loss: 3.3126 19.8317 sec/batch\n",
      "Epoch 1/20  Iteration 64/3560 Training loss: 3.3096 18.7908 sec/batch\n",
      "Epoch 1/20  Iteration 65/3560 Training loss: 3.3068 18.3547 sec/batch\n",
      "Epoch 1/20  Iteration 66/3560 Training loss: 3.3044 20.0395 sec/batch\n",
      "Epoch 1/20  Iteration 67/3560 Training loss: 3.3020 18.6435 sec/batch\n",
      "Epoch 1/20  Iteration 68/3560 Training loss: 3.2991 18.4582 sec/batch\n",
      "Epoch 1/20  Iteration 69/3560 Training loss: 3.2964 19.5017 sec/batch\n",
      "Epoch 1/20  Iteration 70/3560 Training loss: 3.2943 18.4473 sec/batch\n",
      "Epoch 1/20  Iteration 71/3560 Training loss: 3.2919 18.9988 sec/batch\n",
      "Epoch 1/20  Iteration 72/3560 Training loss: 3.2899 18.6448 sec/batch\n",
      "Epoch 1/20  Iteration 73/3560 Training loss: 3.2875 20.1367 sec/batch\n",
      "Epoch 1/20  Iteration 74/3560 Training loss: 3.2853 19.2285 sec/batch\n",
      "Epoch 1/20  Iteration 75/3560 Training loss: 3.2833 19.1178 sec/batch\n",
      "Epoch 1/20  Iteration 76/3560 Training loss: 3.2814 19.7428 sec/batch\n",
      "Epoch 1/20  Iteration 77/3560 Training loss: 3.2794 18.2251 sec/batch\n",
      "Epoch 1/20  Iteration 78/3560 Training loss: 3.2773 18.6959 sec/batch\n",
      "Epoch 1/20  Iteration 79/3560 Training loss: 3.2753 18.7292 sec/batch\n",
      "Epoch 1/20  Iteration 80/3560 Training loss: 3.2732 18.8168 sec/batch\n",
      "Epoch 1/20  Iteration 81/3560 Training loss: 3.2711 19.7469 sec/batch\n",
      "Epoch 1/20  Iteration 82/3560 Training loss: 3.2692 19.8955 sec/batch\n",
      "Epoch 1/20  Iteration 83/3560 Training loss: 3.2674 19.1962 sec/batch\n",
      "Epoch 1/20  Iteration 84/3560 Training loss: 3.2654 18.5587 sec/batch\n",
      "Epoch 1/20  Iteration 85/3560 Training loss: 3.2633 18.6193 sec/batch\n",
      "Epoch 1/20  Iteration 86/3560 Training loss: 3.2613 19.0221 sec/batch\n",
      "Epoch 1/20  Iteration 87/3560 Training loss: 3.2593 19.0278 sec/batch\n",
      "Epoch 1/20  Iteration 88/3560 Training loss: 3.2573 18.3489 sec/batch\n",
      "Epoch 1/20  Iteration 89/3560 Training loss: 3.2556 18.5000 sec/batch\n",
      "Epoch 1/20  Iteration 90/3560 Training loss: 3.2538 19.1022 sec/batch\n",
      "Epoch 1/20  Iteration 91/3560 Training loss: 3.2521 18.3902 sec/batch\n",
      "Epoch 1/20  Iteration 92/3560 Training loss: 3.2503 18.3549 sec/batch\n",
      "Epoch 1/20  Iteration 93/3560 Training loss: 3.2486 18.9306 sec/batch\n",
      "Epoch 1/20  Iteration 94/3560 Training loss: 3.2468 20.1176 sec/batch\n",
      "Epoch 1/20  Iteration 95/3560 Training loss: 3.2449 18.9158 sec/batch\n",
      "Epoch 1/20  Iteration 96/3560 Training loss: 3.2431 18.2904 sec/batch\n",
      "Epoch 1/20  Iteration 97/3560 Training loss: 3.2413 19.4328 sec/batch\n",
      "Epoch 1/20  Iteration 98/3560 Training loss: 3.2395 18.3568 sec/batch\n",
      "Epoch 1/20  Iteration 99/3560 Training loss: 3.2380 18.6882 sec/batch\n",
      "Epoch 1/20  Iteration 100/3560 Training loss: 3.2363 18.0400 sec/batch\n",
      "Epoch 1/20  Iteration 101/3560 Training loss: 3.2346 18.8828 sec/batch\n",
      "Epoch 1/20  Iteration 102/3560 Training loss: 3.2330 17.8701 sec/batch\n",
      "Epoch 1/20  Iteration 103/3560 Training loss: 3.2314 18.3651 sec/batch\n",
      "Epoch 1/20  Iteration 104/3560 Training loss: 3.2297 18.5498 sec/batch\n",
      "Epoch 1/20  Iteration 105/3560 Training loss: 3.2280 18.6250 sec/batch\n",
      "Epoch 1/20  Iteration 106/3560 Training loss: 3.2263 17.8383 sec/batch\n",
      "Epoch 1/20  Iteration 107/3560 Training loss: 3.2245 17.8712 sec/batch\n",
      "Epoch 1/20  Iteration 108/3560 Training loss: 3.2227 18.1007 sec/batch\n",
      "Epoch 1/20  Iteration 109/3560 Training loss: 3.2209 19.5488 sec/batch\n",
      "Epoch 1/20  Iteration 110/3560 Training loss: 3.2189 19.8779 sec/batch\n",
      "Epoch 1/20  Iteration 111/3560 Training loss: 3.2171 21.0368 sec/batch\n",
      "Epoch 1/20  Iteration 112/3560 Training loss: 3.2153 21.5289 sec/batch\n",
      "Epoch 1/20  Iteration 113/3560 Training loss: 3.2134 19.0162 sec/batch\n",
      "Epoch 1/20  Iteration 114/3560 Training loss: 3.2114 19.8113 sec/batch\n",
      "Epoch 1/20  Iteration 115/3560 Training loss: 3.2094 18.0103 sec/batch\n",
      "Epoch 1/20  Iteration 116/3560 Training loss: 3.2073 18.0365 sec/batch\n",
      "Epoch 1/20  Iteration 117/3560 Training loss: 3.2054 18.8434 sec/batch\n",
      "Epoch 1/20  Iteration 118/3560 Training loss: 3.2038 18.0668 sec/batch\n",
      "Epoch 1/20  Iteration 119/3560 Training loss: 3.2020 19.1806 sec/batch\n",
      "Epoch 1/20  Iteration 120/3560 Training loss: 3.2001 18.4460 sec/batch\n",
      "Epoch 1/20  Iteration 121/3560 Training loss: 3.1984 18.5410 sec/batch\n",
      "Epoch 1/20  Iteration 122/3560 Training loss: 3.1966 17.8375 sec/batch\n",
      "Epoch 1/20  Iteration 123/3560 Training loss: 3.1947 17.7417 sec/batch\n",
      "Epoch 1/20  Iteration 124/3560 Training loss: 3.1928 18.3433 sec/batch\n",
      "Epoch 1/20  Iteration 125/3560 Training loss: 3.1908 17.8591 sec/batch\n",
      "Epoch 1/20  Iteration 126/3560 Training loss: 3.1886 18.0805 sec/batch\n",
      "Epoch 1/20  Iteration 127/3560 Training loss: 3.1866 18.3126 sec/batch\n",
      "Epoch 1/20  Iteration 128/3560 Training loss: 3.1846 18.5527 sec/batch\n",
      "Epoch 1/20  Iteration 129/3560 Training loss: 3.1825 18.7737 sec/batch\n",
      "Epoch 1/20  Iteration 130/3560 Training loss: 3.1804 19.2769 sec/batch\n",
      "Epoch 1/20  Iteration 131/3560 Training loss: 3.1783 17.7629 sec/batch\n",
      "Epoch 1/20  Iteration 132/3560 Training loss: 3.1760 18.2087 sec/batch\n",
      "Epoch 1/20  Iteration 133/3560 Training loss: 3.1737 18.1656 sec/batch\n",
      "Epoch 1/20  Iteration 134/3560 Training loss: 3.1714 18.6482 sec/batch\n",
      "Epoch 1/20  Iteration 135/3560 Training loss: 3.1689 18.2817 sec/batch\n",
      "Epoch 1/20  Iteration 136/3560 Training loss: 3.1665 18.2523 sec/batch\n",
      "Epoch 1/20  Iteration 137/3560 Training loss: 3.1642 18.3856 sec/batch\n",
      "Epoch 1/20  Iteration 138/3560 Training loss: 3.1618 18.3277 sec/batch\n",
      "Epoch 1/20  Iteration 139/3560 Training loss: 3.1596 18.7117 sec/batch\n",
      "Epoch 1/20  Iteration 140/3560 Training loss: 3.1572 18.7330 sec/batch\n",
      "Epoch 1/20  Iteration 141/3560 Training loss: 3.1549 18.0884 sec/batch\n",
      "Epoch 1/20  Iteration 142/3560 Training loss: 3.1523 18.8527 sec/batch\n",
      "Epoch 1/20  Iteration 143/3560 Training loss: 3.1498 19.7303 sec/batch\n",
      "Epoch 1/20  Iteration 144/3560 Training loss: 3.1473 19.8709 sec/batch\n",
      "Epoch 1/20  Iteration 145/3560 Training loss: 3.1448 19.4842 sec/batch\n",
      "Epoch 1/20  Iteration 146/3560 Training loss: 3.1423 20.6593 sec/batch\n",
      "Epoch 1/20  Iteration 147/3560 Training loss: 3.1397 19.2245 sec/batch\n",
      "Epoch 1/20  Iteration 148/3560 Training loss: 3.1373 19.6596 sec/batch\n",
      "Epoch 1/20  Iteration 149/3560 Training loss: 3.1346 23.7113 sec/batch\n",
      "Epoch 1/20  Iteration 150/3560 Training loss: 3.1320 20.0778 sec/batch\n",
      "Epoch 1/20  Iteration 151/3560 Training loss: 3.1296 21.0885 sec/batch\n",
      "Epoch 1/20  Iteration 152/3560 Training loss: 3.1272 20.9723 sec/batch\n",
      "Epoch 1/20  Iteration 153/3560 Training loss: 3.1245 18.8309 sec/batch\n",
      "Epoch 1/20  Iteration 154/3560 Training loss: 3.1219 17.4288 sec/batch\n",
      "Epoch 1/20  Iteration 155/3560 Training loss: 3.1191 16.9768 sec/batch\n",
      "Epoch 1/20  Iteration 156/3560 Training loss: 3.1163 17.2590 sec/batch\n",
      "Epoch 1/20  Iteration 157/3560 Training loss: 3.1134 17.3136 sec/batch\n",
      "Epoch 1/20  Iteration 158/3560 Training loss: 3.1105 17.2701 sec/batch\n",
      "Epoch 1/20  Iteration 159/3560 Training loss: 3.1075 17.1278 sec/batch\n",
      "Epoch 1/20  Iteration 160/3560 Training loss: 3.1048 17.6182 sec/batch\n",
      "Epoch 1/20  Iteration 161/3560 Training loss: 3.1020 18.2916 sec/batch\n",
      "Epoch 1/20  Iteration 162/3560 Training loss: 3.0990 18.5394 sec/batch\n",
      "Epoch 1/20  Iteration 163/3560 Training loss: 3.0959 17.2725 sec/batch\n",
      "Epoch 1/20  Iteration 164/3560 Training loss: 3.0930 18.0601 sec/batch\n",
      "Epoch 1/20  Iteration 165/3560 Training loss: 3.0901 15.2913 sec/batch\n",
      "Epoch 1/20  Iteration 166/3560 Training loss: 3.0871 14.5791 sec/batch\n",
      "Epoch 1/20  Iteration 167/3560 Training loss: 3.0841 14.5048 sec/batch\n",
      "Epoch 1/20  Iteration 168/3560 Training loss: 3.0812 15.3664 sec/batch\n",
      "Epoch 1/20  Iteration 169/3560 Training loss: 3.0783 15.2610 sec/batch\n",
      "Epoch 1/20  Iteration 170/3560 Training loss: 3.0753 15.1105 sec/batch\n",
      "Epoch 1/20  Iteration 171/3560 Training loss: 3.0724 15.2107 sec/batch\n",
      "Epoch 1/20  Iteration 172/3560 Training loss: 3.0696 15.3005 sec/batch\n",
      "Epoch 1/20  Iteration 173/3560 Training loss: 3.0669 15.1267 sec/batch\n",
      "Epoch 1/20  Iteration 174/3560 Training loss: 3.0642 15.2436 sec/batch\n",
      "Epoch 1/20  Iteration 175/3560 Training loss: 3.0616 15.0531 sec/batch\n",
      "Epoch 1/20  Iteration 176/3560 Training loss: 3.0596 15.0864 sec/batch\n",
      "Epoch 1/20  Iteration 177/3560 Training loss: 3.0568 14.9082 sec/batch\n",
      "Epoch 1/20  Iteration 178/3560 Training loss: 3.0539 15.0976 sec/batch\n",
      "Epoch 2/20  Iteration 179/3560 Training loss: 2.5874 15.3943 sec/batch\n",
      "Epoch 2/20  Iteration 180/3560 Training loss: 2.5459 15.4686 sec/batch\n",
      "Epoch 2/20  Iteration 181/3560 Training loss: 2.5377 15.5766 sec/batch\n",
      "Epoch 2/20  Iteration 182/3560 Training loss: 2.5350 15.2506 sec/batch\n",
      "Epoch 2/20  Iteration 183/3560 Training loss: 2.5321 14.8197 sec/batch\n",
      "Epoch 2/20  Iteration 184/3560 Training loss: 2.5277 14.8561 sec/batch\n",
      "Epoch 2/20  Iteration 185/3560 Training loss: 2.5264 14.8653 sec/batch\n",
      "Epoch 2/20  Iteration 186/3560 Training loss: 2.5257 14.8090 sec/batch\n",
      "Epoch 2/20  Iteration 187/3560 Training loss: 2.5244 14.9329 sec/batch\n",
      "Epoch 2/20  Iteration 188/3560 Training loss: 2.5212 14.8487 sec/batch\n",
      "Epoch 2/20  Iteration 189/3560 Training loss: 2.5178 14.9059 sec/batch\n",
      "Epoch 2/20  Iteration 190/3560 Training loss: 2.5163 14.9164 sec/batch\n",
      "Epoch 2/20  Iteration 191/3560 Training loss: 2.5140 14.9115 sec/batch\n",
      "Epoch 2/20  Iteration 192/3560 Training loss: 2.5143 15.2557 sec/batch\n",
      "Epoch 2/20  Iteration 193/3560 Training loss: 2.5121 14.9594 sec/batch\n",
      "Epoch 2/20  Iteration 194/3560 Training loss: 2.5100 14.8687 sec/batch\n",
      "Epoch 2/20  Iteration 195/3560 Training loss: 2.5077 14.8259 sec/batch\n",
      "Epoch 2/20  Iteration 196/3560 Training loss: 2.5078 14.9490 sec/batch\n",
      "Epoch 2/20  Iteration 197/3560 Training loss: 2.5058 14.9085 sec/batch\n",
      "Epoch 2/20  Iteration 198/3560 Training loss: 2.5027 14.9514 sec/batch\n",
      "Epoch 2/20  Iteration 199/3560 Training loss: 2.5004 14.8374 sec/batch\n",
      "Epoch 2/20  Iteration 200/3560 Training loss: 2.4998 14.8573 sec/batch\n",
      "Validation loss: 2.37405 Saving checkpoint!\n",
      "Epoch 2/20  Iteration 201/3560 Training loss: 2.4986 15.0530 sec/batch\n",
      "Epoch 2/20  Iteration 202/3560 Training loss: 2.4962 14.9403 sec/batch\n",
      "Epoch 2/20  Iteration 203/3560 Training loss: 2.4940 14.9777 sec/batch\n",
      "Epoch 2/20  Iteration 204/3560 Training loss: 2.4920 14.9157 sec/batch\n",
      "Epoch 2/20  Iteration 205/3560 Training loss: 2.4896 14.9087 sec/batch\n",
      "Epoch 2/20  Iteration 206/3560 Training loss: 2.4876 14.9052 sec/batch\n",
      "Epoch 2/20  Iteration 207/3560 Training loss: 2.4859 14.9410 sec/batch\n",
      "Epoch 2/20  Iteration 208/3560 Training loss: 2.4840 15.0069 sec/batch\n",
      "Epoch 2/20  Iteration 209/3560 Training loss: 2.4829 14.8354 sec/batch\n",
      "Epoch 2/20  Iteration 210/3560 Training loss: 2.4807 14.8461 sec/batch\n",
      "Epoch 2/20  Iteration 211/3560 Training loss: 2.4780 14.8682 sec/batch\n",
      "Epoch 2/20  Iteration 212/3560 Training loss: 2.4765 14.8363 sec/batch\n",
      "Epoch 2/20  Iteration 213/3560 Training loss: 2.4746 16.0953 sec/batch\n",
      "Epoch 2/20  Iteration 214/3560 Training loss: 2.4731 14.9037 sec/batch\n",
      "Epoch 2/20  Iteration 215/3560 Training loss: 2.4713 15.1614 sec/batch\n",
      "Epoch 2/20  Iteration 216/3560 Training loss: 2.4687 14.9143 sec/batch\n",
      "Epoch 2/20  Iteration 217/3560 Training loss: 2.4663 14.9411 sec/batch\n",
      "Epoch 2/20  Iteration 218/3560 Training loss: 2.4644 14.8750 sec/batch\n",
      "Epoch 2/20  Iteration 219/3560 Training loss: 2.4625 14.9651 sec/batch\n",
      "Epoch 2/20  Iteration 220/3560 Training loss: 2.4605 14.6892 sec/batch\n",
      "Epoch 2/20  Iteration 221/3560 Training loss: 2.4583 14.9203 sec/batch\n",
      "Epoch 2/20  Iteration 222/3560 Training loss: 2.4561 14.8866 sec/batch\n",
      "Epoch 2/20  Iteration 223/3560 Training loss: 2.4542 14.8336 sec/batch\n",
      "Epoch 2/20  Iteration 224/3560 Training loss: 2.4516 14.8313 sec/batch\n",
      "Epoch 2/20  Iteration 225/3560 Training loss: 2.4504 14.9295 sec/batch\n",
      "Epoch 2/20  Iteration 226/3560 Training loss: 2.4487 14.8653 sec/batch\n",
      "Epoch 2/20  Iteration 227/3560 Training loss: 2.4470 14.8746 sec/batch\n",
      "Epoch 2/20  Iteration 228/3560 Training loss: 2.4457 14.8904 sec/batch\n",
      "Epoch 2/20  Iteration 229/3560 Training loss: 2.4439 14.8676 sec/batch\n",
      "Epoch 2/20  Iteration 230/3560 Training loss: 2.4423 14.9244 sec/batch\n",
      "Epoch 2/20  Iteration 231/3560 Training loss: 2.4406 15.0847 sec/batch\n",
      "Epoch 2/20  Iteration 232/3560 Training loss: 2.4388 14.8707 sec/batch\n",
      "Epoch 2/20  Iteration 233/3560 Training loss: 2.4371 15.2621 sec/batch\n",
      "Epoch 2/20  Iteration 234/3560 Training loss: 2.4357 14.8843 sec/batch\n",
      "Epoch 2/20  Iteration 235/3560 Training loss: 2.4342 14.9679 sec/batch\n",
      "Epoch 2/20  Iteration 236/3560 Training loss: 2.4325 14.8802 sec/batch\n",
      "Epoch 2/20  Iteration 237/3560 Training loss: 2.4309 14.8123 sec/batch\n",
      "Epoch 2/20  Iteration 238/3560 Training loss: 2.4296 14.9631 sec/batch\n",
      "Epoch 2/20  Iteration 239/3560 Training loss: 2.4282 14.8942 sec/batch\n",
      "Epoch 2/20  Iteration 240/3560 Training loss: 2.4269 14.8798 sec/batch\n",
      "Epoch 2/20  Iteration 241/3560 Training loss: 2.4259 14.9162 sec/batch\n",
      "Epoch 2/20  Iteration 242/3560 Training loss: 2.4245 14.8761 sec/batch\n",
      "Epoch 2/20  Iteration 243/3560 Training loss: 2.4229 14.9841 sec/batch\n",
      "Epoch 2/20  Iteration 244/3560 Training loss: 2.4218 14.8999 sec/batch\n",
      "Epoch 2/20  Iteration 245/3560 Training loss: 2.4203 14.9125 sec/batch\n",
      "Epoch 2/20  Iteration 246/3560 Training loss: 2.4184 14.8734 sec/batch\n",
      "Epoch 2/20  Iteration 247/3560 Training loss: 2.4166 14.8237 sec/batch\n",
      "Epoch 2/20  Iteration 248/3560 Training loss: 2.4153 14.8353 sec/batch\n",
      "Epoch 2/20  Iteration 249/3560 Training loss: 2.4143 14.8517 sec/batch\n",
      "Epoch 2/20  Iteration 250/3560 Training loss: 2.4130 14.8308 sec/batch\n",
      "Epoch 2/20  Iteration 251/3560 Training loss: 2.4117 14.9897 sec/batch\n",
      "Epoch 2/20  Iteration 252/3560 Training loss: 2.4101 14.9390 sec/batch\n",
      "Epoch 2/20  Iteration 253/3560 Training loss: 2.4086 15.8380 sec/batch\n",
      "Epoch 2/20  Iteration 254/3560 Training loss: 2.4076 14.9827 sec/batch\n",
      "Epoch 2/20  Iteration 255/3560 Training loss: 2.4061 14.9429 sec/batch\n",
      "Epoch 2/20  Iteration 256/3560 Training loss: 2.4049 14.8218 sec/batch\n",
      "Epoch 2/20  Iteration 257/3560 Training loss: 2.4033 14.8677 sec/batch\n",
      "Epoch 2/20  Iteration 258/3560 Training loss: 2.4018 14.8947 sec/batch\n",
      "Epoch 2/20  Iteration 259/3560 Training loss: 2.4003 14.8770 sec/batch\n",
      "Epoch 2/20  Iteration 260/3560 Training loss: 2.3992 14.8501 sec/batch\n",
      "Epoch 2/20  Iteration 261/3560 Training loss: 2.3976 14.9029 sec/batch\n",
      "Epoch 2/20  Iteration 262/3560 Training loss: 2.3960 14.8162 sec/batch\n",
      "Epoch 2/20  Iteration 263/3560 Training loss: 2.3942 14.8765 sec/batch\n",
      "Epoch 2/20  Iteration 264/3560 Training loss: 2.3926 14.8670 sec/batch\n",
      "Epoch 2/20  Iteration 265/3560 Training loss: 2.3913 14.8700 sec/batch\n",
      "Epoch 2/20  Iteration 266/3560 Training loss: 2.3899 14.8170 sec/batch\n",
      "Epoch 2/20  Iteration 267/3560 Training loss: 2.3884 15.0162 sec/batch\n",
      "Epoch 2/20  Iteration 268/3560 Training loss: 2.3872 14.8541 sec/batch\n",
      "Epoch 2/20  Iteration 269/3560 Training loss: 2.3857 14.8960 sec/batch\n",
      "Epoch 2/20  Iteration 270/3560 Training loss: 2.3844 14.8563 sec/batch\n",
      "Epoch 2/20  Iteration 271/3560 Training loss: 2.3828 14.8886 sec/batch\n",
      "Epoch 2/20  Iteration 272/3560 Training loss: 2.3813 14.9396 sec/batch\n",
      "Epoch 2/20  Iteration 273/3560 Training loss: 2.3797 15.8285 sec/batch\n",
      "Epoch 2/20  Iteration 274/3560 Training loss: 2.3783 14.8666 sec/batch\n",
      "Epoch 2/20  Iteration 275/3560 Training loss: 2.3770 14.9576 sec/batch\n",
      "Epoch 2/20  Iteration 276/3560 Training loss: 2.3756 14.8431 sec/batch\n",
      "Epoch 2/20  Iteration 277/3560 Training loss: 2.3740 15.8803 sec/batch\n",
      "Epoch 2/20  Iteration 278/3560 Training loss: 2.3725 14.6737 sec/batch\n",
      "Epoch 2/20  Iteration 279/3560 Training loss: 2.3713 14.6025 sec/batch\n",
      "Epoch 2/20  Iteration 280/3560 Training loss: 2.3701 14.4510 sec/batch\n",
      "Epoch 2/20  Iteration 281/3560 Training loss: 2.3685 14.5046 sec/batch\n",
      "Epoch 2/20  Iteration 282/3560 Training loss: 2.3671 14.4602 sec/batch\n",
      "Epoch 2/20  Iteration 283/3560 Training loss: 2.3656 14.5448 sec/batch\n",
      "Epoch 2/20  Iteration 284/3560 Training loss: 2.3643 14.4170 sec/batch\n",
      "Epoch 2/20  Iteration 285/3560 Training loss: 2.3629 14.5272 sec/batch\n",
      "Epoch 2/20  Iteration 286/3560 Training loss: 2.3619 14.5598 sec/batch\n",
      "Epoch 2/20  Iteration 287/3560 Training loss: 2.3607 14.5003 sec/batch\n",
      "Epoch 2/20  Iteration 288/3560 Training loss: 2.3594 14.4319 sec/batch\n",
      "Epoch 2/20  Iteration 289/3560 Training loss: 2.3582 14.4868 sec/batch\n",
      "Epoch 2/20  Iteration 290/3560 Training loss: 2.3569 14.4884 sec/batch\n",
      "Epoch 2/20  Iteration 291/3560 Training loss: 2.3556 14.4866 sec/batch\n",
      "Epoch 2/20  Iteration 292/3560 Training loss: 2.3543 14.5574 sec/batch\n",
      "Epoch 2/20  Iteration 293/3560 Training loss: 2.3529 15.1907 sec/batch\n",
      "Epoch 2/20  Iteration 294/3560 Training loss: 2.3514 14.5000 sec/batch\n",
      "Epoch 2/20  Iteration 295/3560 Training loss: 2.3502 14.5613 sec/batch\n",
      "Epoch 2/20  Iteration 296/3560 Training loss: 2.3490 14.4777 sec/batch\n",
      "Epoch 2/20  Iteration 297/3560 Training loss: 2.3479 14.5163 sec/batch\n",
      "Epoch 2/20  Iteration 298/3560 Training loss: 2.3467 14.5046 sec/batch\n",
      "Epoch 2/20  Iteration 299/3560 Training loss: 2.3455 14.5138 sec/batch\n",
      "Epoch 2/20  Iteration 300/3560 Training loss: 2.3442 14.5773 sec/batch\n",
      "Epoch 2/20  Iteration 301/3560 Training loss: 2.3429 14.4698 sec/batch\n",
      "Epoch 2/20  Iteration 302/3560 Training loss: 2.3419 14.4893 sec/batch\n",
      "Epoch 2/20  Iteration 303/3560 Training loss: 2.3407 14.5256 sec/batch\n",
      "Epoch 2/20  Iteration 304/3560 Training loss: 2.3393 14.5518 sec/batch\n",
      "Epoch 2/20  Iteration 305/3560 Training loss: 2.3383 14.7104 sec/batch\n",
      "Epoch 2/20  Iteration 306/3560 Training loss: 2.3372 14.6324 sec/batch\n",
      "Epoch 2/20  Iteration 307/3560 Training loss: 2.3360 14.4280 sec/batch\n",
      "Epoch 2/20  Iteration 308/3560 Training loss: 2.3349 14.6795 sec/batch\n",
      "Epoch 2/20  Iteration 309/3560 Training loss: 2.3337 17.5690 sec/batch\n",
      "Epoch 2/20  Iteration 310/3560 Training loss: 2.3324 15.3238 sec/batch\n",
      "Epoch 2/20  Iteration 311/3560 Training loss: 2.3312 14.6497 sec/batch\n",
      "Epoch 2/20  Iteration 312/3560 Training loss: 2.3302 16.6130 sec/batch\n",
      "Epoch 2/20  Iteration 313/3560 Training loss: 2.3290 15.3126 sec/batch\n",
      "Epoch 2/20  Iteration 314/3560 Training loss: 2.3280 14.7678 sec/batch\n",
      "Epoch 2/20  Iteration 315/3560 Training loss: 2.3269 14.7335 sec/batch\n",
      "Epoch 2/20  Iteration 316/3560 Training loss: 2.3258 16.5646 sec/batch\n",
      "Epoch 2/20  Iteration 317/3560 Training loss: 2.3249 15.8569 sec/batch\n",
      "Epoch 2/20  Iteration 318/3560 Training loss: 2.3238 14.6549 sec/batch\n",
      "Epoch 2/20  Iteration 319/3560 Training loss: 2.3228 14.6307 sec/batch\n",
      "Epoch 2/20  Iteration 320/3560 Training loss: 2.3216 14.4313 sec/batch\n",
      "Epoch 2/20  Iteration 321/3560 Training loss: 2.3205 13.9230 sec/batch\n",
      "Epoch 2/20  Iteration 322/3560 Training loss: 2.3194 13.8755 sec/batch\n",
      "Epoch 2/20  Iteration 323/3560 Training loss: 2.3184 13.8177 sec/batch\n",
      "Epoch 2/20  Iteration 324/3560 Training loss: 2.3175 14.0182 sec/batch\n",
      "Epoch 2/20  Iteration 325/3560 Training loss: 2.3164 13.8185 sec/batch\n",
      "Epoch 2/20  Iteration 326/3560 Training loss: 2.3154 13.9606 sec/batch\n",
      "Epoch 2/20  Iteration 327/3560 Training loss: 2.3143 13.8293 sec/batch\n",
      "Epoch 2/20  Iteration 328/3560 Training loss: 2.3132 13.9444 sec/batch\n",
      "Epoch 2/20  Iteration 329/3560 Training loss: 2.3121 13.8905 sec/batch\n",
      "Epoch 2/20  Iteration 330/3560 Training loss: 2.3113 13.8425 sec/batch\n",
      "Epoch 2/20  Iteration 331/3560 Training loss: 2.3102 13.9070 sec/batch\n",
      "Epoch 2/20  Iteration 332/3560 Training loss: 2.3092 13.8720 sec/batch\n",
      "Epoch 2/20  Iteration 333/3560 Training loss: 2.3081 14.0048 sec/batch\n",
      "Epoch 2/20  Iteration 334/3560 Training loss: 2.3070 14.4730 sec/batch\n",
      "Epoch 2/20  Iteration 335/3560 Training loss: 2.3060 13.9075 sec/batch\n",
      "Epoch 2/20  Iteration 336/3560 Training loss: 2.3049 13.8504 sec/batch\n",
      "Epoch 2/20  Iteration 337/3560 Training loss: 2.3036 14.0187 sec/batch\n",
      "Epoch 2/20  Iteration 338/3560 Training loss: 2.3028 13.8430 sec/batch\n",
      "Epoch 2/20  Iteration 339/3560 Training loss: 2.3019 13.8490 sec/batch\n",
      "Epoch 2/20  Iteration 340/3560 Training loss: 2.3008 13.8854 sec/batch\n",
      "Epoch 2/20  Iteration 341/3560 Training loss: 2.2997 14.0236 sec/batch\n",
      "Epoch 2/20  Iteration 342/3560 Training loss: 2.2987 13.8358 sec/batch\n",
      "Epoch 2/20  Iteration 343/3560 Training loss: 2.2977 13.8709 sec/batch\n",
      "Epoch 2/20  Iteration 344/3560 Training loss: 2.2967 13.9323 sec/batch\n",
      "Epoch 2/20  Iteration 345/3560 Training loss: 2.2957 13.9264 sec/batch\n",
      "Epoch 2/20  Iteration 346/3560 Training loss: 2.2949 13.9037 sec/batch\n",
      "Epoch 2/20  Iteration 347/3560 Training loss: 2.2939 13.8587 sec/batch\n",
      "Epoch 2/20  Iteration 348/3560 Training loss: 2.2928 13.8801 sec/batch\n",
      "Epoch 2/20  Iteration 349/3560 Training loss: 2.2917 13.8238 sec/batch\n",
      "Epoch 2/20  Iteration 350/3560 Training loss: 2.2906 14.0222 sec/batch\n",
      "Epoch 2/20  Iteration 351/3560 Training loss: 2.2898 13.8580 sec/batch\n",
      "Epoch 2/20  Iteration 352/3560 Training loss: 2.2888 13.8673 sec/batch\n",
      "Epoch 2/20  Iteration 353/3560 Training loss: 2.2879 13.7801 sec/batch\n",
      "Epoch 2/20  Iteration 354/3560 Training loss: 2.2869 13.9399 sec/batch\n",
      "Epoch 2/20  Iteration 355/3560 Training loss: 2.2858 13.8683 sec/batch\n",
      "Epoch 2/20  Iteration 356/3560 Training loss: 2.2848 15.1015 sec/batch\n",
      "Epoch 3/20  Iteration 357/3560 Training loss: 2.1645 13.8753 sec/batch\n",
      "Epoch 3/20  Iteration 358/3560 Training loss: 2.1233 13.9402 sec/batch\n",
      "Epoch 3/20  Iteration 359/3560 Training loss: 2.1106 13.9080 sec/batch\n",
      "Epoch 3/20  Iteration 360/3560 Training loss: 2.1050 13.8891 sec/batch\n",
      "Epoch 3/20  Iteration 361/3560 Training loss: 2.1019 13.8411 sec/batch\n",
      "Epoch 3/20  Iteration 362/3560 Training loss: 2.0952 13.8588 sec/batch\n",
      "Epoch 3/20  Iteration 363/3560 Training loss: 2.0962 13.9081 sec/batch\n",
      "Epoch 3/20  Iteration 364/3560 Training loss: 2.0955 13.7798 sec/batch\n",
      "Epoch 3/20  Iteration 365/3560 Training loss: 2.0984 13.8730 sec/batch\n",
      "Epoch 3/20  Iteration 366/3560 Training loss: 2.0982 13.8695 sec/batch\n",
      "Epoch 3/20  Iteration 367/3560 Training loss: 2.0953 14.4940 sec/batch\n",
      "Epoch 3/20  Iteration 368/3560 Training loss: 2.0930 13.7152 sec/batch\n",
      "Epoch 3/20  Iteration 369/3560 Training loss: 2.0924 13.8300 sec/batch\n",
      "Epoch 3/20  Iteration 370/3560 Training loss: 2.0945 13.6589 sec/batch\n",
      "Epoch 3/20  Iteration 371/3560 Training loss: 2.0938 13.8119 sec/batch\n",
      "Epoch 3/20  Iteration 372/3560 Training loss: 2.0926 13.7661 sec/batch\n",
      "Epoch 3/20  Iteration 373/3560 Training loss: 2.0915 13.7121 sec/batch\n",
      "Epoch 3/20  Iteration 374/3560 Training loss: 2.0938 13.6936 sec/batch\n",
      "Epoch 3/20  Iteration 375/3560 Training loss: 2.0933 13.6942 sec/batch\n",
      "Epoch 3/20  Iteration 376/3560 Training loss: 2.0924 13.8993 sec/batch\n",
      "Epoch 3/20  Iteration 377/3560 Training loss: 2.0910 14.0579 sec/batch\n",
      "Epoch 3/20  Iteration 378/3560 Training loss: 2.0920 13.6822 sec/batch\n",
      "Epoch 3/20  Iteration 379/3560 Training loss: 2.0912 13.7036 sec/batch\n",
      "Epoch 3/20  Iteration 380/3560 Training loss: 2.0897 13.8443 sec/batch\n",
      "Epoch 3/20  Iteration 381/3560 Training loss: 2.0886 13.7238 sec/batch\n",
      "Epoch 3/20  Iteration 382/3560 Training loss: 2.0869 13.7110 sec/batch\n",
      "Epoch 3/20  Iteration 383/3560 Training loss: 2.0854 13.7857 sec/batch\n",
      "Epoch 3/20  Iteration 384/3560 Training loss: 2.0852 13.8774 sec/batch\n",
      "Epoch 3/20  Iteration 385/3560 Training loss: 2.0855 13.7675 sec/batch\n",
      "Epoch 3/20  Iteration 386/3560 Training loss: 2.0848 13.7221 sec/batch\n",
      "Epoch 3/20  Iteration 387/3560 Training loss: 2.0841 13.7096 sec/batch\n",
      "Epoch 3/20  Iteration 388/3560 Training loss: 2.0829 13.7078 sec/batch\n",
      "Epoch 3/20  Iteration 389/3560 Training loss: 2.0820 13.8297 sec/batch\n",
      "Epoch 3/20  Iteration 390/3560 Training loss: 2.0819 13.7643 sec/batch\n",
      "Epoch 3/20  Iteration 391/3560 Training loss: 2.0807 13.9435 sec/batch\n",
      "Epoch 3/20  Iteration 392/3560 Training loss: 2.0797 13.7193 sec/batch\n",
      "Epoch 3/20  Iteration 393/3560 Training loss: 2.0787 13.8407 sec/batch\n",
      "Epoch 3/20  Iteration 394/3560 Training loss: 2.0768 13.7111 sec/batch\n",
      "Epoch 3/20  Iteration 395/3560 Training loss: 2.0750 13.7049 sec/batch\n",
      "Epoch 3/20  Iteration 396/3560 Training loss: 2.0735 13.6974 sec/batch\n",
      "Epoch 3/20  Iteration 397/3560 Training loss: 2.0723 13.7009 sec/batch\n",
      "Epoch 3/20  Iteration 398/3560 Training loss: 2.0714 13.8307 sec/batch\n",
      "Epoch 3/20  Iteration 399/3560 Training loss: 2.0701 14.3538 sec/batch\n",
      "Epoch 3/20  Iteration 400/3560 Training loss: 2.0687 13.6733 sec/batch\n",
      "Validation loss: 1.93582 Saving checkpoint!\n",
      "Epoch 3/20  Iteration 401/3560 Training loss: 2.0689 13.6733 sec/batch\n",
      "Epoch 3/20  Iteration 402/3560 Training loss: 2.0669 13.6691 sec/batch\n",
      "Epoch 3/20  Iteration 403/3560 Training loss: 2.0662 13.8378 sec/batch\n",
      "Epoch 3/20  Iteration 404/3560 Training loss: 2.0651 13.6322 sec/batch\n",
      "Epoch 3/20  Iteration 405/3560 Training loss: 2.0643 13.7176 sec/batch\n",
      "Epoch 3/20  Iteration 406/3560 Training loss: 2.0640 13.7259 sec/batch\n",
      "Epoch 3/20  Iteration 407/3560 Training loss: 2.0625 13.6819 sec/batch\n",
      "Epoch 3/20  Iteration 408/3560 Training loss: 2.0623 13.7254 sec/batch\n",
      "Epoch 3/20  Iteration 409/3560 Training loss: 2.0613 13.7177 sec/batch\n",
      "Epoch 3/20  Iteration 410/3560 Training loss: 2.0606 13.7284 sec/batch\n",
      "Epoch 3/20  Iteration 411/3560 Training loss: 2.0595 13.6658 sec/batch\n",
      "Epoch 3/20  Iteration 412/3560 Training loss: 2.0588 13.8914 sec/batch\n",
      "Epoch 3/20  Iteration 413/3560 Training loss: 2.0581 14.2300 sec/batch\n",
      "Epoch 3/20  Iteration 414/3560 Training loss: 2.0570 13.7437 sec/batch\n",
      "Epoch 3/20  Iteration 415/3560 Training loss: 2.0560 13.6727 sec/batch\n",
      "Epoch 3/20  Iteration 416/3560 Training loss: 2.0559 13.7927 sec/batch\n",
      "Epoch 3/20  Iteration 417/3560 Training loss: 2.0552 13.6505 sec/batch\n",
      "Epoch 3/20  Iteration 418/3560 Training loss: 2.0551 13.6894 sec/batch\n",
      "Epoch 3/20  Iteration 419/3560 Training loss: 2.0549 13.7405 sec/batch\n",
      "Epoch 3/20  Iteration 420/3560 Training loss: 2.0544 13.6959 sec/batch\n",
      "Epoch 3/20  Iteration 421/3560 Training loss: 2.0536 13.7600 sec/batch\n",
      "Epoch 3/20  Iteration 422/3560 Training loss: 2.0534 13.7637 sec/batch\n",
      "Epoch 3/20  Iteration 423/3560 Training loss: 2.0528 13.6846 sec/batch\n",
      "Epoch 3/20  Iteration 424/3560 Training loss: 2.0517 13.6979 sec/batch\n",
      "Epoch 3/20  Iteration 425/3560 Training loss: 2.0508 13.7687 sec/batch\n",
      "Epoch 3/20  Iteration 426/3560 Training loss: 2.0502 13.7040 sec/batch\n",
      "Epoch 3/20  Iteration 427/3560 Training loss: 2.0500 13.6595 sec/batch\n",
      "Epoch 3/20  Iteration 428/3560 Training loss: 2.0495 13.6987 sec/batch\n",
      "Epoch 3/20  Iteration 429/3560 Training loss: 2.0491 13.8379 sec/batch\n",
      "Epoch 3/20  Iteration 430/3560 Training loss: 2.0482 13.6774 sec/batch\n",
      "Epoch 3/20  Iteration 431/3560 Training loss: 2.0474 13.6837 sec/batch\n",
      "Epoch 3/20  Iteration 432/3560 Training loss: 2.0472 13.6560 sec/batch\n",
      "Epoch 3/20  Iteration 433/3560 Training loss: 2.0464 14.0515 sec/batch\n",
      "Epoch 3/20  Iteration 434/3560 Training loss: 2.0458 13.7903 sec/batch\n",
      "Epoch 3/20  Iteration 435/3560 Training loss: 2.0447 14.7565 sec/batch\n",
      "Epoch 3/20  Iteration 436/3560 Training loss: 2.0438 13.7552 sec/batch\n",
      "Epoch 3/20  Iteration 437/3560 Training loss: 2.0426 13.7150 sec/batch\n",
      "Epoch 3/20  Iteration 438/3560 Training loss: 2.0421 13.7858 sec/batch\n",
      "Epoch 3/20  Iteration 439/3560 Training loss: 2.0409 13.6730 sec/batch\n",
      "Epoch 3/20  Iteration 440/3560 Training loss: 2.0401 13.7088 sec/batch\n",
      "Epoch 3/20  Iteration 441/3560 Training loss: 2.0390 13.6882 sec/batch\n",
      "Epoch 3/20  Iteration 442/3560 Training loss: 2.0380 13.8839 sec/batch\n",
      "Epoch 3/20  Iteration 443/3560 Training loss: 2.0372 13.8144 sec/batch\n",
      "Epoch 3/20  Iteration 444/3560 Training loss: 2.0364 13.6647 sec/batch\n",
      "Epoch 3/20  Iteration 445/3560 Training loss: 2.0356 13.6843 sec/batch\n",
      "Epoch 3/20  Iteration 446/3560 Training loss: 2.0352 13.6872 sec/batch\n",
      "Epoch 3/20  Iteration 447/3560 Training loss: 2.0344 13.7929 sec/batch\n",
      "Epoch 3/20  Iteration 448/3560 Training loss: 2.0337 13.7158 sec/batch\n",
      "Epoch 3/20  Iteration 449/3560 Training loss: 2.0327 13.7284 sec/batch\n",
      "Epoch 3/20  Iteration 450/3560 Training loss: 2.0318 13.6143 sec/batch\n",
      "Epoch 3/20  Iteration 451/3560 Training loss: 2.0308 13.7483 sec/batch\n",
      "Epoch 3/20  Iteration 452/3560 Training loss: 2.0301 13.7150 sec/batch\n",
      "Epoch 3/20  Iteration 453/3560 Training loss: 2.0294 13.6774 sec/batch\n",
      "Epoch 3/20  Iteration 454/3560 Training loss: 2.0285 13.6774 sec/batch\n",
      "Epoch 3/20  Iteration 455/3560 Training loss: 2.0276 13.8074 sec/batch\n",
      "Epoch 3/20  Iteration 456/3560 Training loss: 2.0266 13.7796 sec/batch\n",
      "Epoch 3/20  Iteration 457/3560 Training loss: 2.0261 14.7712 sec/batch\n",
      "Epoch 3/20  Iteration 458/3560 Training loss: 2.0255 13.7214 sec/batch\n",
      "Epoch 3/20  Iteration 459/3560 Training loss: 2.0246 13.6591 sec/batch\n",
      "Epoch 3/20  Iteration 460/3560 Training loss: 2.0239 13.7467 sec/batch\n",
      "Epoch 3/20  Iteration 461/3560 Training loss: 2.0231 13.6915 sec/batch\n",
      "Epoch 3/20  Iteration 462/3560 Training loss: 2.0225 13.7462 sec/batch\n",
      "Epoch 3/20  Iteration 463/3560 Training loss: 2.0219 13.6780 sec/batch\n",
      "Epoch 3/20  Iteration 464/3560 Training loss: 2.0213 13.8951 sec/batch\n",
      "Epoch 3/20  Iteration 465/3560 Training loss: 2.0208 13.7045 sec/batch\n",
      "Epoch 3/20  Iteration 466/3560 Training loss: 2.0202 13.6741 sec/batch\n",
      "Epoch 3/20  Iteration 467/3560 Training loss: 2.0195 13.6706 sec/batch\n",
      "Epoch 3/20  Iteration 468/3560 Training loss: 2.0188 13.6827 sec/batch\n",
      "Epoch 3/20  Iteration 469/3560 Training loss: 2.0179 13.7665 sec/batch\n",
      "Epoch 3/20  Iteration 470/3560 Training loss: 2.0173 13.6926 sec/batch\n",
      "Epoch 3/20  Iteration 471/3560 Training loss: 2.0164 13.7240 sec/batch\n",
      "Epoch 3/20  Iteration 472/3560 Training loss: 2.0155 13.7129 sec/batch\n",
      "Epoch 3/20  Iteration 473/3560 Training loss: 2.0148 13.7889 sec/batch\n",
      "Epoch 3/20  Iteration 474/3560 Training loss: 2.0141 13.6480 sec/batch\n",
      "Epoch 3/20  Iteration 475/3560 Training loss: 2.0135 13.7268 sec/batch\n",
      "Epoch 3/20  Iteration 476/3560 Training loss: 2.0128 13.7733 sec/batch\n",
      "Epoch 3/20  Iteration 477/3560 Training loss: 2.0123 13.7614 sec/batch\n",
      "Epoch 3/20  Iteration 478/3560 Training loss: 2.0114 13.7175 sec/batch\n",
      "Epoch 3/20  Iteration 479/3560 Training loss: 2.0106 14.1772 sec/batch\n",
      "Epoch 3/20  Iteration 480/3560 Training loss: 2.0101 13.8314 sec/batch\n",
      "Epoch 3/20  Iteration 481/3560 Training loss: 2.0094 13.7689 sec/batch\n",
      "Epoch 3/20  Iteration 482/3560 Training loss: 2.0085 13.7305 sec/batch\n",
      "Epoch 3/20  Iteration 483/3560 Training loss: 2.0081 13.7156 sec/batch\n",
      "Epoch 3/20  Iteration 484/3560 Training loss: 2.0075 13.7027 sec/batch\n",
      "Epoch 3/20  Iteration 485/3560 Training loss: 2.0069 13.7491 sec/batch\n",
      "Epoch 3/20  Iteration 486/3560 Training loss: 2.0063 13.7849 sec/batch\n",
      "Epoch 3/20  Iteration 487/3560 Training loss: 2.0055 13.7197 sec/batch\n",
      "Epoch 3/20  Iteration 488/3560 Training loss: 2.0046 13.6723 sec/batch\n",
      "Epoch 3/20  Iteration 489/3560 Training loss: 2.0040 13.6071 sec/batch\n",
      "Epoch 3/20  Iteration 490/3560 Training loss: 2.0035 15.1201 sec/batch\n",
      "Epoch 3/20  Iteration 491/3560 Training loss: 2.0028 13.4721 sec/batch\n",
      "Epoch 3/20  Iteration 492/3560 Training loss: 2.0023 13.5814 sec/batch\n",
      "Epoch 3/20  Iteration 493/3560 Training loss: 2.0018 13.5594 sec/batch\n",
      "Epoch 3/20  Iteration 494/3560 Training loss: 2.0012 13.4473 sec/batch\n",
      "Epoch 3/20  Iteration 495/3560 Training loss: 2.0008 13.5648 sec/batch\n",
      "Epoch 3/20  Iteration 496/3560 Training loss: 2.0002 13.5073 sec/batch\n",
      "Epoch 3/20  Iteration 497/3560 Training loss: 1.9998 13.5293 sec/batch\n",
      "Epoch 3/20  Iteration 498/3560 Training loss: 1.9992 13.5380 sec/batch\n",
      "Epoch 3/20  Iteration 499/3560 Training loss: 1.9986 13.6677 sec/batch\n",
      "Epoch 3/20  Iteration 500/3560 Training loss: 1.9981 13.4603 sec/batch\n",
      "Epoch 3/20  Iteration 501/3560 Training loss: 1.9974 14.1428 sec/batch\n",
      "Epoch 3/20  Iteration 502/3560 Training loss: 1.9969 13.5460 sec/batch\n",
      "Epoch 3/20  Iteration 503/3560 Training loss: 1.9965 13.6182 sec/batch\n",
      "Epoch 3/20  Iteration 504/3560 Training loss: 1.9961 13.5306 sec/batch\n",
      "Epoch 3/20  Iteration 505/3560 Training loss: 1.9956 13.5064 sec/batch\n",
      "Epoch 3/20  Iteration 506/3560 Training loss: 1.9949 13.4984 sec/batch\n",
      "Epoch 3/20  Iteration 507/3560 Training loss: 1.9943 13.5417 sec/batch\n",
      "Epoch 3/20  Iteration 508/3560 Training loss: 1.9939 13.6297 sec/batch\n",
      "Epoch 3/20  Iteration 509/3560 Training loss: 1.9935 13.5118 sec/batch\n",
      "Epoch 3/20  Iteration 510/3560 Training loss: 1.9929 13.5739 sec/batch\n",
      "Epoch 3/20  Iteration 511/3560 Training loss: 1.9923 13.5073 sec/batch\n",
      "Epoch 3/20  Iteration 512/3560 Training loss: 1.9918 13.5895 sec/batch\n",
      "Epoch 3/20  Iteration 513/3560 Training loss: 1.9913 13.5352 sec/batch\n",
      "Epoch 3/20  Iteration 514/3560 Training loss: 1.9907 13.5203 sec/batch\n",
      "Epoch 3/20  Iteration 515/3560 Training loss: 1.9900 13.5172 sec/batch\n",
      "Epoch 3/20  Iteration 516/3560 Training loss: 1.9897 13.5044 sec/batch\n",
      "Epoch 3/20  Iteration 517/3560 Training loss: 1.9893 13.6563 sec/batch\n",
      "Epoch 3/20  Iteration 518/3560 Training loss: 1.9888 13.4901 sec/batch\n",
      "Epoch 3/20  Iteration 519/3560 Training loss: 1.9883 13.5086 sec/batch\n",
      "Epoch 3/20  Iteration 520/3560 Training loss: 1.9878 13.4682 sec/batch\n",
      "Epoch 3/20  Iteration 521/3560 Training loss: 1.9873 13.6130 sec/batch\n",
      "Epoch 3/20  Iteration 522/3560 Training loss: 1.9867 13.5720 sec/batch\n",
      "Epoch 3/20  Iteration 523/3560 Training loss: 1.9862 14.7802 sec/batch\n",
      "Epoch 3/20  Iteration 524/3560 Training loss: 1.9860 13.5146 sec/batch\n",
      "Epoch 3/20  Iteration 525/3560 Training loss: 1.9854 13.5511 sec/batch\n",
      "Epoch 3/20  Iteration 526/3560 Training loss: 1.9849 13.5415 sec/batch\n",
      "Epoch 3/20  Iteration 527/3560 Training loss: 1.9842 13.4876 sec/batch\n",
      "Epoch 3/20  Iteration 528/3560 Training loss: 1.9835 13.5079 sec/batch\n",
      "Epoch 3/20  Iteration 529/3560 Training loss: 1.9831 13.5373 sec/batch\n",
      "Epoch 3/20  Iteration 530/3560 Training loss: 1.9826 13.5432 sec/batch\n",
      "Epoch 3/20  Iteration 531/3560 Training loss: 1.9822 13.4998 sec/batch\n",
      "Epoch 3/20  Iteration 532/3560 Training loss: 1.9816 13.5270 sec/batch\n",
      "Epoch 3/20  Iteration 533/3560 Training loss: 1.9810 13.4859 sec/batch\n",
      "Epoch 3/20  Iteration 534/3560 Training loss: 1.9805 13.6152 sec/batch\n",
      "Epoch 4/20  Iteration 535/3560 Training loss: 1.9557 13.5246 sec/batch\n",
      "Epoch 4/20  Iteration 536/3560 Training loss: 1.9141 13.4750 sec/batch\n",
      "Epoch 4/20  Iteration 537/3560 Training loss: 1.8984 13.6110 sec/batch\n",
      "Epoch 4/20  Iteration 538/3560 Training loss: 1.8907 13.4648 sec/batch\n",
      "Epoch 4/20  Iteration 539/3560 Training loss: 1.8881 13.6305 sec/batch\n",
      "Epoch 4/20  Iteration 540/3560 Training loss: 1.8770 13.5165 sec/batch\n",
      "Epoch 4/20  Iteration 541/3560 Training loss: 1.8774 13.5395 sec/batch\n",
      "Epoch 4/20  Iteration 542/3560 Training loss: 1.8756 13.5051 sec/batch\n",
      "Epoch 4/20  Iteration 543/3560 Training loss: 1.8793 13.6163 sec/batch\n",
      "Epoch 4/20  Iteration 544/3560 Training loss: 1.8784 13.4607 sec/batch\n",
      "Epoch 4/20  Iteration 545/3560 Training loss: 1.8750 13.8735 sec/batch\n",
      "Epoch 4/20  Iteration 546/3560 Training loss: 1.8731 13.5254 sec/batch\n",
      "Epoch 4/20  Iteration 547/3560 Training loss: 1.8729 13.4940 sec/batch\n",
      "Epoch 4/20  Iteration 548/3560 Training loss: 1.8750 13.5443 sec/batch\n",
      "Epoch 4/20  Iteration 549/3560 Training loss: 1.8738 13.5509 sec/batch\n",
      "Epoch 4/20  Iteration 550/3560 Training loss: 1.8709 13.4877 sec/batch\n",
      "Epoch 4/20  Iteration 551/3560 Training loss: 1.8705 13.4906 sec/batch\n",
      "Epoch 4/20  Iteration 552/3560 Training loss: 1.8730 13.6404 sec/batch\n",
      "Epoch 4/20  Iteration 553/3560 Training loss: 1.8730 13.5368 sec/batch\n",
      "Epoch 4/20  Iteration 554/3560 Training loss: 1.8735 13.5201 sec/batch\n",
      "Epoch 4/20  Iteration 555/3560 Training loss: 1.8724 13.5006 sec/batch\n",
      "Epoch 4/20  Iteration 556/3560 Training loss: 1.8731 13.5138 sec/batch\n",
      "Epoch 4/20  Iteration 557/3560 Training loss: 1.8720 13.6457 sec/batch\n",
      "Epoch 4/20  Iteration 558/3560 Training loss: 1.8710 13.4860 sec/batch\n",
      "Epoch 4/20  Iteration 559/3560 Training loss: 1.8705 13.5366 sec/batch\n",
      "Epoch 4/20  Iteration 560/3560 Training loss: 1.8689 13.5107 sec/batch\n",
      "Epoch 4/20  Iteration 561/3560 Training loss: 1.8677 13.6274 sec/batch\n",
      "Epoch 4/20  Iteration 562/3560 Training loss: 1.8674 13.5591 sec/batch\n",
      "Epoch 4/20  Iteration 563/3560 Training loss: 1.8683 13.4917 sec/batch\n",
      "Epoch 4/20  Iteration 564/3560 Training loss: 1.8680 13.4831 sec/batch\n",
      "Epoch 4/20  Iteration 565/3560 Training loss: 1.8674 13.5994 sec/batch\n",
      "Epoch 4/20  Iteration 566/3560 Training loss: 1.8658 13.4673 sec/batch\n",
      "Epoch 4/20  Iteration 567/3560 Training loss: 1.8657 14.1373 sec/batch\n",
      "Epoch 4/20  Iteration 568/3560 Training loss: 1.8661 13.5502 sec/batch\n",
      "Epoch 4/20  Iteration 569/3560 Training loss: 1.8651 13.4577 sec/batch\n",
      "Epoch 4/20  Iteration 570/3560 Training loss: 1.8645 14.0175 sec/batch\n",
      "Epoch 4/20  Iteration 571/3560 Training loss: 1.8634 13.5322 sec/batch\n",
      "Epoch 4/20  Iteration 572/3560 Training loss: 1.8622 13.5438 sec/batch\n",
      "Epoch 4/20  Iteration 573/3560 Training loss: 1.8603 13.5487 sec/batch\n",
      "Epoch 4/20  Iteration 574/3560 Training loss: 1.8593 13.5898 sec/batch\n",
      "Epoch 4/20  Iteration 575/3560 Training loss: 1.8582 13.5048 sec/batch\n",
      "Epoch 4/20  Iteration 576/3560 Training loss: 1.8581 13.5280 sec/batch\n",
      "Epoch 4/20  Iteration 577/3560 Training loss: 1.8571 13.6268 sec/batch\n",
      "Epoch 4/20  Iteration 578/3560 Training loss: 1.8559 13.5199 sec/batch\n",
      "Epoch 4/20  Iteration 579/3560 Training loss: 1.8558 13.6670 sec/batch\n",
      "Epoch 4/20  Iteration 580/3560 Training loss: 1.8541 13.5390 sec/batch\n",
      "Epoch 4/20  Iteration 581/3560 Training loss: 1.8533 13.6069 sec/batch\n",
      "Epoch 4/20  Iteration 582/3560 Training loss: 1.8523 13.5033 sec/batch\n",
      "Epoch 4/20  Iteration 583/3560 Training loss: 1.8518 13.5624 sec/batch\n",
      "Epoch 4/20  Iteration 584/3560 Training loss: 1.8520 13.4849 sec/batch\n",
      "Epoch 4/20  Iteration 585/3560 Training loss: 1.8509 13.5280 sec/batch\n",
      "Epoch 4/20  Iteration 586/3560 Training loss: 1.8515 13.4710 sec/batch\n",
      "Epoch 4/20  Iteration 587/3560 Training loss: 1.8508 13.6097 sec/batch\n",
      "Epoch 4/20  Iteration 588/3560 Training loss: 1.8503 13.5609 sec/batch\n",
      "Epoch 4/20  Iteration 589/3560 Training loss: 1.8494 14.0618 sec/batch\n",
      "Epoch 4/20  Iteration 590/3560 Training loss: 1.8491 13.5575 sec/batch\n",
      "Epoch 4/20  Iteration 591/3560 Training loss: 1.8488 13.5043 sec/batch\n",
      "Epoch 4/20  Iteration 592/3560 Training loss: 1.8481 13.5654 sec/batch\n",
      "Epoch 4/20  Iteration 593/3560 Training loss: 1.8472 13.5296 sec/batch\n",
      "Epoch 4/20  Iteration 594/3560 Training loss: 1.8475 13.4959 sec/batch\n",
      "Epoch 4/20  Iteration 595/3560 Training loss: 1.8470 13.4950 sec/batch\n",
      "Epoch 4/20  Iteration 596/3560 Training loss: 1.8474 13.6536 sec/batch\n",
      "Epoch 4/20  Iteration 597/3560 Training loss: 1.8474 13.5375 sec/batch\n",
      "Epoch 4/20  Iteration 598/3560 Training loss: 1.8473 13.5055 sec/batch\n",
      "Epoch 4/20  Iteration 599/3560 Training loss: 1.8468 13.4844 sec/batch\n",
      "Epoch 4/20  Iteration 600/3560 Training loss: 1.8468 13.5202 sec/batch\n",
      "Validation loss: 1.71146 Saving checkpoint!\n",
      "Epoch 4/20  Iteration 601/3560 Training loss: 1.8472 13.5282 sec/batch\n",
      "Epoch 4/20  Iteration 602/3560 Training loss: 1.8465 13.5567 sec/batch\n",
      "Epoch 4/20  Iteration 603/3560 Training loss: 1.8459 13.5414 sec/batch\n",
      "Epoch 4/20  Iteration 604/3560 Training loss: 1.8454 14.5985 sec/batch\n",
      "Epoch 4/20  Iteration 605/3560 Training loss: 1.8457 13.5167 sec/batch\n",
      "Epoch 4/20  Iteration 606/3560 Training loss: 1.8453 13.6378 sec/batch\n",
      "Epoch 4/20  Iteration 607/3560 Training loss: 1.8453 13.5153 sec/batch\n",
      "Epoch 4/20  Iteration 608/3560 Training loss: 1.8445 13.5046 sec/batch\n",
      "Epoch 4/20  Iteration 609/3560 Training loss: 1.8441 13.5407 sec/batch\n",
      "Epoch 4/20  Iteration 610/3560 Training loss: 1.8440 13.5402 sec/batch\n",
      "Epoch 4/20  Iteration 611/3560 Training loss: 1.8434 13.6003 sec/batch\n",
      "Epoch 4/20  Iteration 612/3560 Training loss: 1.8430 13.4896 sec/batch\n",
      "Epoch 4/20  Iteration 613/3560 Training loss: 1.8421 13.5475 sec/batch\n",
      "Epoch 4/20  Iteration 614/3560 Training loss: 1.8415 13.4993 sec/batch\n",
      "Epoch 4/20  Iteration 615/3560 Training loss: 1.8407 13.6546 sec/batch\n",
      "Epoch 4/20  Iteration 616/3560 Training loss: 1.8404 14.3418 sec/batch\n",
      "Epoch 4/20  Iteration 617/3560 Training loss: 1.8395 13.6326 sec/batch\n",
      "Epoch 4/20  Iteration 618/3560 Training loss: 1.8390 13.7534 sec/batch\n",
      "Epoch 4/20  Iteration 619/3560 Training loss: 1.8383 13.5558 sec/batch\n",
      "Epoch 4/20  Iteration 620/3560 Training loss: 1.8376 13.5642 sec/batch\n",
      "Epoch 4/20  Iteration 621/3560 Training loss: 1.8371 13.5868 sec/batch\n",
      "Epoch 4/20  Iteration 622/3560 Training loss: 1.8364 13.5534 sec/batch\n",
      "Epoch 4/20  Iteration 623/3560 Training loss: 1.8356 13.5957 sec/batch\n",
      "Epoch 4/20  Iteration 624/3560 Training loss: 1.8354 13.6481 sec/batch\n",
      "Epoch 4/20  Iteration 625/3560 Training loss: 1.8348 13.5901 sec/batch\n",
      "Epoch 4/20  Iteration 626/3560 Training loss: 1.8343 13.8893 sec/batch\n",
      "Epoch 4/20  Iteration 627/3560 Training loss: 1.8334 13.6030 sec/batch\n",
      "Epoch 4/20  Iteration 628/3560 Training loss: 1.8328 13.5439 sec/batch\n",
      "Epoch 4/20  Iteration 629/3560 Training loss: 1.8321 13.6081 sec/batch\n",
      "Epoch 4/20  Iteration 630/3560 Training loss: 1.8316 13.6230 sec/batch\n",
      "Epoch 4/20  Iteration 631/3560 Training loss: 1.8311 13.6154 sec/batch\n",
      "Epoch 4/20  Iteration 632/3560 Training loss: 1.8303 13.5744 sec/batch\n",
      "Epoch 4/20  Iteration 633/3560 Training loss: 1.8296 13.6712 sec/batch\n",
      "Epoch 4/20  Iteration 634/3560 Training loss: 1.8287 13.5788 sec/batch\n",
      "Epoch 4/20  Iteration 635/3560 Training loss: 1.8283 15.3639 sec/batch\n",
      "Epoch 4/20  Iteration 636/3560 Training loss: 1.8279 18.9911 sec/batch\n",
      "Epoch 4/20  Iteration 637/3560 Training loss: 1.8272 14.9796 sec/batch\n",
      "Epoch 4/20  Iteration 638/3560 Training loss: 1.8267 18.6431 sec/batch\n",
      "Epoch 4/20  Iteration 639/3560 Training loss: 1.8262 14.9441 sec/batch\n",
      "Epoch 4/20  Iteration 640/3560 Training loss: 1.8257 15.3461 sec/batch\n",
      "Epoch 4/20  Iteration 641/3560 Training loss: 1.8253 15.0532 sec/batch\n",
      "Epoch 4/20  Iteration 642/3560 Training loss: 1.8248 15.1049 sec/batch\n",
      "Epoch 4/20  Iteration 643/3560 Training loss: 1.8245 18.9363 sec/batch\n",
      "Epoch 4/20  Iteration 644/3560 Training loss: 1.8241 19.7390 sec/batch\n",
      "Epoch 4/20  Iteration 645/3560 Training loss: 1.8235 22.5922 sec/batch\n",
      "Epoch 4/20  Iteration 646/3560 Training loss: 1.8230 17.3822 sec/batch\n",
      "Epoch 4/20  Iteration 647/3560 Training loss: 1.8225 20.0067 sec/batch\n",
      "Epoch 4/20  Iteration 648/3560 Training loss: 1.8220 18.6888 sec/batch\n",
      "Epoch 4/20  Iteration 649/3560 Training loss: 1.8214 17.6100 sec/batch\n",
      "Epoch 4/20  Iteration 650/3560 Training loss: 1.8206 18.2441 sec/batch\n",
      "Epoch 4/20  Iteration 651/3560 Training loss: 1.8201 18.0739 sec/batch\n",
      "Epoch 4/20  Iteration 652/3560 Training loss: 1.8197 16.9516 sec/batch\n",
      "Epoch 4/20  Iteration 653/3560 Training loss: 1.8192 14.9252 sec/batch\n",
      "Epoch 4/20  Iteration 654/3560 Training loss: 1.8188 17.5529 sec/batch\n",
      "Epoch 4/20  Iteration 655/3560 Training loss: 1.8185 15.8870 sec/batch\n",
      "Epoch 4/20  Iteration 656/3560 Training loss: 1.8178 22.1830 sec/batch\n",
      "Epoch 4/20  Iteration 657/3560 Training loss: 1.8172 22.3219 sec/batch\n",
      "Epoch 4/20  Iteration 658/3560 Training loss: 1.8170 15.2942 sec/batch\n",
      "Epoch 4/20  Iteration 659/3560 Training loss: 1.8166 14.2716 sec/batch\n",
      "Epoch 4/20  Iteration 660/3560 Training loss: 1.8158 13.9559 sec/batch\n",
      "Epoch 4/20  Iteration 661/3560 Training loss: 1.8156 13.9612 sec/batch\n",
      "Epoch 4/20  Iteration 662/3560 Training loss: 1.8153 14.1970 sec/batch\n",
      "Epoch 4/20  Iteration 663/3560 Training loss: 1.8149 18.2509 sec/batch\n",
      "Epoch 4/20  Iteration 664/3560 Training loss: 1.8144 16.6927 sec/batch\n",
      "Epoch 4/20  Iteration 665/3560 Training loss: 1.8138 15.9439 sec/batch\n",
      "Epoch 4/20  Iteration 666/3560 Training loss: 1.8132 13.9660 sec/batch\n",
      "Epoch 4/20  Iteration 667/3560 Training loss: 1.8128 16.6140 sec/batch\n",
      "Epoch 4/20  Iteration 668/3560 Training loss: 1.8125 16.4611 sec/batch\n",
      "Epoch 4/20  Iteration 669/3560 Training loss: 1.8121 15.0986 sec/batch\n",
      "Epoch 4/20  Iteration 670/3560 Training loss: 1.8118 17.3056 sec/batch\n",
      "Epoch 4/20  Iteration 671/3560 Training loss: 1.8115 15.7221 sec/batch\n",
      "Epoch 4/20  Iteration 672/3560 Training loss: 1.8112 15.1135 sec/batch\n",
      "Epoch 4/20  Iteration 673/3560 Training loss: 1.8110 16.3090 sec/batch\n",
      "Epoch 4/20  Iteration 674/3560 Training loss: 1.8105 13.8827 sec/batch\n",
      "Epoch 4/20  Iteration 675/3560 Training loss: 1.8104 13.8788 sec/batch\n",
      "Epoch 4/20  Iteration 676/3560 Training loss: 1.8099 13.9027 sec/batch\n",
      "Epoch 4/20  Iteration 677/3560 Training loss: 1.8095 13.8559 sec/batch\n",
      "Epoch 4/20  Iteration 678/3560 Training loss: 1.8092 14.3034 sec/batch\n",
      "Epoch 4/20  Iteration 679/3560 Training loss: 1.8087 14.8363 sec/batch\n",
      "Epoch 4/20  Iteration 680/3560 Training loss: 1.8083 14.2326 sec/batch\n",
      "Epoch 4/20  Iteration 681/3560 Training loss: 1.8080 14.6044 sec/batch\n",
      "Epoch 4/20  Iteration 682/3560 Training loss: 1.8079 14.8546 sec/batch\n",
      "Epoch 4/20  Iteration 683/3560 Training loss: 1.8075 14.6808 sec/batch\n",
      "Epoch 4/20  Iteration 684/3560 Training loss: 1.8070 14.8745 sec/batch\n",
      "Epoch 4/20  Iteration 685/3560 Training loss: 1.8063 14.6963 sec/batch\n",
      "Epoch 4/20  Iteration 686/3560 Training loss: 1.8061 13.6878 sec/batch\n",
      "Epoch 4/20  Iteration 687/3560 Training loss: 1.8058 13.9289 sec/batch\n",
      "Epoch 4/20  Iteration 688/3560 Training loss: 1.8055 13.6854 sec/batch\n",
      "Epoch 4/20  Iteration 689/3560 Training loss: 1.8051 13.7024 sec/batch\n",
      "Epoch 4/20  Iteration 690/3560 Training loss: 1.8047 14.5220 sec/batch\n",
      "Epoch 4/20  Iteration 691/3560 Training loss: 1.8043 16.7073 sec/batch\n",
      "Epoch 4/20  Iteration 692/3560 Training loss: 1.8039 15.8727 sec/batch\n",
      "Epoch 4/20  Iteration 693/3560 Training loss: 1.8033 20.8900 sec/batch\n",
      "Epoch 4/20  Iteration 694/3560 Training loss: 1.8031 19.5380 sec/batch\n",
      "Epoch 4/20  Iteration 695/3560 Training loss: 1.8030 20.0541 sec/batch\n",
      "Epoch 4/20  Iteration 696/3560 Training loss: 1.8026 19.6161 sec/batch\n",
      "Epoch 4/20  Iteration 697/3560 Training loss: 1.8023 19.9707 sec/batch\n",
      "Epoch 4/20  Iteration 698/3560 Training loss: 1.8020 19.3739 sec/batch\n",
      "Epoch 4/20  Iteration 699/3560 Training loss: 1.8016 19.4473 sec/batch\n",
      "Epoch 4/20  Iteration 700/3560 Training loss: 1.8012 20.9456 sec/batch\n",
      "Epoch 4/20  Iteration 701/3560 Training loss: 1.8009 19.5801 sec/batch\n",
      "Epoch 4/20  Iteration 702/3560 Training loss: 1.8010 17.0971 sec/batch\n",
      "Epoch 4/20  Iteration 703/3560 Training loss: 1.8006 16.3126 sec/batch\n",
      "Epoch 4/20  Iteration 704/3560 Training loss: 1.8002 17.2840 sec/batch\n",
      "Epoch 4/20  Iteration 705/3560 Training loss: 1.7997 17.0451 sec/batch\n",
      "Epoch 4/20  Iteration 706/3560 Training loss: 1.7993 17.3065 sec/batch\n",
      "Epoch 4/20  Iteration 707/3560 Training loss: 1.7990 17.3594 sec/batch\n",
      "Epoch 4/20  Iteration 708/3560 Training loss: 1.7987 17.4222 sec/batch\n",
      "Epoch 4/20  Iteration 709/3560 Training loss: 1.7984 16.7194 sec/batch\n",
      "Epoch 4/20  Iteration 710/3560 Training loss: 1.7980 16.8755 sec/batch\n",
      "Epoch 4/20  Iteration 711/3560 Training loss: 1.7975 17.3018 sec/batch\n",
      "Epoch 4/20  Iteration 712/3560 Training loss: 1.7973 17.4144 sec/batch\n",
      "Epoch 5/20  Iteration 713/3560 Training loss: 1.8080 17.8497 sec/batch\n",
      "Epoch 5/20  Iteration 714/3560 Training loss: 1.7692 16.7730 sec/batch\n",
      "Epoch 5/20  Iteration 715/3560 Training loss: 1.7522 17.7358 sec/batch\n",
      "Epoch 5/20  Iteration 716/3560 Training loss: 1.7463 17.0281 sec/batch\n",
      "Epoch 5/20  Iteration 717/3560 Training loss: 1.7412 17.8695 sec/batch\n",
      "Epoch 5/20  Iteration 718/3560 Training loss: 1.7310 16.9535 sec/batch\n",
      "Epoch 5/20  Iteration 719/3560 Training loss: 1.7296 18.9971 sec/batch\n",
      "Epoch 5/20  Iteration 720/3560 Training loss: 1.7272 19.9226 sec/batch\n",
      "Epoch 5/20  Iteration 721/3560 Training loss: 1.7290 19.5958 sec/batch\n",
      "Epoch 5/20  Iteration 722/3560 Training loss: 1.7279 19.7859 sec/batch\n",
      "Epoch 5/20  Iteration 723/3560 Training loss: 1.7239 19.8426 sec/batch\n",
      "Epoch 5/20  Iteration 724/3560 Training loss: 1.7218 19.5450 sec/batch\n",
      "Epoch 5/20  Iteration 725/3560 Training loss: 1.7215 19.3761 sec/batch\n",
      "Epoch 5/20  Iteration 726/3560 Training loss: 1.7242 20.0652 sec/batch\n",
      "Epoch 5/20  Iteration 727/3560 Training loss: 1.7230 19.6689 sec/batch\n",
      "Epoch 5/20  Iteration 728/3560 Training loss: 1.7209 17.9135 sec/batch\n",
      "Epoch 5/20  Iteration 729/3560 Training loss: 1.7204 16.9356 sec/batch\n",
      "Epoch 5/20  Iteration 730/3560 Training loss: 1.7221 16.8726 sec/batch\n",
      "Epoch 5/20  Iteration 731/3560 Training loss: 1.7220 17.4617 sec/batch\n",
      "Epoch 5/20  Iteration 732/3560 Training loss: 1.7226 17.7584 sec/batch\n",
      "Epoch 5/20  Iteration 733/3560 Training loss: 1.7223 19.2318 sec/batch\n",
      "Epoch 5/20  Iteration 734/3560 Training loss: 1.7234 16.8192 sec/batch\n",
      "Epoch 5/20  Iteration 735/3560 Training loss: 1.7223 17.7481 sec/batch\n",
      "Epoch 5/20  Iteration 736/3560 Training loss: 1.7221 17.4390 sec/batch\n",
      "Epoch 5/20  Iteration 737/3560 Training loss: 1.7218 17.7261 sec/batch\n",
      "Epoch 5/20  Iteration 738/3560 Training loss: 1.7207 16.9144 sec/batch\n",
      "Epoch 5/20  Iteration 739/3560 Training loss: 1.7196 16.7323 sec/batch\n",
      "Epoch 5/20  Iteration 740/3560 Training loss: 1.7196 17.1382 sec/batch\n",
      "Epoch 5/20  Iteration 741/3560 Training loss: 1.7202 17.3620 sec/batch\n",
      "Epoch 5/20  Iteration 742/3560 Training loss: 1.7203 16.6332 sec/batch\n",
      "Epoch 5/20  Iteration 743/3560 Training loss: 1.7197 16.6638 sec/batch\n",
      "Epoch 5/20  Iteration 744/3560 Training loss: 1.7188 17.5322 sec/batch\n",
      "Epoch 5/20  Iteration 745/3560 Training loss: 1.7193 17.4858 sec/batch\n",
      "Epoch 5/20  Iteration 746/3560 Training loss: 1.7196 19.5050 sec/batch\n",
      "Epoch 5/20  Iteration 747/3560 Training loss: 1.7192 19.8049 sec/batch\n",
      "Epoch 5/20  Iteration 748/3560 Training loss: 1.7186 20.1853 sec/batch\n",
      "Epoch 5/20  Iteration 749/3560 Training loss: 1.7177 19.7199 sec/batch\n",
      "Epoch 5/20  Iteration 750/3560 Training loss: 1.7165 20.3573 sec/batch\n",
      "Epoch 5/20  Iteration 751/3560 Training loss: 1.7147 19.3812 sec/batch\n",
      "Epoch 5/20  Iteration 752/3560 Training loss: 1.7138 19.7230 sec/batch\n",
      "Epoch 5/20  Iteration 753/3560 Training loss: 1.7132 19.4756 sec/batch\n",
      "Epoch 5/20  Iteration 754/3560 Training loss: 1.7135 19.1474 sec/batch\n",
      "Epoch 5/20  Iteration 755/3560 Training loss: 1.7127 17.0023 sec/batch\n",
      "Epoch 5/20  Iteration 756/3560 Training loss: 1.7117 17.3725 sec/batch\n",
      "Epoch 5/20  Iteration 757/3560 Training loss: 1.7117 17.2150 sec/batch\n",
      "Epoch 5/20  Iteration 758/3560 Training loss: 1.7103 16.8243 sec/batch\n",
      "Epoch 5/20  Iteration 759/3560 Training loss: 1.7098 16.8000 sec/batch\n",
      "Epoch 5/20  Iteration 760/3560 Training loss: 1.7088 16.8209 sec/batch\n",
      "Epoch 5/20  Iteration 761/3560 Training loss: 1.7082 17.2541 sec/batch\n",
      "Epoch 5/20  Iteration 762/3560 Training loss: 1.7086 16.9835 sec/batch\n",
      "Epoch 5/20  Iteration 763/3560 Training loss: 1.7078 17.6066 sec/batch\n",
      "Epoch 5/20  Iteration 764/3560 Training loss: 1.7084 18.0704 sec/batch\n",
      "Epoch 5/20  Iteration 765/3560 Training loss: 1.7079 16.8886 sec/batch\n",
      "Epoch 5/20  Iteration 766/3560 Training loss: 1.7078 17.5489 sec/batch\n",
      "Epoch 5/20  Iteration 767/3560 Training loss: 1.7075 17.7446 sec/batch\n",
      "Epoch 5/20  Iteration 768/3560 Training loss: 1.7073 16.9792 sec/batch\n",
      "Epoch 5/20  Iteration 769/3560 Training loss: 1.7074 16.6611 sec/batch\n",
      "Epoch 5/20  Iteration 770/3560 Training loss: 1.7069 16.9610 sec/batch\n",
      "Epoch 5/20  Iteration 771/3560 Training loss: 1.7063 17.2217 sec/batch\n",
      "Epoch 5/20  Iteration 772/3560 Training loss: 1.7069 19.4333 sec/batch\n",
      "Epoch 5/20  Iteration 773/3560 Training loss: 1.7065 20.2542 sec/batch\n",
      "Epoch 5/20  Iteration 774/3560 Training loss: 1.7071 20.1711 sec/batch\n",
      "Epoch 5/20  Iteration 775/3560 Training loss: 1.7073 19.9054 sec/batch\n",
      "Epoch 5/20  Iteration 776/3560 Training loss: 1.7075 19.8255 sec/batch\n",
      "Epoch 5/20  Iteration 777/3560 Training loss: 1.7071 20.0432 sec/batch\n",
      "Epoch 5/20  Iteration 778/3560 Training loss: 1.7072 20.0192 sec/batch\n",
      "Epoch 5/20  Iteration 779/3560 Training loss: 1.7072 20.0494 sec/batch\n",
      "Epoch 5/20  Iteration 780/3560 Training loss: 1.7066 19.7872 sec/batch\n",
      "Epoch 5/20  Iteration 781/3560 Training loss: 1.7063 18.1440 sec/batch\n",
      "Epoch 5/20  Iteration 782/3560 Training loss: 1.7059 17.3481 sec/batch\n",
      "Epoch 5/20  Iteration 783/3560 Training loss: 1.7061 17.2553 sec/batch\n",
      "Epoch 5/20  Iteration 784/3560 Training loss: 1.7060 17.0687 sec/batch\n",
      "Epoch 5/20  Iteration 785/3560 Training loss: 1.7061 17.2797 sec/batch\n",
      "Epoch 5/20  Iteration 786/3560 Training loss: 1.7056 17.9045 sec/batch\n",
      "Epoch 5/20  Iteration 787/3560 Training loss: 1.7053 17.6521 sec/batch\n",
      "Epoch 5/20  Iteration 788/3560 Training loss: 1.7052 17.4989 sec/batch\n",
      "Epoch 5/20  Iteration 789/3560 Training loss: 1.7047 16.9358 sec/batch\n",
      "Epoch 5/20  Iteration 790/3560 Training loss: 1.7044 17.2948 sec/batch\n",
      "Epoch 5/20  Iteration 791/3560 Training loss: 1.7036 17.1413 sec/batch\n",
      "Epoch 5/20  Iteration 792/3560 Training loss: 1.7031 17.5649 sec/batch\n",
      "Epoch 5/20  Iteration 793/3560 Training loss: 1.7024 16.6323 sec/batch\n",
      "Epoch 5/20  Iteration 794/3560 Training loss: 1.7022 16.7070 sec/batch\n",
      "Epoch 5/20  Iteration 795/3560 Training loss: 1.7014 16.9341 sec/batch\n",
      "Epoch 5/20  Iteration 796/3560 Training loss: 1.7011 17.7482 sec/batch\n",
      "Epoch 5/20  Iteration 797/3560 Training loss: 1.7005 17.5091 sec/batch\n",
      "Epoch 5/20  Iteration 798/3560 Training loss: 1.6998 17.3558 sec/batch\n",
      "Epoch 5/20  Iteration 799/3560 Training loss: 1.6993 19.0327 sec/batch\n",
      "Epoch 5/20  Iteration 800/3560 Training loss: 1.6987 21.4576 sec/batch\n",
      "Validation loss: 1.55625 Saving checkpoint!\n",
      "Epoch 5/20  Iteration 801/3560 Training loss: 1.6988 17.0713 sec/batch\n",
      "Epoch 5/20  Iteration 802/3560 Training loss: 1.6986 16.4760 sec/batch\n",
      "Epoch 5/20  Iteration 803/3560 Training loss: 1.6982 16.5378 sec/batch\n",
      "Epoch 5/20  Iteration 804/3560 Training loss: 1.6977 17.7000 sec/batch\n",
      "Epoch 5/20  Iteration 805/3560 Training loss: 1.6971 16.4475 sec/batch\n",
      "Epoch 5/20  Iteration 806/3560 Training loss: 1.6965 16.2656 sec/batch\n",
      "Epoch 5/20  Iteration 807/3560 Training loss: 1.6959 16.7697 sec/batch\n",
      "Epoch 5/20  Iteration 808/3560 Training loss: 1.6956 17.4559 sec/batch\n",
      "Epoch 5/20  Iteration 809/3560 Training loss: 1.6953 16.9294 sec/batch\n",
      "Epoch 5/20  Iteration 810/3560 Training loss: 1.6946 16.4604 sec/batch\n",
      "Epoch 5/20  Iteration 811/3560 Training loss: 1.6940 17.3877 sec/batch\n",
      "Epoch 5/20  Iteration 812/3560 Training loss: 1.6931 16.4215 sec/batch\n",
      "Epoch 5/20  Iteration 813/3560 Training loss: 1.6928 17.3545 sec/batch\n",
      "Epoch 5/20  Iteration 814/3560 Training loss: 1.6923 16.8943 sec/batch\n",
      "Epoch 5/20  Iteration 815/3560 Training loss: 1.6918 16.7749 sec/batch\n",
      "Epoch 5/20  Iteration 816/3560 Training loss: 1.6913 16.9804 sec/batch\n",
      "Epoch 5/20  Iteration 817/3560 Training loss: 1.6908 16.8199 sec/batch\n",
      "Epoch 5/20  Iteration 818/3560 Training loss: 1.6905 19.2294 sec/batch\n",
      "Epoch 5/20  Iteration 819/3560 Training loss: 1.6900 18.9987 sec/batch\n",
      "Epoch 5/20  Iteration 820/3560 Training loss: 1.6896 19.9953 sec/batch\n",
      "Epoch 5/20  Iteration 821/3560 Training loss: 1.6893 19.6793 sec/batch\n",
      "Epoch 5/20  Iteration 822/3560 Training loss: 1.6891 20.3663 sec/batch\n",
      "Epoch 5/20  Iteration 823/3560 Training loss: 1.6887 19.1716 sec/batch\n",
      "Epoch 5/20  Iteration 824/3560 Training loss: 1.6883 19.7995 sec/batch\n",
      "Epoch 5/20  Iteration 825/3560 Training loss: 1.6879 20.3453 sec/batch\n",
      "Epoch 5/20  Iteration 826/3560 Training loss: 1.6875 19.6825 sec/batch\n",
      "Epoch 5/20  Iteration 827/3560 Training loss: 1.6869 17.7129 sec/batch\n",
      "Epoch 5/20  Iteration 828/3560 Training loss: 1.6863 16.6651 sec/batch\n",
      "Epoch 5/20  Iteration 829/3560 Training loss: 1.6860 16.7263 sec/batch\n",
      "Epoch 5/20  Iteration 830/3560 Training loss: 1.6857 17.2337 sec/batch\n",
      "Epoch 5/20  Iteration 831/3560 Training loss: 1.6852 16.9989 sec/batch\n",
      "Epoch 5/20  Iteration 832/3560 Training loss: 1.6849 16.9860 sec/batch\n",
      "Epoch 5/20  Iteration 833/3560 Training loss: 1.6846 16.8877 sec/batch\n",
      "Epoch 5/20  Iteration 834/3560 Training loss: 1.6840 16.9802 sec/batch\n",
      "Epoch 5/20  Iteration 835/3560 Training loss: 1.6834 16.3582 sec/batch\n",
      "Epoch 5/20  Iteration 836/3560 Training loss: 1.6832 16.8806 sec/batch\n",
      "Epoch 5/20  Iteration 837/3560 Training loss: 1.6829 16.8045 sec/batch\n",
      "Epoch 5/20  Iteration 838/3560 Training loss: 1.6823 16.3323 sec/batch\n",
      "Epoch 5/20  Iteration 839/3560 Training loss: 1.6821 17.6285 sec/batch\n",
      "Epoch 5/20  Iteration 840/3560 Training loss: 1.6819 16.3445 sec/batch\n",
      "Epoch 5/20  Iteration 841/3560 Training loss: 1.6816 16.9273 sec/batch\n",
      "Epoch 5/20  Iteration 842/3560 Training loss: 1.6811 16.7838 sec/batch\n",
      "Epoch 5/20  Iteration 843/3560 Training loss: 1.6805 17.2506 sec/batch\n",
      "Epoch 5/20  Iteration 844/3560 Training loss: 1.6800 15.9537 sec/batch\n",
      "Epoch 5/20  Iteration 845/3560 Training loss: 1.6798 19.9360 sec/batch\n",
      "Epoch 5/20  Iteration 846/3560 Training loss: 1.6796 19.4849 sec/batch\n",
      "Epoch 5/20  Iteration 847/3560 Training loss: 1.6793 19.7435 sec/batch\n",
      "Epoch 5/20  Iteration 848/3560 Training loss: 1.6791 19.5007 sec/batch\n",
      "Epoch 5/20  Iteration 849/3560 Training loss: 1.6790 19.8387 sec/batch\n",
      "Epoch 5/20  Iteration 850/3560 Training loss: 1.6787 19.3752 sec/batch\n",
      "Epoch 5/20  Iteration 851/3560 Training loss: 1.6786 20.6315 sec/batch\n",
      "Epoch 5/20  Iteration 852/3560 Training loss: 1.6783 18.7833 sec/batch\n",
      "Epoch 5/20  Iteration 853/3560 Training loss: 1.6784 19.5827 sec/batch\n",
      "Epoch 5/20  Iteration 854/3560 Training loss: 1.6781 17.7241 sec/batch\n",
      "Epoch 5/20  Iteration 855/3560 Training loss: 1.6778 16.6615 sec/batch\n",
      "Epoch 5/20  Iteration 856/3560 Training loss: 1.6776 16.5117 sec/batch\n",
      "Epoch 5/20  Iteration 857/3560 Training loss: 1.6772 16.8982 sec/batch\n",
      "Epoch 5/20  Iteration 858/3560 Training loss: 1.6771 17.2138 sec/batch\n",
      "Epoch 5/20  Iteration 859/3560 Training loss: 1.6770 17.7518 sec/batch\n",
      "Epoch 5/20  Iteration 860/3560 Training loss: 1.6769 16.9127 sec/batch\n",
      "Epoch 5/20  Iteration 861/3560 Training loss: 1.6767 16.5961 sec/batch\n",
      "Epoch 5/20  Iteration 862/3560 Training loss: 1.6763 17.0368 sec/batch\n",
      "Epoch 5/20  Iteration 863/3560 Training loss: 1.6758 16.4711 sec/batch\n",
      "Epoch 5/20  Iteration 864/3560 Training loss: 1.6756 16.3013 sec/batch\n",
      "Epoch 5/20  Iteration 865/3560 Training loss: 1.6754 16.5260 sec/batch\n",
      "Epoch 5/20  Iteration 866/3560 Training loss: 1.6752 16.9944 sec/batch\n",
      "Epoch 5/20  Iteration 867/3560 Training loss: 1.6748 17.0322 sec/batch\n",
      "Epoch 5/20  Iteration 868/3560 Training loss: 1.6746 16.7295 sec/batch\n",
      "Epoch 5/20  Iteration 869/3560 Training loss: 1.6743 17.5268 sec/batch\n",
      "Epoch 5/20  Iteration 870/3560 Training loss: 1.6741 16.8000 sec/batch\n",
      "Epoch 5/20  Iteration 871/3560 Training loss: 1.6736 16.7886 sec/batch\n",
      "Epoch 5/20  Iteration 872/3560 Training loss: 1.6734 18.8157 sec/batch\n",
      "Epoch 5/20  Iteration 873/3560 Training loss: 1.6734 18.9597 sec/batch\n",
      "Epoch 5/20  Iteration 874/3560 Training loss: 1.6731 19.7131 sec/batch\n",
      "Epoch 5/20  Iteration 875/3560 Training loss: 1.6729 19.9863 sec/batch\n",
      "Epoch 5/20  Iteration 876/3560 Training loss: 1.6726 22.2355 sec/batch\n",
      "Epoch 5/20  Iteration 877/3560 Training loss: 1.6723 21.1455 sec/batch\n",
      "Epoch 5/20  Iteration 878/3560 Training loss: 1.6720 19.2064 sec/batch\n",
      "Epoch 5/20  Iteration 879/3560 Training loss: 1.6718 19.5962 sec/batch\n",
      "Epoch 5/20  Iteration 880/3560 Training loss: 1.6720 19.0620 sec/batch\n",
      "Epoch 5/20  Iteration 881/3560 Training loss: 1.6717 17.2328 sec/batch\n",
      "Epoch 5/20  Iteration 882/3560 Training loss: 1.6714 16.9425 sec/batch\n",
      "Epoch 5/20  Iteration 883/3560 Training loss: 1.6711 16.5754 sec/batch\n",
      "Epoch 5/20  Iteration 884/3560 Training loss: 1.6707 17.0263 sec/batch\n",
      "Epoch 5/20  Iteration 885/3560 Training loss: 1.6705 16.5087 sec/batch\n",
      "Epoch 5/20  Iteration 886/3560 Training loss: 1.6703 17.2340 sec/batch\n",
      "Epoch 5/20  Iteration 887/3560 Training loss: 1.6701 16.4336 sec/batch\n",
      "Epoch 5/20  Iteration 888/3560 Training loss: 1.6698 16.6044 sec/batch\n",
      "Epoch 5/20  Iteration 889/3560 Training loss: 1.6694 17.0283 sec/batch\n",
      "Epoch 5/20  Iteration 890/3560 Training loss: 1.6693 16.7248 sec/batch\n",
      "Epoch 6/20  Iteration 891/3560 Training loss: 1.7174 16.8522 sec/batch\n",
      "Epoch 6/20  Iteration 892/3560 Training loss: 1.6679 16.6170 sec/batch\n",
      "Epoch 6/20  Iteration 893/3560 Training loss: 1.6510 16.3984 sec/batch\n",
      "Epoch 6/20  Iteration 894/3560 Training loss: 1.6432 16.5314 sec/batch\n",
      "Epoch 6/20  Iteration 895/3560 Training loss: 1.6381 17.0646 sec/batch\n",
      "Epoch 6/20  Iteration 896/3560 Training loss: 1.6255 17.9196 sec/batch\n",
      "Epoch 6/20  Iteration 897/3560 Training loss: 1.6251 17.3672 sec/batch\n",
      "Epoch 6/20  Iteration 898/3560 Training loss: 1.6228 17.5132 sec/batch\n",
      "Epoch 6/20  Iteration 899/3560 Training loss: 1.6229 20.3733 sec/batch\n",
      "Epoch 6/20  Iteration 900/3560 Training loss: 1.6216 20.1061 sec/batch\n",
      "Epoch 6/20  Iteration 901/3560 Training loss: 1.6183 20.8485 sec/batch\n",
      "Epoch 6/20  Iteration 902/3560 Training loss: 1.6168 20.2315 sec/batch\n",
      "Epoch 6/20  Iteration 903/3560 Training loss: 1.6163 20.9739 sec/batch\n",
      "Epoch 6/20  Iteration 904/3560 Training loss: 1.6187 20.0576 sec/batch\n",
      "Epoch 6/20  Iteration 905/3560 Training loss: 1.6170 20.8654 sec/batch\n",
      "Epoch 6/20  Iteration 906/3560 Training loss: 1.6157 19.9686 sec/batch\n",
      "Epoch 6/20  Iteration 907/3560 Training loss: 1.6163 18.1722 sec/batch\n",
      "Epoch 6/20  Iteration 908/3560 Training loss: 1.6182 17.0873 sec/batch\n",
      "Epoch 6/20  Iteration 909/3560 Training loss: 1.6187 18.3027 sec/batch\n",
      "Epoch 6/20  Iteration 910/3560 Training loss: 1.6193 17.0412 sec/batch\n",
      "Epoch 6/20  Iteration 911/3560 Training loss: 1.6184 17.8231 sec/batch\n",
      "Epoch 6/20  Iteration 912/3560 Training loss: 1.6189 16.3869 sec/batch\n",
      "Epoch 6/20  Iteration 913/3560 Training loss: 1.6176 17.1824 sec/batch\n",
      "Epoch 6/20  Iteration 914/3560 Training loss: 1.6172 16.9329 sec/batch\n",
      "Epoch 6/20  Iteration 915/3560 Training loss: 1.6168 16.8363 sec/batch\n",
      "Epoch 6/20  Iteration 916/3560 Training loss: 1.6149 17.0056 sec/batch\n",
      "Epoch 6/20  Iteration 917/3560 Training loss: 1.6134 17.6778 sec/batch\n",
      "Epoch 6/20  Iteration 918/3560 Training loss: 1.6138 16.7286 sec/batch\n",
      "Epoch 6/20  Iteration 919/3560 Training loss: 1.6139 16.7445 sec/batch\n",
      "Epoch 6/20  Iteration 920/3560 Training loss: 1.6141 17.6634 sec/batch\n",
      "Epoch 6/20  Iteration 921/3560 Training loss: 1.6137 17.4093 sec/batch\n",
      "Epoch 6/20  Iteration 922/3560 Training loss: 1.6124 16.6634 sec/batch\n",
      "Epoch 6/20  Iteration 923/3560 Training loss: 1.6126 17.0520 sec/batch\n",
      "Epoch 6/20  Iteration 924/3560 Training loss: 1.6129 16.7032 sec/batch\n",
      "Epoch 6/20  Iteration 925/3560 Training loss: 1.6126 19.5616 sec/batch\n",
      "Epoch 6/20  Iteration 926/3560 Training loss: 1.6122 21.8717 sec/batch\n",
      "Epoch 6/20  Iteration 927/3560 Training loss: 1.6114 20.2194 sec/batch\n",
      "Epoch 6/20  Iteration 928/3560 Training loss: 1.6102 20.4000 sec/batch\n",
      "Epoch 6/20  Iteration 929/3560 Training loss: 1.6084 20.2847 sec/batch\n",
      "Epoch 6/20  Iteration 930/3560 Training loss: 1.6076 19.6187 sec/batch\n",
      "Epoch 6/20  Iteration 931/3560 Training loss: 1.6068 20.7719 sec/batch\n",
      "Epoch 6/20  Iteration 932/3560 Training loss: 1.6071 20.4064 sec/batch\n",
      "Epoch 6/20  Iteration 933/3560 Training loss: 1.6062 19.9730 sec/batch\n",
      "Epoch 6/20  Iteration 934/3560 Training loss: 1.6050 17.2477 sec/batch\n",
      "Epoch 6/20  Iteration 935/3560 Training loss: 1.6051 17.1695 sec/batch\n",
      "Epoch 6/20  Iteration 936/3560 Training loss: 1.6038 15.9352 sec/batch\n",
      "Epoch 6/20  Iteration 937/3560 Training loss: 1.6033 17.1085 sec/batch\n",
      "Epoch 6/20  Iteration 938/3560 Training loss: 1.6025 16.6865 sec/batch\n",
      "Epoch 6/20  Iteration 939/3560 Training loss: 1.6022 17.7091 sec/batch\n",
      "Epoch 6/20  Iteration 940/3560 Training loss: 1.6026 16.9600 sec/batch\n",
      "Epoch 6/20  Iteration 941/3560 Training loss: 1.6021 17.0263 sec/batch\n",
      "Epoch 6/20  Iteration 942/3560 Training loss: 1.6027 17.2062 sec/batch\n",
      "Epoch 6/20  Iteration 943/3560 Training loss: 1.6025 16.8615 sec/batch\n",
      "Epoch 6/20  Iteration 944/3560 Training loss: 1.6025 16.8882 sec/batch\n",
      "Epoch 6/20  Iteration 945/3560 Training loss: 1.6020 16.9782 sec/batch\n",
      "Epoch 6/20  Iteration 946/3560 Training loss: 1.6019 17.1122 sec/batch\n",
      "Epoch 6/20  Iteration 947/3560 Training loss: 1.6022 16.4173 sec/batch\n",
      "Epoch 6/20  Iteration 948/3560 Training loss: 1.6018 16.5676 sec/batch\n",
      "Epoch 6/20  Iteration 949/3560 Training loss: 1.6011 16.8131 sec/batch\n",
      "Epoch 6/20  Iteration 950/3560 Training loss: 1.6015 18.1431 sec/batch\n",
      "Epoch 6/20  Iteration 951/3560 Training loss: 1.6013 18.1434 sec/batch\n",
      "Epoch 6/20  Iteration 952/3560 Training loss: 1.6020 19.7943 sec/batch\n",
      "Epoch 6/20  Iteration 953/3560 Training loss: 1.6022 20.5144 sec/batch\n",
      "Epoch 6/20  Iteration 954/3560 Training loss: 1.6022 20.3961 sec/batch\n",
      "Epoch 6/20  Iteration 955/3560 Training loss: 1.6019 20.4939 sec/batch\n",
      "Epoch 6/20  Iteration 956/3560 Training loss: 1.6019 19.8990 sec/batch\n",
      "Epoch 6/20  Iteration 957/3560 Training loss: 1.6021 20.2740 sec/batch\n",
      "Epoch 6/20  Iteration 958/3560 Training loss: 1.6016 20.8334 sec/batch\n",
      "Epoch 6/20  Iteration 959/3560 Training loss: 1.6015 21.0403 sec/batch\n",
      "Epoch 6/20  Iteration 960/3560 Training loss: 1.6011 18.7128 sec/batch\n",
      "Epoch 6/20  Iteration 961/3560 Training loss: 1.6013 17.9308 sec/batch\n",
      "Epoch 6/20  Iteration 962/3560 Training loss: 1.6013 16.9990 sec/batch\n",
      "Epoch 6/20  Iteration 963/3560 Training loss: 1.6016 17.7794 sec/batch\n",
      "Epoch 6/20  Iteration 964/3560 Training loss: 1.6011 16.9627 sec/batch\n",
      "Epoch 6/20  Iteration 965/3560 Training loss: 1.6007 16.3589 sec/batch\n",
      "Epoch 6/20  Iteration 966/3560 Training loss: 1.6007 16.7845 sec/batch\n",
      "Epoch 6/20  Iteration 967/3560 Training loss: 1.6003 16.7607 sec/batch\n",
      "Epoch 6/20  Iteration 968/3560 Training loss: 1.6002 16.3540 sec/batch\n",
      "Epoch 6/20  Iteration 969/3560 Training loss: 1.5993 16.5061 sec/batch\n",
      "Epoch 6/20  Iteration 970/3560 Training loss: 1.5991 16.6855 sec/batch\n",
      "Epoch 6/20  Iteration 971/3560 Training loss: 1.5984 17.0769 sec/batch\n",
      "Epoch 6/20  Iteration 972/3560 Training loss: 1.5983 16.8347 sec/batch\n",
      "Epoch 6/20  Iteration 973/3560 Training loss: 1.5975 16.7488 sec/batch\n",
      "Epoch 6/20  Iteration 974/3560 Training loss: 1.5973 16.2806 sec/batch\n",
      "Epoch 6/20  Iteration 975/3560 Training loss: 1.5969 16.7628 sec/batch\n",
      "Epoch 6/20  Iteration 976/3560 Training loss: 1.5965 17.5610 sec/batch\n",
      "Epoch 6/20  Iteration 977/3560 Training loss: 1.5960 16.9330 sec/batch\n",
      "Epoch 6/20  Iteration 978/3560 Training loss: 1.5956 19.3934 sec/batch\n",
      "Epoch 6/20  Iteration 979/3560 Training loss: 1.5950 19.7430 sec/batch\n",
      "Epoch 6/20  Iteration 980/3560 Training loss: 1.5949 19.8817 sec/batch\n",
      "Epoch 6/20  Iteration 981/3560 Training loss: 1.5944 19.9414 sec/batch\n",
      "Epoch 6/20  Iteration 982/3560 Training loss: 1.5940 20.3496 sec/batch\n",
      "Epoch 6/20  Iteration 983/3560 Training loss: 1.5934 20.1258 sec/batch\n",
      "Epoch 6/20  Iteration 984/3560 Training loss: 1.5928 19.8188 sec/batch\n",
      "Epoch 6/20  Iteration 985/3560 Training loss: 1.5925 20.0868 sec/batch\n",
      "Epoch 6/20  Iteration 986/3560 Training loss: 1.5922 19.4968 sec/batch\n",
      "Epoch 6/20  Iteration 987/3560 Training loss: 1.5919 17.7657 sec/batch\n",
      "Epoch 6/20  Iteration 988/3560 Training loss: 1.5914 16.3213 sec/batch\n",
      "Epoch 6/20  Iteration 989/3560 Training loss: 1.5908 16.8523 sec/batch\n",
      "Epoch 6/20  Iteration 990/3560 Training loss: 1.5902 16.8198 sec/batch\n",
      "Epoch 6/20  Iteration 991/3560 Training loss: 1.5900 17.2255 sec/batch\n",
      "Epoch 6/20  Iteration 992/3560 Training loss: 1.5897 18.3255 sec/batch\n",
      "Epoch 6/20  Iteration 993/3560 Training loss: 1.5894 17.3601 sec/batch\n",
      "Epoch 6/20  Iteration 994/3560 Training loss: 1.5891 17.0171 sec/batch\n",
      "Epoch 6/20  Iteration 995/3560 Training loss: 1.5887 17.1538 sec/batch\n",
      "Epoch 6/20  Iteration 996/3560 Training loss: 1.5885 16.6824 sec/batch\n",
      "Epoch 6/20  Iteration 997/3560 Training loss: 1.5882 16.7812 sec/batch\n",
      "Epoch 6/20  Iteration 998/3560 Training loss: 1.5879 17.1493 sec/batch\n",
      "Epoch 6/20  Iteration 999/3560 Training loss: 1.5877 16.9567 sec/batch\n",
      "Epoch 6/20  Iteration 1000/3560 Training loss: 1.5875 16.6221 sec/batch\n",
      "Validation loss: 1.4476 Saving checkpoint!\n",
      "Epoch 6/20  Iteration 1001/3560 Training loss: 1.5879 19.5979 sec/batch\n",
      "Epoch 6/20  Iteration 1002/3560 Training loss: 1.5876 20.2486 sec/batch\n",
      "Epoch 6/20  Iteration 1003/3560 Training loss: 1.5874 19.7986 sec/batch\n",
      "Epoch 6/20  Iteration 1004/3560 Training loss: 1.5871 19.5297 sec/batch\n",
      "Epoch 6/20  Iteration 1005/3560 Training loss: 1.5867 17.2767 sec/batch\n",
      "Epoch 6/20  Iteration 1006/3560 Training loss: 1.5862 16.9033 sec/batch\n",
      "Epoch 6/20  Iteration 1007/3560 Training loss: 1.5860 17.2012 sec/batch\n",
      "Epoch 6/20  Iteration 1008/3560 Training loss: 1.5857 16.7208 sec/batch\n",
      "Epoch 6/20  Iteration 1009/3560 Training loss: 1.5854 16.7268 sec/batch\n",
      "Epoch 6/20  Iteration 1010/3560 Training loss: 1.5851 16.9858 sec/batch\n",
      "Epoch 6/20  Iteration 1011/3560 Training loss: 1.5849 16.7870 sec/batch\n",
      "Epoch 6/20  Iteration 1012/3560 Training loss: 1.5843 17.2230 sec/batch\n",
      "Epoch 6/20  Iteration 1013/3560 Training loss: 1.5837 17.3035 sec/batch\n",
      "Epoch 6/20  Iteration 1014/3560 Training loss: 1.5835 16.7968 sec/batch\n",
      "Epoch 6/20  Iteration 1015/3560 Training loss: 1.5833 17.0811 sec/batch\n",
      "Epoch 6/20  Iteration 1016/3560 Training loss: 1.5828 17.1521 sec/batch\n",
      "Epoch 6/20  Iteration 1017/3560 Training loss: 1.5827 18.4510 sec/batch\n",
      "Epoch 6/20  Iteration 1018/3560 Training loss: 1.5825 16.9379 sec/batch\n",
      "Epoch 6/20  Iteration 1019/3560 Training loss: 1.5822 17.1025 sec/batch\n",
      "Epoch 6/20  Iteration 1020/3560 Training loss: 1.5818 16.9904 sec/batch\n",
      "Epoch 6/20  Iteration 1021/3560 Training loss: 1.5812 17.1278 sec/batch\n",
      "Epoch 6/20  Iteration 1022/3560 Training loss: 1.5807 18.5384 sec/batch\n",
      "Epoch 6/20  Iteration 1023/3560 Training loss: 1.5806 20.0344 sec/batch\n",
      "Epoch 6/20  Iteration 1024/3560 Training loss: 1.5805 20.5158 sec/batch\n",
      "Epoch 6/20  Iteration 1025/3560 Training loss: 1.5803 20.7067 sec/batch\n",
      "Epoch 6/20  Iteration 1026/3560 Training loss: 1.5801 20.5464 sec/batch\n",
      "Epoch 6/20  Iteration 1027/3560 Training loss: 1.5801 20.0799 sec/batch\n",
      "Epoch 6/20  Iteration 1028/3560 Training loss: 1.5799 20.6249 sec/batch\n",
      "Epoch 6/20  Iteration 1029/3560 Training loss: 1.5798 19.9290 sec/batch\n",
      "Epoch 6/20  Iteration 1030/3560 Training loss: 1.5795 19.6337 sec/batch\n",
      "Epoch 6/20  Iteration 1031/3560 Training loss: 1.5797 17.8605 sec/batch\n",
      "Epoch 6/20  Iteration 1032/3560 Training loss: 1.5794 18.1568 sec/batch\n",
      "Epoch 6/20  Iteration 1033/3560 Training loss: 1.5791 16.8585 sec/batch\n",
      "Epoch 6/20  Iteration 1034/3560 Training loss: 1.5791 17.1585 sec/batch\n",
      "Epoch 6/20  Iteration 1035/3560 Training loss: 1.5787 17.1505 sec/batch\n",
      "Epoch 6/20  Iteration 1036/3560 Training loss: 1.5786 17.6290 sec/batch\n",
      "Epoch 6/20  Iteration 1037/3560 Training loss: 1.5783 17.2542 sec/batch\n",
      "Epoch 6/20  Iteration 1038/3560 Training loss: 1.5784 16.8042 sec/batch\n",
      "Epoch 6/20  Iteration 1039/3560 Training loss: 1.5782 16.4243 sec/batch\n",
      "Epoch 6/20  Iteration 1040/3560 Training loss: 1.5778 16.7341 sec/batch\n",
      "Epoch 6/20  Iteration 1041/3560 Training loss: 1.5773 17.2320 sec/batch\n",
      "Epoch 6/20  Iteration 1042/3560 Training loss: 1.5771 17.4425 sec/batch\n",
      "Epoch 6/20  Iteration 1043/3560 Training loss: 1.5769 16.5445 sec/batch\n",
      "Epoch 6/20  Iteration 1044/3560 Training loss: 1.5766 16.8359 sec/batch\n",
      "Epoch 6/20  Iteration 1045/3560 Training loss: 1.5764 17.0875 sec/batch\n",
      "Epoch 6/20  Iteration 1046/3560 Training loss: 1.5762 16.6897 sec/batch\n",
      "Epoch 6/20  Iteration 1047/3560 Training loss: 1.5760 16.9995 sec/batch\n",
      "Epoch 6/20  Iteration 1048/3560 Training loss: 1.5758 16.4239 sec/batch\n",
      "Epoch 6/20  Iteration 1049/3560 Training loss: 1.5753 20.3467 sec/batch\n",
      "Epoch 6/20  Iteration 1050/3560 Training loss: 1.5752 21.7723 sec/batch\n",
      "Epoch 6/20  Iteration 1051/3560 Training loss: 1.5753 21.1355 sec/batch\n",
      "Epoch 6/20  Iteration 1052/3560 Training loss: 1.5750 20.3150 sec/batch\n",
      "Epoch 6/20  Iteration 1053/3560 Training loss: 1.5748 19.8348 sec/batch\n",
      "Epoch 6/20  Iteration 1054/3560 Training loss: 1.5746 19.7390 sec/batch\n",
      "Epoch 6/20  Iteration 1055/3560 Training loss: 1.5743 20.3330 sec/batch\n",
      "Epoch 6/20  Iteration 1056/3560 Training loss: 1.5740 19.9992 sec/batch\n",
      "Epoch 6/20  Iteration 1057/3560 Training loss: 1.5739 20.0368 sec/batch\n",
      "Epoch 6/20  Iteration 1058/3560 Training loss: 1.5741 17.1395 sec/batch\n",
      "Epoch 6/20  Iteration 1059/3560 Training loss: 1.5739 17.0736 sec/batch\n",
      "Epoch 6/20  Iteration 1060/3560 Training loss: 1.5737 17.1821 sec/batch\n",
      "Epoch 6/20  Iteration 1061/3560 Training loss: 1.5734 16.8242 sec/batch\n",
      "Epoch 6/20  Iteration 1062/3560 Training loss: 1.5730 16.5210 sec/batch\n",
      "Epoch 6/20  Iteration 1063/3560 Training loss: 1.5729 16.8685 sec/batch\n",
      "Epoch 6/20  Iteration 1064/3560 Training loss: 1.5727 17.5097 sec/batch\n",
      "Epoch 6/20  Iteration 1065/3560 Training loss: 1.5726 16.7618 sec/batch\n",
      "Epoch 6/20  Iteration 1066/3560 Training loss: 1.5723 17.1910 sec/batch\n",
      "Epoch 6/20  Iteration 1067/3560 Training loss: 1.5719 17.5803 sec/batch\n",
      "Epoch 6/20  Iteration 1068/3560 Training loss: 1.5719 16.3252 sec/batch\n",
      "Epoch 7/20  Iteration 1069/3560 Training loss: 1.6329 17.3779 sec/batch\n",
      "Epoch 7/20  Iteration 1070/3560 Training loss: 1.5888 16.2067 sec/batch\n",
      "Epoch 7/20  Iteration 1071/3560 Training loss: 1.5686 17.2189 sec/batch\n",
      "Epoch 7/20  Iteration 1072/3560 Training loss: 1.5610 16.6001 sec/batch\n",
      "Epoch 7/20  Iteration 1073/3560 Training loss: 1.5511 16.4087 sec/batch\n",
      "Epoch 7/20  Iteration 1074/3560 Training loss: 1.5413 17.0847 sec/batch\n",
      "Epoch 7/20  Iteration 1075/3560 Training loss: 1.5408 18.1973 sec/batch\n",
      "Epoch 7/20  Iteration 1076/3560 Training loss: 1.5383 20.4583 sec/batch\n",
      "Epoch 7/20  Iteration 1077/3560 Training loss: 1.5385 19.9499 sec/batch\n",
      "Epoch 7/20  Iteration 1078/3560 Training loss: 1.5368 19.8061 sec/batch\n",
      "Epoch 7/20  Iteration 1079/3560 Training loss: 1.5334 19.9846 sec/batch\n",
      "Epoch 7/20  Iteration 1080/3560 Training loss: 1.5311 20.1846 sec/batch\n",
      "Epoch 7/20  Iteration 1081/3560 Training loss: 1.5300 20.4850 sec/batch\n",
      "Epoch 7/20  Iteration 1082/3560 Training loss: 1.5314 21.2707 sec/batch\n",
      "Epoch 7/20  Iteration 1083/3560 Training loss: 1.5301 20.7755 sec/batch\n",
      "Epoch 7/20  Iteration 1084/3560 Training loss: 1.5276 18.2062 sec/batch\n",
      "Epoch 7/20  Iteration 1085/3560 Training loss: 1.5267 17.3653 sec/batch\n",
      "Epoch 7/20  Iteration 1086/3560 Training loss: 1.5278 16.6533 sec/batch\n",
      "Epoch 7/20  Iteration 1087/3560 Training loss: 1.5281 17.2395 sec/batch\n",
      "Epoch 7/20  Iteration 1088/3560 Training loss: 1.5287 16.9070 sec/batch\n",
      "Epoch 7/20  Iteration 1089/3560 Training loss: 1.5280 16.6020 sec/batch\n",
      "Epoch 7/20  Iteration 1090/3560 Training loss: 1.5281 17.0395 sec/batch\n",
      "Epoch 7/20  Iteration 1091/3560 Training loss: 1.5270 16.9563 sec/batch\n",
      "Epoch 7/20  Iteration 1092/3560 Training loss: 1.5266 17.0367 sec/batch\n",
      "Epoch 7/20  Iteration 1093/3560 Training loss: 1.5264 17.1841 sec/batch\n",
      "Epoch 7/20  Iteration 1094/3560 Training loss: 1.5246 16.8593 sec/batch\n",
      "Epoch 7/20  Iteration 1095/3560 Training loss: 1.5234 17.9757 sec/batch\n",
      "Epoch 7/20  Iteration 1096/3560 Training loss: 1.5236 17.0931 sec/batch\n",
      "Epoch 7/20  Iteration 1097/3560 Training loss: 1.5238 17.0743 sec/batch\n",
      "Epoch 7/20  Iteration 1098/3560 Training loss: 1.5237 16.8944 sec/batch\n",
      "Epoch 7/20  Iteration 1099/3560 Training loss: 1.5232 16.9098 sec/batch\n",
      "Epoch 7/20  Iteration 1100/3560 Training loss: 1.5220 17.3259 sec/batch\n",
      "Epoch 7/20  Iteration 1101/3560 Training loss: 1.5220 16.0830 sec/batch\n",
      "Epoch 7/20  Iteration 1102/3560 Training loss: 1.5222 20.1887 sec/batch\n",
      "Epoch 7/20  Iteration 1103/3560 Training loss: 1.5218 20.3920 sec/batch\n",
      "Epoch 7/20  Iteration 1104/3560 Training loss: 1.5214 20.4167 sec/batch\n",
      "Epoch 7/20  Iteration 1105/3560 Training loss: 1.5203 20.0231 sec/batch\n",
      "Epoch 7/20  Iteration 1106/3560 Training loss: 1.5192 20.3944 sec/batch\n",
      "Epoch 7/20  Iteration 1107/3560 Training loss: 1.5175 19.6506 sec/batch\n",
      "Epoch 7/20  Iteration 1108/3560 Training loss: 1.5166 20.8151 sec/batch\n",
      "Epoch 7/20  Iteration 1109/3560 Training loss: 1.5161 19.9183 sec/batch\n",
      "Epoch 7/20  Iteration 1110/3560 Training loss: 1.5165 20.1408 sec/batch\n",
      "Epoch 7/20  Iteration 1111/3560 Training loss: 1.5159 16.9561 sec/batch\n",
      "Epoch 7/20  Iteration 1112/3560 Training loss: 1.5148 17.3408 sec/batch\n",
      "Epoch 7/20  Iteration 1113/3560 Training loss: 1.5147 18.2366 sec/batch\n",
      "Epoch 7/20  Iteration 1114/3560 Training loss: 1.5136 16.5191 sec/batch\n",
      "Epoch 7/20  Iteration 1115/3560 Training loss: 1.5131 17.0167 sec/batch\n",
      "Epoch 7/20  Iteration 1116/3560 Training loss: 1.5124 17.9192 sec/batch\n",
      "Epoch 7/20  Iteration 1117/3560 Training loss: 1.5118 17.2922 sec/batch\n",
      "Epoch 7/20  Iteration 1118/3560 Training loss: 1.5123 16.6248 sec/batch\n",
      "Epoch 7/20  Iteration 1119/3560 Training loss: 1.5117 17.1277 sec/batch\n",
      "Epoch 7/20  Iteration 1120/3560 Training loss: 1.5125 17.1584 sec/batch\n",
      "Epoch 7/20  Iteration 1121/3560 Training loss: 1.5121 17.2735 sec/batch\n",
      "Epoch 7/20  Iteration 1122/3560 Training loss: 1.5121 17.1249 sec/batch\n",
      "Epoch 7/20  Iteration 1123/3560 Training loss: 1.5116 16.6769 sec/batch\n",
      "Epoch 7/20  Iteration 1124/3560 Training loss: 1.5115 16.7384 sec/batch\n",
      "Epoch 7/20  Iteration 1125/3560 Training loss: 1.5117 17.2218 sec/batch\n",
      "Epoch 7/20  Iteration 1126/3560 Training loss: 1.5110 17.1645 sec/batch\n",
      "Epoch 7/20  Iteration 1127/3560 Training loss: 1.5104 16.7421 sec/batch\n",
      "Epoch 7/20  Iteration 1128/3560 Training loss: 1.5108 18.9096 sec/batch\n",
      "Epoch 7/20  Iteration 1129/3560 Training loss: 1.5106 20.4685 sec/batch\n",
      "Epoch 7/20  Iteration 1130/3560 Training loss: 1.5113 20.1465 sec/batch\n",
      "Epoch 7/20  Iteration 1131/3560 Training loss: 1.5115 20.3243 sec/batch\n",
      "Epoch 7/20  Iteration 1132/3560 Training loss: 1.5114 22.1287 sec/batch\n",
      "Epoch 7/20  Iteration 1133/3560 Training loss: 1.5111 20.4394 sec/batch\n",
      "Epoch 7/20  Iteration 1134/3560 Training loss: 1.5110 20.5534 sec/batch\n",
      "Epoch 7/20  Iteration 1135/3560 Training loss: 1.5112 20.1941 sec/batch\n",
      "Epoch 7/20  Iteration 1136/3560 Training loss: 1.5108 20.6150 sec/batch\n",
      "Epoch 7/20  Iteration 1137/3560 Training loss: 1.5109 17.8143 sec/batch\n",
      "Epoch 7/20  Iteration 1138/3560 Training loss: 1.5107 16.7089 sec/batch\n",
      "Epoch 7/20  Iteration 1139/3560 Training loss: 1.5112 16.6926 sec/batch\n",
      "Epoch 7/20  Iteration 1140/3560 Training loss: 1.5113 17.2158 sec/batch\n",
      "Epoch 7/20  Iteration 1141/3560 Training loss: 1.5116 16.9107 sec/batch\n",
      "Epoch 7/20  Iteration 1142/3560 Training loss: 1.5112 17.3720 sec/batch\n",
      "Epoch 7/20  Iteration 1143/3560 Training loss: 1.5109 16.6959 sec/batch\n",
      "Epoch 7/20  Iteration 1144/3560 Training loss: 1.5109 16.9629 sec/batch\n",
      "Epoch 7/20  Iteration 1145/3560 Training loss: 1.5106 17.7923 sec/batch\n",
      "Epoch 7/20  Iteration 1146/3560 Training loss: 1.5104 17.3867 sec/batch\n",
      "Epoch 7/20  Iteration 1147/3560 Training loss: 1.5096 17.0779 sec/batch\n",
      "Epoch 7/20  Iteration 1148/3560 Training loss: 1.5092 17.2211 sec/batch\n",
      "Epoch 7/20  Iteration 1149/3560 Training loss: 1.5086 17.3729 sec/batch\n",
      "Epoch 7/20  Iteration 1150/3560 Training loss: 1.5084 16.6854 sec/batch\n",
      "Epoch 7/20  Iteration 1151/3560 Training loss: 1.5078 16.7955 sec/batch\n",
      "Epoch 7/20  Iteration 1152/3560 Training loss: 1.5077 17.1988 sec/batch\n",
      "Epoch 7/20  Iteration 1153/3560 Training loss: 1.5072 17.1033 sec/batch\n",
      "Epoch 7/20  Iteration 1154/3560 Training loss: 1.5069 17.5655 sec/batch\n",
      "Epoch 7/20  Iteration 1155/3560 Training loss: 1.5063 20.2780 sec/batch\n",
      "Epoch 7/20  Iteration 1156/3560 Training loss: 1.5059 20.8581 sec/batch\n",
      "Epoch 7/20  Iteration 1157/3560 Training loss: 1.5054 19.6578 sec/batch\n",
      "Epoch 7/20  Iteration 1158/3560 Training loss: 1.5053 20.7260 sec/batch\n",
      "Epoch 7/20  Iteration 1159/3560 Training loss: 1.5049 19.7645 sec/batch\n",
      "Epoch 7/20  Iteration 1160/3560 Training loss: 1.5045 19.8823 sec/batch\n",
      "Epoch 7/20  Iteration 1161/3560 Training loss: 1.5041 20.4660 sec/batch\n",
      "Epoch 7/20  Iteration 1162/3560 Training loss: 1.5036 20.1607 sec/batch\n",
      "Epoch 7/20  Iteration 1163/3560 Training loss: 1.5031 18.7968 sec/batch\n",
      "Epoch 7/20  Iteration 1164/3560 Training loss: 1.5031 17.0013 sec/batch\n",
      "Epoch 7/20  Iteration 1165/3560 Training loss: 1.5029 18.1957 sec/batch\n",
      "Epoch 7/20  Iteration 1166/3560 Training loss: 1.5023 17.1205 sec/batch\n",
      "Epoch 7/20  Iteration 1167/3560 Training loss: 1.5019 17.2129 sec/batch\n",
      "Epoch 7/20  Iteration 1168/3560 Training loss: 1.5013 16.5712 sec/batch\n",
      "Epoch 7/20  Iteration 1169/3560 Training loss: 1.5011 17.6807 sec/batch\n",
      "Epoch 7/20  Iteration 1170/3560 Training loss: 1.5009 16.3226 sec/batch\n",
      "Epoch 7/20  Iteration 1171/3560 Training loss: 1.5006 16.6860 sec/batch\n",
      "Epoch 7/20  Iteration 1172/3560 Training loss: 1.5003 16.3979 sec/batch\n",
      "Epoch 7/20  Iteration 1173/3560 Training loss: 1.5000 16.8226 sec/batch\n",
      "Epoch 7/20  Iteration 1174/3560 Training loss: 1.4997 17.2866 sec/batch\n",
      "Epoch 7/20  Iteration 1175/3560 Training loss: 1.4994 17.5283 sec/batch\n",
      "Epoch 7/20  Iteration 1176/3560 Training loss: 1.4992 16.6821 sec/batch\n",
      "Epoch 7/20  Iteration 1177/3560 Training loss: 1.4989 16.4467 sec/batch\n",
      "Epoch 7/20  Iteration 1178/3560 Training loss: 1.4988 16.7886 sec/batch\n",
      "Epoch 7/20  Iteration 1179/3560 Training loss: 1.4984 17.5908 sec/batch\n",
      "Epoch 7/20  Iteration 1180/3560 Training loss: 1.4981 17.1684 sec/batch\n",
      "Epoch 7/20  Iteration 1181/3560 Training loss: 1.4978 19.1730 sec/batch\n",
      "Epoch 7/20  Iteration 1182/3560 Training loss: 1.4975 21.1333 sec/batch\n",
      "Epoch 7/20  Iteration 1183/3560 Training loss: 1.4971 20.5912 sec/batch\n",
      "Epoch 7/20  Iteration 1184/3560 Training loss: 1.4967 19.3195 sec/batch\n",
      "Epoch 7/20  Iteration 1185/3560 Training loss: 1.4966 20.5083 sec/batch\n",
      "Epoch 7/20  Iteration 1186/3560 Training loss: 1.4964 20.2002 sec/batch\n",
      "Epoch 7/20  Iteration 1187/3560 Training loss: 1.4961 19.7463 sec/batch\n",
      "Epoch 7/20  Iteration 1188/3560 Training loss: 1.4959 19.6681 sec/batch\n",
      "Epoch 7/20  Iteration 1189/3560 Training loss: 1.4957 20.5385 sec/batch\n",
      "Epoch 7/20  Iteration 1190/3560 Training loss: 1.4952 18.0836 sec/batch\n",
      "Epoch 7/20  Iteration 1191/3560 Training loss: 1.4946 17.3860 sec/batch\n",
      "Epoch 7/20  Iteration 1192/3560 Training loss: 1.4945 16.8066 sec/batch\n",
      "Epoch 7/20  Iteration 1193/3560 Training loss: 1.4944 16.9133 sec/batch\n",
      "Epoch 7/20  Iteration 1194/3560 Training loss: 1.4939 16.6410 sec/batch\n",
      "Epoch 7/20  Iteration 1195/3560 Training loss: 1.4939 16.6822 sec/batch\n",
      "Epoch 7/20  Iteration 1196/3560 Training loss: 1.4938 16.9199 sec/batch\n",
      "Epoch 7/20  Iteration 1197/3560 Training loss: 1.4936 17.2567 sec/batch\n",
      "Epoch 7/20  Iteration 1198/3560 Training loss: 1.4932 16.9707 sec/batch\n",
      "Epoch 7/20  Iteration 1199/3560 Training loss: 1.4927 17.2480 sec/batch\n",
      "Epoch 7/20  Iteration 1200/3560 Training loss: 1.4923 16.7347 sec/batch\n",
      "Validation loss: 1.36156 Saving checkpoint!\n",
      "Epoch 7/20  Iteration 1201/3560 Training loss: 1.4932 19.7727 sec/batch\n",
      "Epoch 7/20  Iteration 1202/3560 Training loss: 1.4931 19.9906 sec/batch\n",
      "Epoch 7/20  Iteration 1203/3560 Training loss: 1.4929 19.7774 sec/batch\n",
      "Epoch 7/20  Iteration 1204/3560 Training loss: 1.4929 19.9444 sec/batch\n",
      "Epoch 7/20  Iteration 1205/3560 Training loss: 1.4929 20.3935 sec/batch\n",
      "Epoch 7/20  Iteration 1206/3560 Training loss: 1.4929 20.6602 sec/batch\n",
      "Epoch 7/20  Iteration 1207/3560 Training loss: 1.4929 21.6084 sec/batch\n",
      "Epoch 7/20  Iteration 1208/3560 Training loss: 1.4927 20.1787 sec/batch\n",
      "Epoch 7/20  Iteration 1209/3560 Training loss: 1.4930 18.9391 sec/batch\n",
      "Epoch 7/20  Iteration 1210/3560 Training loss: 1.4929 17.1301 sec/batch\n",
      "Epoch 7/20  Iteration 1211/3560 Training loss: 1.4926 16.5793 sec/batch\n",
      "Epoch 7/20  Iteration 1212/3560 Training loss: 1.4926 17.0566 sec/batch\n",
      "Epoch 7/20  Iteration 1213/3560 Training loss: 1.4925 17.1639 sec/batch\n",
      "Epoch 7/20  Iteration 1214/3560 Training loss: 1.4923 17.1896 sec/batch\n",
      "Epoch 7/20  Iteration 1215/3560 Training loss: 1.4923 16.9037 sec/batch\n",
      "Epoch 7/20  Iteration 1216/3560 Training loss: 1.4924 17.0505 sec/batch\n",
      "Epoch 7/20  Iteration 1217/3560 Training loss: 1.4923 17.0050 sec/batch\n",
      "Epoch 7/20  Iteration 1218/3560 Training loss: 1.4920 17.5237 sec/batch\n",
      "Epoch 7/20  Iteration 1219/3560 Training loss: 1.4916 16.9878 sec/batch\n",
      "Epoch 7/20  Iteration 1220/3560 Training loss: 1.4914 17.5952 sec/batch\n",
      "Epoch 7/20  Iteration 1221/3560 Training loss: 1.4913 16.4535 sec/batch\n",
      "Epoch 7/20  Iteration 1222/3560 Training loss: 1.4911 17.3155 sec/batch\n",
      "Epoch 7/20  Iteration 1223/3560 Training loss: 1.4909 16.4696 sec/batch\n",
      "Epoch 7/20  Iteration 1224/3560 Training loss: 1.4908 17.0425 sec/batch\n",
      "Epoch 7/20  Iteration 1225/3560 Training loss: 1.4907 17.6357 sec/batch\n",
      "Epoch 7/20  Iteration 1226/3560 Training loss: 1.4905 17.5124 sec/batch\n",
      "Epoch 7/20  Iteration 1227/3560 Training loss: 1.4901 19.9032 sec/batch\n",
      "Epoch 7/20  Iteration 1228/3560 Training loss: 1.4901 19.7706 sec/batch\n",
      "Epoch 7/20  Iteration 1229/3560 Training loss: 1.4903 20.3069 sec/batch\n",
      "Epoch 7/20  Iteration 1230/3560 Training loss: 1.4901 20.4903 sec/batch\n",
      "Epoch 7/20  Iteration 1231/3560 Training loss: 1.4900 20.9284 sec/batch\n",
      "Epoch 7/20  Iteration 1232/3560 Training loss: 1.4899 20.4733 sec/batch\n",
      "Epoch 7/20  Iteration 1233/3560 Training loss: 1.4897 20.4959 sec/batch\n",
      "Epoch 7/20  Iteration 1234/3560 Training loss: 1.4895 20.5219 sec/batch\n",
      "Epoch 7/20  Iteration 1235/3560 Training loss: 1.4895 20.2543 sec/batch\n",
      "Epoch 7/20  Iteration 1236/3560 Training loss: 1.4897 17.7461 sec/batch\n",
      "Epoch 7/20  Iteration 1237/3560 Training loss: 1.4896 17.5988 sec/batch\n",
      "Epoch 7/20  Iteration 1238/3560 Training loss: 1.4894 16.4360 sec/batch\n",
      "Epoch 7/20  Iteration 1239/3560 Training loss: 1.4892 16.9383 sec/batch\n",
      "Epoch 7/20  Iteration 1240/3560 Training loss: 1.4889 17.8446 sec/batch\n",
      "Epoch 7/20  Iteration 1241/3560 Training loss: 1.4888 18.0585 sec/batch\n",
      "Epoch 7/20  Iteration 1242/3560 Training loss: 1.4888 16.9255 sec/batch\n",
      "Epoch 7/20  Iteration 1243/3560 Training loss: 1.4888 16.8480 sec/batch\n",
      "Epoch 7/20  Iteration 1244/3560 Training loss: 1.4885 17.7709 sec/batch\n",
      "Epoch 7/20  Iteration 1245/3560 Training loss: 1.4882 17.3106 sec/batch\n",
      "Epoch 7/20  Iteration 1246/3560 Training loss: 1.4881 16.8642 sec/batch\n",
      "Epoch 8/20  Iteration 1247/3560 Training loss: 1.5947 16.6908 sec/batch\n",
      "Epoch 8/20  Iteration 1248/3560 Training loss: 1.5395 17.0114 sec/batch\n",
      "Epoch 8/20  Iteration 1249/3560 Training loss: 1.5203 16.3778 sec/batch\n",
      "Epoch 8/20  Iteration 1250/3560 Training loss: 1.5137 17.4227 sec/batch\n",
      "Epoch 8/20  Iteration 1251/3560 Training loss: 1.5007 16.7891 sec/batch\n",
      "Epoch 8/20  Iteration 1252/3560 Training loss: 1.4880 17.0186 sec/batch\n",
      "Epoch 8/20  Iteration 1253/3560 Training loss: 1.4860 19.9838 sec/batch\n",
      "Epoch 8/20  Iteration 1254/3560 Training loss: 1.4832 20.6310 sec/batch\n",
      "Epoch 8/20  Iteration 1255/3560 Training loss: 1.4819 20.2775 sec/batch\n",
      "Epoch 8/20  Iteration 1256/3560 Training loss: 1.4793 20.0192 sec/batch\n",
      "Epoch 8/20  Iteration 1257/3560 Training loss: 1.4759 22.8598 sec/batch\n",
      "Epoch 8/20  Iteration 1258/3560 Training loss: 1.4742 20.3676 sec/batch\n",
      "Epoch 8/20  Iteration 1259/3560 Training loss: 1.4736 20.6620 sec/batch\n",
      "Epoch 8/20  Iteration 1260/3560 Training loss: 1.4750 20.5988 sec/batch\n",
      "Epoch 8/20  Iteration 1261/3560 Training loss: 1.4740 19.8620 sec/batch\n",
      "Epoch 8/20  Iteration 1262/3560 Training loss: 1.4718 16.7445 sec/batch\n",
      "Epoch 8/20  Iteration 1263/3560 Training loss: 1.4715 16.9767 sec/batch\n",
      "Epoch 8/20  Iteration 1264/3560 Training loss: 1.4723 16.7430 sec/batch\n",
      "Epoch 8/20  Iteration 1265/3560 Training loss: 1.4719 16.6117 sec/batch\n",
      "Epoch 8/20  Iteration 1266/3560 Training loss: 1.4721 17.4830 sec/batch\n",
      "Epoch 8/20  Iteration 1267/3560 Training loss: 1.4714 17.0552 sec/batch\n",
      "Epoch 8/20  Iteration 1268/3560 Training loss: 1.4716 16.7855 sec/batch\n",
      "Epoch 8/20  Iteration 1269/3560 Training loss: 1.4702 17.0652 sec/batch\n",
      "Epoch 8/20  Iteration 1270/3560 Training loss: 1.4702 19.5878 sec/batch\n",
      "Epoch 8/20  Iteration 1271/3560 Training loss: 1.4697 16.8945 sec/batch\n",
      "Epoch 8/20  Iteration 1272/3560 Training loss: 1.4678 17.4406 sec/batch\n",
      "Epoch 8/20  Iteration 1273/3560 Training loss: 1.4664 16.8720 sec/batch\n",
      "Epoch 8/20  Iteration 1274/3560 Training loss: 1.4665 17.6847 sec/batch\n",
      "Epoch 8/20  Iteration 1275/3560 Training loss: 1.4667 22.1323 sec/batch\n",
      "Epoch 8/20  Iteration 1276/3560 Training loss: 1.4667 17.7661 sec/batch\n",
      "Epoch 8/20  Iteration 1277/3560 Training loss: 1.4661 17.3364 sec/batch\n",
      "Epoch 8/20  Iteration 1278/3560 Training loss: 1.4649 17.8968 sec/batch\n",
      "Epoch 8/20  Iteration 1279/3560 Training loss: 1.4647 20.2060 sec/batch\n",
      "Epoch 8/20  Iteration 1280/3560 Training loss: 1.4646 20.7673 sec/batch\n",
      "Epoch 8/20  Iteration 1281/3560 Training loss: 1.4640 20.9666 sec/batch\n",
      "Epoch 8/20  Iteration 1282/3560 Training loss: 1.4635 20.6303 sec/batch\n",
      "Epoch 8/20  Iteration 1283/3560 Training loss: 1.4625 32.9328 sec/batch\n",
      "Epoch 8/20  Iteration 1284/3560 Training loss: 1.4611 37.3965 sec/batch\n",
      "Epoch 8/20  Iteration 1285/3560 Training loss: 1.4593 33.2851 sec/batch\n",
      "Epoch 8/20  Iteration 1286/3560 Training loss: 1.4586 32.3662 sec/batch\n",
      "Epoch 8/20  Iteration 1287/3560 Training loss: 1.4579 28.5275 sec/batch\n",
      "Epoch 8/20  Iteration 1288/3560 Training loss: 1.4583 27.1664 sec/batch\n",
      "Epoch 8/20  Iteration 1289/3560 Training loss: 1.4575 25.4233 sec/batch\n",
      "Epoch 8/20  Iteration 1290/3560 Training loss: 1.4565 31.1912 sec/batch\n",
      "Epoch 8/20  Iteration 1291/3560 Training loss: 1.4565 30.9971 sec/batch\n",
      "Epoch 8/20  Iteration 1292/3560 Training loss: 1.4553 27.9003 sec/batch\n",
      "Epoch 8/20  Iteration 1293/3560 Training loss: 1.4549 26.8661 sec/batch\n",
      "Epoch 8/20  Iteration 1294/3560 Training loss: 1.4542 26.9415 sec/batch\n",
      "Epoch 8/20  Iteration 1295/3560 Training loss: 1.4538 28.2095 sec/batch\n",
      "Epoch 8/20  Iteration 1296/3560 Training loss: 1.4540 35.0935 sec/batch\n",
      "Epoch 8/20  Iteration 1297/3560 Training loss: 1.4532 33.4756 sec/batch\n",
      "Epoch 8/20  Iteration 1298/3560 Training loss: 1.4540 30.9172 sec/batch\n",
      "Epoch 8/20  Iteration 1299/3560 Training loss: 1.4535 38.4787 sec/batch\n",
      "Epoch 8/20  Iteration 1300/3560 Training loss: 1.4534 39.9759 sec/batch\n",
      "Epoch 8/20  Iteration 1301/3560 Training loss: 1.4532 35.6806 sec/batch\n",
      "Epoch 8/20  Iteration 1302/3560 Training loss: 1.4531 32.9640 sec/batch\n",
      "Epoch 8/20  Iteration 1303/3560 Training loss: 1.4533 33.2665 sec/batch\n",
      "Epoch 8/20  Iteration 1304/3560 Training loss: 1.4526 32.8134 sec/batch\n",
      "Epoch 8/20  Iteration 1305/3560 Training loss: 1.4519 37.4234 sec/batch\n",
      "Epoch 8/20  Iteration 1306/3560 Training loss: 1.4523 41.0034 sec/batch\n",
      "Epoch 8/20  Iteration 1307/3560 Training loss: 1.4522 35.8711 sec/batch\n",
      "Epoch 8/20  Iteration 1308/3560 Training loss: 1.4530 32.5684 sec/batch\n",
      "Epoch 8/20  Iteration 1309/3560 Training loss: 1.4531 35.4152 sec/batch\n",
      "Epoch 8/20  Iteration 1310/3560 Training loss: 1.4530 32.2548 sec/batch\n",
      "Epoch 8/20  Iteration 1311/3560 Training loss: 1.4527 37.3612 sec/batch\n",
      "Epoch 8/20  Iteration 1312/3560 Training loss: 1.4527 25.0640 sec/batch\n",
      "Epoch 8/20  Iteration 1313/3560 Training loss: 1.4526 21.1691 sec/batch\n",
      "Epoch 8/20  Iteration 1314/3560 Training loss: 1.4522 20.7823 sec/batch\n",
      "Epoch 8/20  Iteration 1315/3560 Training loss: 1.4520 20.8118 sec/batch\n",
      "Epoch 8/20  Iteration 1316/3560 Training loss: 1.4518 19.6174 sec/batch\n",
      "Epoch 8/20  Iteration 1317/3560 Training loss: 1.4520 17.8394 sec/batch\n",
      "Epoch 8/20  Iteration 1318/3560 Training loss: 1.4520 17.6730 sec/batch\n",
      "Epoch 8/20  Iteration 1319/3560 Training loss: 1.4523 17.4051 sec/batch\n",
      "Epoch 8/20  Iteration 1320/3560 Training loss: 1.4520 18.1673 sec/batch\n",
      "Epoch 8/20  Iteration 1321/3560 Training loss: 1.4517 17.0795 sec/batch\n",
      "Epoch 8/20  Iteration 1322/3560 Training loss: 1.4516 16.9614 sec/batch\n",
      "Epoch 8/20  Iteration 1323/3560 Training loss: 1.4513 17.6461 sec/batch\n",
      "Epoch 8/20  Iteration 1324/3560 Training loss: 1.4511 18.0564 sec/batch\n",
      "Epoch 8/20  Iteration 1325/3560 Training loss: 1.4503 16.6655 sec/batch\n",
      "Epoch 8/20  Iteration 1326/3560 Training loss: 1.4500 13.8706 sec/batch\n",
      "Epoch 8/20  Iteration 1327/3560 Training loss: 1.4493 13.9905 sec/batch\n",
      "Epoch 8/20  Iteration 1328/3560 Training loss: 1.4491 13.8620 sec/batch\n",
      "Epoch 8/20  Iteration 1329/3560 Training loss: 1.4484 13.8857 sec/batch\n",
      "Epoch 8/20  Iteration 1330/3560 Training loss: 1.4482 13.9240 sec/batch\n",
      "Epoch 8/20  Iteration 1331/3560 Training loss: 1.4478 14.2320 sec/batch\n",
      "Epoch 8/20  Iteration 1332/3560 Training loss: 1.4474 13.9120 sec/batch\n",
      "Epoch 8/20  Iteration 1333/3560 Training loss: 1.4470 15.0136 sec/batch\n",
      "Epoch 8/20  Iteration 1334/3560 Training loss: 1.4466 13.9705 sec/batch\n",
      "Epoch 8/20  Iteration 1335/3560 Training loss: 1.4461 13.8408 sec/batch\n",
      "Epoch 8/20  Iteration 1336/3560 Training loss: 1.4460 13.9469 sec/batch\n",
      "Epoch 8/20  Iteration 1337/3560 Training loss: 1.4457 13.9306 sec/batch\n",
      "Epoch 8/20  Iteration 1338/3560 Training loss: 1.4455 13.9028 sec/batch\n",
      "Epoch 8/20  Iteration 1339/3560 Training loss: 1.4450 13.9058 sec/batch\n",
      "Epoch 8/20  Iteration 1340/3560 Training loss: 1.4446 13.9059 sec/batch\n",
      "Epoch 8/20  Iteration 1341/3560 Training loss: 1.4442 13.8898 sec/batch\n",
      "Epoch 8/20  Iteration 1342/3560 Training loss: 1.4441 13.9455 sec/batch\n",
      "Epoch 8/20  Iteration 1343/3560 Training loss: 1.4439 13.9094 sec/batch\n",
      "Epoch 8/20  Iteration 1344/3560 Training loss: 1.4434 13.9365 sec/batch\n",
      "Epoch 8/20  Iteration 1345/3560 Training loss: 1.4430 13.9900 sec/batch\n",
      "Epoch 8/20  Iteration 1346/3560 Training loss: 1.4424 13.9158 sec/batch\n",
      "Epoch 8/20  Iteration 1347/3560 Training loss: 1.4423 13.9342 sec/batch\n",
      "Epoch 8/20  Iteration 1348/3560 Training loss: 1.4420 13.8180 sec/batch\n",
      "Epoch 8/20  Iteration 1349/3560 Training loss: 1.4418 13.8865 sec/batch\n",
      "Epoch 8/20  Iteration 1350/3560 Training loss: 1.4415 13.8813 sec/batch\n",
      "Epoch 8/20  Iteration 1351/3560 Training loss: 1.4412 13.9281 sec/batch\n",
      "Epoch 8/20  Iteration 1352/3560 Training loss: 1.4410 13.9432 sec/batch\n",
      "Epoch 8/20  Iteration 1353/3560 Training loss: 1.4407 13.9572 sec/batch\n",
      "Epoch 8/20  Iteration 1354/3560 Training loss: 1.4407 13.9322 sec/batch\n",
      "Epoch 8/20  Iteration 1355/3560 Training loss: 1.4405 14.3846 sec/batch\n",
      "Epoch 8/20  Iteration 1356/3560 Training loss: 1.4405 13.8434 sec/batch\n",
      "Epoch 8/20  Iteration 1357/3560 Training loss: 1.4401 14.3009 sec/batch\n",
      "Epoch 8/20  Iteration 1358/3560 Training loss: 1.4399 16.2801 sec/batch\n",
      "Epoch 8/20  Iteration 1359/3560 Training loss: 1.4397 15.0236 sec/batch\n",
      "Epoch 8/20  Iteration 1360/3560 Training loss: 1.4395 20.4866 sec/batch\n",
      "Epoch 8/20  Iteration 1361/3560 Training loss: 1.4391 14.2461 sec/batch\n",
      "Epoch 8/20  Iteration 1362/3560 Training loss: 1.4386 13.9796 sec/batch\n",
      "Epoch 8/20  Iteration 1363/3560 Training loss: 1.4384 13.9960 sec/batch\n",
      "Epoch 8/20  Iteration 1364/3560 Training loss: 1.4383 13.9489 sec/batch\n",
      "Epoch 8/20  Iteration 1365/3560 Training loss: 1.4380 14.0288 sec/batch\n",
      "Epoch 8/20  Iteration 1366/3560 Training loss: 1.4379 14.0090 sec/batch\n",
      "Epoch 8/20  Iteration 1367/3560 Training loss: 1.4376 14.0282 sec/batch\n",
      "Epoch 8/20  Iteration 1368/3560 Training loss: 1.4371 13.9436 sec/batch\n",
      "Epoch 8/20  Iteration 1369/3560 Training loss: 1.4365 14.0969 sec/batch\n",
      "Epoch 8/20  Iteration 1370/3560 Training loss: 1.4364 13.9470 sec/batch\n",
      "Epoch 8/20  Iteration 1371/3560 Training loss: 1.4363 13.9740 sec/batch\n",
      "Epoch 8/20  Iteration 1372/3560 Training loss: 1.4358 13.9698 sec/batch\n",
      "Epoch 8/20  Iteration 1373/3560 Training loss: 1.4358 13.9825 sec/batch\n",
      "Epoch 8/20  Iteration 1374/3560 Training loss: 1.4356 13.9505 sec/batch\n",
      "Epoch 8/20  Iteration 1375/3560 Training loss: 1.4354 14.6208 sec/batch\n",
      "Epoch 8/20  Iteration 1376/3560 Training loss: 1.4350 14.0118 sec/batch\n",
      "Epoch 8/20  Iteration 1377/3560 Training loss: 1.4345 13.9917 sec/batch\n",
      "Epoch 8/20  Iteration 1378/3560 Training loss: 1.4341 14.0573 sec/batch\n",
      "Epoch 8/20  Iteration 1379/3560 Training loss: 1.4340 13.9983 sec/batch\n",
      "Epoch 8/20  Iteration 1380/3560 Training loss: 1.4339 13.9518 sec/batch\n",
      "Epoch 8/20  Iteration 1381/3560 Training loss: 1.4338 14.0034 sec/batch\n",
      "Epoch 8/20  Iteration 1382/3560 Training loss: 1.4338 13.9900 sec/batch\n",
      "Epoch 8/20  Iteration 1383/3560 Training loss: 1.4338 13.9424 sec/batch\n",
      "Epoch 8/20  Iteration 1384/3560 Training loss: 1.4338 13.9801 sec/batch\n",
      "Epoch 8/20  Iteration 1385/3560 Training loss: 1.4338 13.9324 sec/batch\n",
      "Epoch 8/20  Iteration 1386/3560 Training loss: 1.4336 13.9552 sec/batch\n",
      "Epoch 8/20  Iteration 1387/3560 Training loss: 1.4339 13.9750 sec/batch\n",
      "Epoch 8/20  Iteration 1388/3560 Training loss: 1.4337 13.9267 sec/batch\n",
      "Epoch 8/20  Iteration 1389/3560 Training loss: 1.4335 14.0044 sec/batch\n",
      "Epoch 8/20  Iteration 1390/3560 Training loss: 1.4336 13.9577 sec/batch\n",
      "Epoch 8/20  Iteration 1391/3560 Training loss: 1.4333 13.9795 sec/batch\n",
      "Epoch 8/20  Iteration 1392/3560 Training loss: 1.4333 13.9658 sec/batch\n",
      "Epoch 8/20  Iteration 1393/3560 Training loss: 1.4332 13.9376 sec/batch\n",
      "Epoch 8/20  Iteration 1394/3560 Training loss: 1.4334 13.9160 sec/batch\n",
      "Epoch 8/20  Iteration 1395/3560 Training loss: 1.4334 13.9625 sec/batch\n",
      "Epoch 8/20  Iteration 1396/3560 Training loss: 1.4331 13.8977 sec/batch\n",
      "Epoch 8/20  Iteration 1397/3560 Training loss: 1.4328 14.3011 sec/batch\n",
      "Epoch 8/20  Iteration 1398/3560 Training loss: 1.4326 13.9603 sec/batch\n",
      "Epoch 8/20  Iteration 1399/3560 Training loss: 1.4325 14.0186 sec/batch\n",
      "Epoch 8/20  Iteration 1400/3560 Training loss: 1.4324 13.9384 sec/batch\n",
      "Validation loss: 1.30572 Saving checkpoint!\n",
      "Epoch 8/20  Iteration 1401/3560 Training loss: 1.4332 13.9063 sec/batch\n",
      "Epoch 8/20  Iteration 1402/3560 Training loss: 1.4332 13.9611 sec/batch\n",
      "Epoch 8/20  Iteration 1403/3560 Training loss: 1.4332 14.0149 sec/batch\n",
      "Epoch 8/20  Iteration 1404/3560 Training loss: 1.4330 13.9425 sec/batch\n",
      "Epoch 8/20  Iteration 1405/3560 Training loss: 1.4328 14.0921 sec/batch\n",
      "Epoch 8/20  Iteration 1406/3560 Training loss: 1.4329 13.8947 sec/batch\n",
      "Epoch 8/20  Iteration 1407/3560 Training loss: 1.4330 13.9725 sec/batch\n",
      "Epoch 8/20  Iteration 1408/3560 Training loss: 1.4330 13.9582 sec/batch\n",
      "Epoch 8/20  Iteration 1409/3560 Training loss: 1.4328 13.9750 sec/batch\n",
      "Epoch 8/20  Iteration 1410/3560 Training loss: 1.4328 13.9635 sec/batch\n",
      "Epoch 8/20  Iteration 1411/3560 Training loss: 1.4327 15.2470 sec/batch\n",
      "Epoch 8/20  Iteration 1412/3560 Training loss: 1.4326 13.9689 sec/batch\n",
      "Epoch 8/20  Iteration 1413/3560 Training loss: 1.4327 14.0276 sec/batch\n",
      "Epoch 8/20  Iteration 1414/3560 Training loss: 1.4329 13.9541 sec/batch\n",
      "Epoch 8/20  Iteration 1415/3560 Training loss: 1.4328 13.9664 sec/batch\n",
      "Epoch 8/20  Iteration 1416/3560 Training loss: 1.4328 13.9552 sec/batch\n",
      "Epoch 8/20  Iteration 1417/3560 Training loss: 1.4326 13.9351 sec/batch\n",
      "Epoch 8/20  Iteration 1418/3560 Training loss: 1.4324 13.9640 sec/batch\n",
      "Epoch 8/20  Iteration 1419/3560 Training loss: 1.4324 13.9292 sec/batch\n",
      "Epoch 8/20  Iteration 1420/3560 Training loss: 1.4324 13.9539 sec/batch\n",
      "Epoch 8/20  Iteration 1421/3560 Training loss: 1.4323 13.9819 sec/batch\n",
      "Epoch 8/20  Iteration 1422/3560 Training loss: 1.4321 14.0195 sec/batch\n",
      "Epoch 8/20  Iteration 1423/3560 Training loss: 1.4319 13.9856 sec/batch\n",
      "Epoch 8/20  Iteration 1424/3560 Training loss: 1.4319 14.0081 sec/batch\n",
      "Epoch 9/20  Iteration 1425/3560 Training loss: 1.5479 13.9423 sec/batch\n",
      "Epoch 9/20  Iteration 1426/3560 Training loss: 1.4872 13.9407 sec/batch\n",
      "Epoch 9/20  Iteration 1427/3560 Training loss: 1.4634 13.8894 sec/batch\n",
      "Epoch 9/20  Iteration 1428/3560 Training loss: 1.4520 13.9225 sec/batch\n",
      "Epoch 9/20  Iteration 1429/3560 Training loss: 1.4407 14.0135 sec/batch\n",
      "Epoch 9/20  Iteration 1430/3560 Training loss: 1.4281 13.9509 sec/batch\n",
      "Epoch 9/20  Iteration 1431/3560 Training loss: 1.4267 14.0758 sec/batch\n",
      "Epoch 9/20  Iteration 1432/3560 Training loss: 1.4226 14.2481 sec/batch\n",
      "Epoch 9/20  Iteration 1433/3560 Training loss: 1.4210 14.0162 sec/batch\n",
      "Epoch 9/20  Iteration 1434/3560 Training loss: 1.4193 13.9504 sec/batch\n",
      "Epoch 9/20  Iteration 1435/3560 Training loss: 1.4152 14.0284 sec/batch\n",
      "Epoch 9/20  Iteration 1436/3560 Training loss: 1.4135 14.2135 sec/batch\n",
      "Epoch 9/20  Iteration 1437/3560 Training loss: 1.4125 14.0045 sec/batch\n",
      "Epoch 9/20  Iteration 1438/3560 Training loss: 1.4135 13.9748 sec/batch\n",
      "Epoch 9/20  Iteration 1439/3560 Training loss: 1.4124 14.0764 sec/batch\n",
      "Epoch 9/20  Iteration 1440/3560 Training loss: 1.4097 13.9180 sec/batch\n",
      "Epoch 9/20  Iteration 1441/3560 Training loss: 1.4097 13.9801 sec/batch\n",
      "Epoch 9/20  Iteration 1442/3560 Training loss: 1.4107 14.0215 sec/batch\n",
      "Epoch 9/20  Iteration 1443/3560 Training loss: 1.4107 13.9554 sec/batch\n",
      "Epoch 9/20  Iteration 1444/3560 Training loss: 1.4119 14.2954 sec/batch\n",
      "Epoch 9/20  Iteration 1445/3560 Training loss: 1.4110 14.0132 sec/batch\n",
      "Epoch 9/20  Iteration 1446/3560 Training loss: 1.4110 13.9222 sec/batch\n",
      "Epoch 9/20  Iteration 1447/3560 Training loss: 1.4097 13.9447 sec/batch\n",
      "Epoch 9/20  Iteration 1448/3560 Training loss: 1.4095 13.9916 sec/batch\n",
      "Epoch 9/20  Iteration 1449/3560 Training loss: 1.4092 13.9451 sec/batch\n",
      "Epoch 9/20  Iteration 1450/3560 Training loss: 1.4075 13.9993 sec/batch\n",
      "Epoch 9/20  Iteration 1451/3560 Training loss: 1.4060 13.9367 sec/batch\n",
      "Epoch 9/20  Iteration 1452/3560 Training loss: 1.4067 13.9605 sec/batch\n",
      "Epoch 9/20  Iteration 1453/3560 Training loss: 1.4068 13.9612 sec/batch\n",
      "Epoch 9/20  Iteration 1454/3560 Training loss: 1.4069 14.5703 sec/batch\n",
      "Epoch 9/20  Iteration 1455/3560 Training loss: 1.4064 13.9615 sec/batch\n",
      "Epoch 9/20  Iteration 1456/3560 Training loss: 1.4051 14.1370 sec/batch\n",
      "Epoch 9/20  Iteration 1457/3560 Training loss: 1.4052 14.0031 sec/batch\n",
      "Epoch 9/20  Iteration 1458/3560 Training loss: 1.4051 14.0219 sec/batch\n",
      "Epoch 9/20  Iteration 1459/3560 Training loss: 1.4048 13.9206 sec/batch\n",
      "Epoch 9/20  Iteration 1460/3560 Training loss: 1.4047 13.9680 sec/batch\n",
      "Epoch 9/20  Iteration 1461/3560 Training loss: 1.4038 14.0014 sec/batch\n",
      "Epoch 9/20  Iteration 1462/3560 Training loss: 1.4026 13.9420 sec/batch\n",
      "Epoch 9/20  Iteration 1463/3560 Training loss: 1.4009 13.9955 sec/batch\n",
      "Epoch 9/20  Iteration 1464/3560 Training loss: 1.4003 13.9592 sec/batch\n",
      "Epoch 9/20  Iteration 1465/3560 Training loss: 1.3998 14.0524 sec/batch\n",
      "Epoch 9/20  Iteration 1466/3560 Training loss: 1.4002 13.9748 sec/batch\n",
      "Epoch 9/20  Iteration 1467/3560 Training loss: 1.3997 14.0617 sec/batch\n",
      "Epoch 9/20  Iteration 1468/3560 Training loss: 1.3990 14.0042 sec/batch\n",
      "Epoch 9/20  Iteration 1469/3560 Training loss: 1.3991 13.9901 sec/batch\n",
      "Epoch 9/20  Iteration 1470/3560 Training loss: 1.3981 13.9959 sec/batch\n",
      "Epoch 9/20  Iteration 1471/3560 Training loss: 1.3977 13.9315 sec/batch\n",
      "Epoch 9/20  Iteration 1472/3560 Training loss: 1.3972 14.0510 sec/batch\n",
      "Epoch 9/20  Iteration 1473/3560 Training loss: 1.3971 14.0467 sec/batch\n",
      "Epoch 9/20  Iteration 1474/3560 Training loss: 1.3973 13.9796 sec/batch\n",
      "Epoch 9/20  Iteration 1475/3560 Training loss: 1.3969 15.8987 sec/batch\n",
      "Epoch 9/20  Iteration 1476/3560 Training loss: 1.3976 14.1159 sec/batch\n",
      "Epoch 9/20  Iteration 1477/3560 Training loss: 1.3974 13.9837 sec/batch\n",
      "Epoch 9/20  Iteration 1478/3560 Training loss: 1.3976 13.9581 sec/batch\n",
      "Epoch 9/20  Iteration 1479/3560 Training loss: 1.3974 14.0401 sec/batch\n",
      "Epoch 9/20  Iteration 1480/3560 Training loss: 1.3974 13.9356 sec/batch\n",
      "Epoch 9/20  Iteration 1481/3560 Training loss: 1.3978 13.9403 sec/batch\n",
      "Epoch 9/20  Iteration 1482/3560 Training loss: 1.3975 14.0084 sec/batch\n",
      "Epoch 9/20  Iteration 1483/3560 Training loss: 1.3968 13.9786 sec/batch\n",
      "Epoch 9/20  Iteration 1484/3560 Training loss: 1.3974 13.9369 sec/batch\n",
      "Epoch 9/20  Iteration 1485/3560 Training loss: 1.3973 13.9741 sec/batch\n",
      "Epoch 9/20  Iteration 1486/3560 Training loss: 1.3982 13.9239 sec/batch\n",
      "Epoch 9/20  Iteration 1487/3560 Training loss: 1.3986 13.9934 sec/batch\n",
      "Epoch 9/20  Iteration 1488/3560 Training loss: 1.3987 13.9576 sec/batch\n",
      "Epoch 9/20  Iteration 1489/3560 Training loss: 1.3987 13.9914 sec/batch\n",
      "Epoch 9/20  Iteration 1490/3560 Training loss: 1.3987 14.0313 sec/batch\n",
      "Epoch 9/20  Iteration 1491/3560 Training loss: 1.3987 13.9947 sec/batch\n",
      "Epoch 9/20  Iteration 1492/3560 Training loss: 1.3983 13.9804 sec/batch\n",
      "Epoch 9/20  Iteration 1493/3560 Training loss: 1.3984 14.0138 sec/batch\n",
      "Epoch 9/20  Iteration 1494/3560 Training loss: 1.3982 13.9161 sec/batch\n",
      "Epoch 9/20  Iteration 1495/3560 Training loss: 1.3988 13.9803 sec/batch\n",
      "Epoch 9/20  Iteration 1496/3560 Training loss: 1.3989 14.2552 sec/batch\n",
      "Epoch 9/20  Iteration 1497/3560 Training loss: 1.3992 14.0095 sec/batch\n",
      "Epoch 9/20  Iteration 1498/3560 Training loss: 1.3988 13.9356 sec/batch\n",
      "Epoch 9/20  Iteration 1499/3560 Training loss: 1.3986 13.9890 sec/batch\n",
      "Epoch 9/20  Iteration 1500/3560 Training loss: 1.3987 13.9095 sec/batch\n",
      "Epoch 9/20  Iteration 1501/3560 Training loss: 1.3984 14.0190 sec/batch\n",
      "Epoch 9/20  Iteration 1502/3560 Training loss: 1.3983 13.9305 sec/batch\n",
      "Epoch 9/20  Iteration 1503/3560 Training loss: 1.3977 13.9751 sec/batch\n",
      "Epoch 9/20  Iteration 1504/3560 Training loss: 1.3976 14.0147 sec/batch\n",
      "Epoch 9/20  Iteration 1505/3560 Training loss: 1.3970 13.9454 sec/batch\n",
      "Epoch 9/20  Iteration 1506/3560 Training loss: 1.3969 13.9354 sec/batch\n",
      "Epoch 9/20  Iteration 1507/3560 Training loss: 1.3963 13.9837 sec/batch\n",
      "Epoch 9/20  Iteration 1508/3560 Training loss: 1.3962 14.0538 sec/batch\n",
      "Epoch 9/20  Iteration 1509/3560 Training loss: 1.3957 14.0037 sec/batch\n",
      "Epoch 9/20  Iteration 1510/3560 Training loss: 1.3955 14.0403 sec/batch\n",
      "Epoch 9/20  Iteration 1511/3560 Training loss: 1.3952 13.9751 sec/batch\n",
      "Epoch 9/20  Iteration 1512/3560 Training loss: 1.3947 14.0037 sec/batch\n",
      "Epoch 9/20  Iteration 1513/3560 Training loss: 1.3943 13.9849 sec/batch\n",
      "Epoch 9/20  Iteration 1514/3560 Training loss: 1.3944 14.0740 sec/batch\n",
      "Epoch 9/20  Iteration 1515/3560 Training loss: 1.3941 13.9974 sec/batch\n",
      "Epoch 9/20  Iteration 1516/3560 Training loss: 1.3939 14.0604 sec/batch\n",
      "Epoch 9/20  Iteration 1517/3560 Training loss: 1.3934 13.9766 sec/batch\n",
      "Epoch 9/20  Iteration 1518/3560 Training loss: 1.3930 14.6657 sec/batch\n",
      "Epoch 9/20  Iteration 1519/3560 Training loss: 1.3927 14.0155 sec/batch\n",
      "Epoch 9/20  Iteration 1520/3560 Training loss: 1.3926 14.0095 sec/batch\n",
      "Epoch 9/20  Iteration 1521/3560 Training loss: 1.3925 13.9640 sec/batch\n",
      "Epoch 9/20  Iteration 1522/3560 Training loss: 1.3921 14.0042 sec/batch\n",
      "Epoch 9/20  Iteration 1523/3560 Training loss: 1.3919 13.9711 sec/batch\n",
      "Epoch 9/20  Iteration 1524/3560 Training loss: 1.3915 13.9779 sec/batch\n",
      "Epoch 9/20  Iteration 1525/3560 Training loss: 1.3915 14.0186 sec/batch\n",
      "Epoch 9/20  Iteration 1526/3560 Training loss: 1.3913 14.0068 sec/batch\n",
      "Epoch 9/20  Iteration 1527/3560 Training loss: 1.3912 14.0767 sec/batch\n",
      "Epoch 9/20  Iteration 1528/3560 Training loss: 1.3911 13.9938 sec/batch\n",
      "Epoch 9/20  Iteration 1529/3560 Training loss: 1.3909 13.9888 sec/batch\n",
      "Epoch 9/20  Iteration 1530/3560 Training loss: 1.3909 14.0035 sec/batch\n",
      "Epoch 9/20  Iteration 1531/3560 Training loss: 1.3907 14.0302 sec/batch\n",
      "Epoch 9/20  Iteration 1532/3560 Training loss: 1.3906 14.0130 sec/batch\n",
      "Epoch 9/20  Iteration 1533/3560 Training loss: 1.3904 14.0458 sec/batch\n",
      "Epoch 9/20  Iteration 1534/3560 Training loss: 1.3904 13.9782 sec/batch\n",
      "Epoch 9/20  Iteration 1535/3560 Training loss: 1.3902 14.0074 sec/batch\n",
      "Epoch 9/20  Iteration 1536/3560 Training loss: 1.3900 13.9805 sec/batch\n",
      "Epoch 9/20  Iteration 1537/3560 Training loss: 1.3898 13.9649 sec/batch\n",
      "Epoch 9/20  Iteration 1538/3560 Training loss: 1.3895 13.9493 sec/batch\n",
      "Epoch 9/20  Iteration 1539/3560 Training loss: 1.3892 14.4588 sec/batch\n",
      "Epoch 9/20  Iteration 1540/3560 Training loss: 1.3888 14.1120 sec/batch\n",
      "Epoch 9/20  Iteration 1541/3560 Training loss: 1.3886 14.0605 sec/batch\n",
      "Epoch 9/20  Iteration 1542/3560 Training loss: 1.3886 14.0759 sec/batch\n",
      "Epoch 9/20  Iteration 1543/3560 Training loss: 1.3883 13.9628 sec/batch\n",
      "Epoch 9/20  Iteration 1544/3560 Training loss: 1.3883 14.0272 sec/batch\n",
      "Epoch 9/20  Iteration 1545/3560 Training loss: 1.3882 14.0129 sec/batch\n",
      "Epoch 9/20  Iteration 1546/3560 Training loss: 1.3877 14.0570 sec/batch\n",
      "Epoch 9/20  Iteration 1547/3560 Training loss: 1.3873 13.9818 sec/batch\n",
      "Epoch 9/20  Iteration 1548/3560 Training loss: 1.3873 14.0835 sec/batch\n",
      "Epoch 9/20  Iteration 1549/3560 Training loss: 1.3871 14.0289 sec/batch\n",
      "Epoch 9/20  Iteration 1550/3560 Training loss: 1.3867 14.1525 sec/batch\n",
      "Epoch 9/20  Iteration 1551/3560 Training loss: 1.3867 14.0360 sec/batch\n",
      "Epoch 9/20  Iteration 1552/3560 Training loss: 1.3867 14.0467 sec/batch\n",
      "Epoch 9/20  Iteration 1553/3560 Training loss: 1.3865 14.0680 sec/batch\n",
      "Epoch 9/20  Iteration 1554/3560 Training loss: 1.3862 14.0143 sec/batch\n",
      "Epoch 9/20  Iteration 1555/3560 Training loss: 1.3858 14.0350 sec/batch\n",
      "Epoch 9/20  Iteration 1556/3560 Training loss: 1.3855 14.0445 sec/batch\n",
      "Epoch 9/20  Iteration 1557/3560 Training loss: 1.3856 14.0334 sec/batch\n",
      "Epoch 9/20  Iteration 1558/3560 Training loss: 1.3856 14.0757 sec/batch\n",
      "Epoch 9/20  Iteration 1559/3560 Training loss: 1.3855 14.0670 sec/batch\n",
      "Epoch 9/20  Iteration 1560/3560 Training loss: 1.3855 14.4597 sec/batch\n",
      "Epoch 9/20  Iteration 1561/3560 Training loss: 1.3856 14.8377 sec/batch\n",
      "Epoch 9/20  Iteration 1562/3560 Training loss: 1.3857 14.0913 sec/batch\n",
      "Epoch 9/20  Iteration 1563/3560 Training loss: 1.3857 14.0153 sec/batch\n",
      "Epoch 9/20  Iteration 1564/3560 Training loss: 1.3855 14.0383 sec/batch\n",
      "Epoch 9/20  Iteration 1565/3560 Training loss: 1.3858 14.0861 sec/batch\n",
      "Epoch 9/20  Iteration 1566/3560 Training loss: 1.3858 14.0635 sec/batch\n",
      "Epoch 9/20  Iteration 1567/3560 Training loss: 1.3856 14.1247 sec/batch\n",
      "Epoch 9/20  Iteration 1568/3560 Training loss: 1.3857 14.3387 sec/batch\n",
      "Epoch 9/20  Iteration 1569/3560 Training loss: 1.3855 14.7076 sec/batch\n",
      "Epoch 9/20  Iteration 1570/3560 Training loss: 1.3856 14.0410 sec/batch\n",
      "Epoch 9/20  Iteration 1571/3560 Training loss: 1.3856 13.7262 sec/batch\n",
      "Epoch 9/20  Iteration 1572/3560 Training loss: 1.3858 13.9191 sec/batch\n",
      "Epoch 9/20  Iteration 1573/3560 Training loss: 1.3859 13.7591 sec/batch\n",
      "Epoch 9/20  Iteration 1574/3560 Training loss: 1.3858 13.7780 sec/batch\n",
      "Epoch 9/20  Iteration 1575/3560 Training loss: 1.3854 13.7602 sec/batch\n",
      "Epoch 9/20  Iteration 1576/3560 Training loss: 1.3852 13.7673 sec/batch\n",
      "Epoch 9/20  Iteration 1577/3560 Training loss: 1.3852 13.7566 sec/batch\n",
      "Epoch 9/20  Iteration 1578/3560 Training loss: 1.3851 13.7498 sec/batch\n",
      "Epoch 9/20  Iteration 1579/3560 Training loss: 1.3851 13.7106 sec/batch\n",
      "Epoch 9/20  Iteration 1580/3560 Training loss: 1.3850 13.6536 sec/batch\n",
      "Epoch 9/20  Iteration 1581/3560 Training loss: 1.3850 13.6909 sec/batch\n",
      "Epoch 9/20  Iteration 1582/3560 Training loss: 1.3849 14.1008 sec/batch\n",
      "Epoch 9/20  Iteration 1583/3560 Training loss: 1.3846 13.7181 sec/batch\n",
      "Epoch 9/20  Iteration 1584/3560 Training loss: 1.3846 13.6903 sec/batch\n",
      "Epoch 9/20  Iteration 1585/3560 Training loss: 1.3848 13.8403 sec/batch\n",
      "Epoch 9/20  Iteration 1586/3560 Training loss: 1.3847 13.7652 sec/batch\n",
      "Epoch 9/20  Iteration 1587/3560 Training loss: 1.3846 13.7223 sec/batch\n",
      "Epoch 9/20  Iteration 1588/3560 Training loss: 1.3845 13.7339 sec/batch\n",
      "Epoch 9/20  Iteration 1589/3560 Training loss: 1.3845 13.7382 sec/batch\n",
      "Epoch 9/20  Iteration 1590/3560 Training loss: 1.3843 13.7569 sec/batch\n",
      "Epoch 9/20  Iteration 1591/3560 Training loss: 1.3844 13.7181 sec/batch\n",
      "Epoch 9/20  Iteration 1592/3560 Training loss: 1.3847 13.7332 sec/batch\n",
      "Epoch 9/20  Iteration 1593/3560 Training loss: 1.3847 13.7556 sec/batch\n",
      "Epoch 9/20  Iteration 1594/3560 Training loss: 1.3847 13.7036 sec/batch\n",
      "Epoch 9/20  Iteration 1595/3560 Training loss: 1.3845 13.7090 sec/batch\n",
      "Epoch 9/20  Iteration 1596/3560 Training loss: 1.3843 13.7637 sec/batch\n",
      "Epoch 9/20  Iteration 1597/3560 Training loss: 1.3844 13.7431 sec/batch\n",
      "Epoch 9/20  Iteration 1598/3560 Training loss: 1.3843 13.6836 sec/batch\n",
      "Epoch 9/20  Iteration 1599/3560 Training loss: 1.3843 13.7481 sec/batch\n",
      "Epoch 9/20  Iteration 1600/3560 Training loss: 1.3841 13.7450 sec/batch\n",
      "Validation loss: 1.26598 Saving checkpoint!\n",
      "Epoch 9/20  Iteration 1601/3560 Training loss: 1.3849 13.7015 sec/batch\n",
      "Epoch 9/20  Iteration 1602/3560 Training loss: 1.3852 13.7114 sec/batch\n",
      "Epoch 10/20  Iteration 1603/3560 Training loss: 1.4948 13.8187 sec/batch\n",
      "Epoch 10/20  Iteration 1604/3560 Training loss: 1.4459 13.7520 sec/batch\n",
      "Epoch 10/20  Iteration 1605/3560 Training loss: 1.4232 13.7416 sec/batch\n",
      "Epoch 10/20  Iteration 1606/3560 Training loss: 1.4163 13.7586 sec/batch\n",
      "Epoch 10/20  Iteration 1607/3560 Training loss: 1.4042 13.6738 sec/batch\n",
      "Epoch 10/20  Iteration 1608/3560 Training loss: 1.3910 13.6923 sec/batch\n",
      "Epoch 10/20  Iteration 1609/3560 Training loss: 1.3912 13.7441 sec/batch\n",
      "Epoch 10/20  Iteration 1610/3560 Training loss: 1.3881 13.6862 sec/batch\n",
      "Epoch 10/20  Iteration 1611/3560 Training loss: 1.3883 13.7193 sec/batch\n",
      "Epoch 10/20  Iteration 1612/3560 Training loss: 1.3867 13.8172 sec/batch\n",
      "Epoch 10/20  Iteration 1613/3560 Training loss: 1.3815 13.7060 sec/batch\n",
      "Epoch 10/20  Iteration 1614/3560 Training loss: 1.3801 13.7302 sec/batch\n",
      "Epoch 10/20  Iteration 1615/3560 Training loss: 1.3797 13.7106 sec/batch\n",
      "Epoch 10/20  Iteration 1616/3560 Training loss: 1.3806 13.7337 sec/batch\n",
      "Epoch 10/20  Iteration 1617/3560 Training loss: 1.3792 13.7532 sec/batch\n",
      "Epoch 10/20  Iteration 1618/3560 Training loss: 1.3773 14.0909 sec/batch\n",
      "Epoch 10/20  Iteration 1619/3560 Training loss: 1.3771 13.7632 sec/batch\n",
      "Epoch 10/20  Iteration 1620/3560 Training loss: 1.3780 13.7433 sec/batch\n",
      "Epoch 10/20  Iteration 1621/3560 Training loss: 1.3772 13.8465 sec/batch\n",
      "Epoch 10/20  Iteration 1622/3560 Training loss: 1.3783 13.7424 sec/batch\n",
      "Epoch 10/20  Iteration 1623/3560 Training loss: 1.3772 13.7538 sec/batch\n",
      "Epoch 10/20  Iteration 1624/3560 Training loss: 1.3771 13.7619 sec/batch\n",
      "Epoch 10/20  Iteration 1625/3560 Training loss: 1.3758 13.7722 sec/batch\n",
      "Epoch 10/20  Iteration 1626/3560 Training loss: 1.3757 13.7442 sec/batch\n",
      "Epoch 10/20  Iteration 1627/3560 Training loss: 1.3752 13.7406 sec/batch\n",
      "Epoch 10/20  Iteration 1628/3560 Training loss: 1.3732 13.7497 sec/batch\n",
      "Epoch 10/20  Iteration 1629/3560 Training loss: 1.3717 13.7576 sec/batch\n",
      "Epoch 10/20  Iteration 1630/3560 Training loss: 1.3720 13.7884 sec/batch\n",
      "Epoch 10/20  Iteration 1631/3560 Training loss: 1.3720 13.7177 sec/batch\n",
      "Epoch 10/20  Iteration 1632/3560 Training loss: 1.3719 13.8272 sec/batch\n",
      "Epoch 10/20  Iteration 1633/3560 Training loss: 1.3711 13.6667 sec/batch\n",
      "Epoch 10/20  Iteration 1634/3560 Training loss: 1.3696 13.7266 sec/batch\n",
      "Epoch 10/20  Iteration 1635/3560 Training loss: 1.3697 13.7200 sec/batch\n",
      "Epoch 10/20  Iteration 1636/3560 Training loss: 1.3696 13.7845 sec/batch\n",
      "Epoch 10/20  Iteration 1637/3560 Training loss: 1.3694 13.6959 sec/batch\n",
      "Epoch 10/20  Iteration 1638/3560 Training loss: 1.3692 13.8343 sec/batch\n",
      "Epoch 10/20  Iteration 1639/3560 Training loss: 1.3682 13.6766 sec/batch\n",
      "Epoch 10/20  Iteration 1640/3560 Training loss: 1.3670 14.9004 sec/batch\n",
      "Epoch 10/20  Iteration 1641/3560 Training loss: 1.3655 13.7325 sec/batch\n",
      "Epoch 10/20  Iteration 1642/3560 Training loss: 1.3650 13.6761 sec/batch\n",
      "Epoch 10/20  Iteration 1643/3560 Training loss: 1.3645 13.7504 sec/batch\n",
      "Epoch 10/20  Iteration 1644/3560 Training loss: 1.3650 13.7531 sec/batch\n",
      "Epoch 10/20  Iteration 1645/3560 Training loss: 1.3645 13.7430 sec/batch\n",
      "Epoch 10/20  Iteration 1646/3560 Training loss: 1.3635 13.7138 sec/batch\n",
      "Epoch 10/20  Iteration 1647/3560 Training loss: 1.3636 13.7952 sec/batch\n",
      "Epoch 10/20  Iteration 1648/3560 Training loss: 1.3627 13.7042 sec/batch\n",
      "Epoch 10/20  Iteration 1649/3560 Training loss: 1.3621 13.7913 sec/batch\n",
      "Epoch 10/20  Iteration 1650/3560 Training loss: 1.3617 13.7473 sec/batch\n",
      "Epoch 10/20  Iteration 1651/3560 Training loss: 1.3616 13.6559 sec/batch\n",
      "Epoch 10/20  Iteration 1652/3560 Training loss: 1.3617 13.7831 sec/batch\n",
      "Epoch 10/20  Iteration 1653/3560 Training loss: 1.3610 13.7456 sec/batch\n",
      "Epoch 10/20  Iteration 1654/3560 Training loss: 1.3617 13.7134 sec/batch\n",
      "Epoch 10/20  Iteration 1655/3560 Training loss: 1.3613 13.7408 sec/batch\n",
      "Epoch 10/20  Iteration 1656/3560 Training loss: 1.3614 13.8006 sec/batch\n",
      "Epoch 10/20  Iteration 1657/3560 Training loss: 1.3610 13.7284 sec/batch\n",
      "Epoch 10/20  Iteration 1658/3560 Training loss: 1.3609 13.7007 sec/batch\n",
      "Epoch 10/20  Iteration 1659/3560 Training loss: 1.3613 13.6786 sec/batch\n",
      "Epoch 10/20  Iteration 1660/3560 Training loss: 1.3608 13.7727 sec/batch\n",
      "Epoch 10/20  Iteration 1661/3560 Training loss: 1.3602 13.7899 sec/batch\n",
      "Epoch 10/20  Iteration 1662/3560 Training loss: 1.3607 14.0367 sec/batch\n",
      "Epoch 10/20  Iteration 1663/3560 Training loss: 1.3606 13.6809 sec/batch\n",
      "Epoch 10/20  Iteration 1664/3560 Training loss: 1.3614 13.8193 sec/batch\n",
      "Epoch 10/20  Iteration 1665/3560 Training loss: 1.3617 13.7465 sec/batch\n",
      "Epoch 10/20  Iteration 1666/3560 Training loss: 1.3616 13.7045 sec/batch\n",
      "Epoch 10/20  Iteration 1667/3560 Training loss: 1.3616 13.7252 sec/batch\n",
      "Epoch 10/20  Iteration 1668/3560 Training loss: 1.3616 13.7163 sec/batch\n",
      "Epoch 10/20  Iteration 1669/3560 Training loss: 1.3617 13.6760 sec/batch\n",
      "Epoch 10/20  Iteration 1670/3560 Training loss: 1.3614 13.7200 sec/batch\n",
      "Epoch 10/20  Iteration 1671/3560 Training loss: 1.3614 13.7421 sec/batch\n",
      "Epoch 10/20  Iteration 1672/3560 Training loss: 1.3612 13.6787 sec/batch\n",
      "Epoch 10/20  Iteration 1673/3560 Training loss: 1.3617 13.8197 sec/batch\n",
      "Epoch 10/20  Iteration 1674/3560 Training loss: 1.3620 13.7497 sec/batch\n",
      "Epoch 10/20  Iteration 1675/3560 Training loss: 1.3624 13.7071 sec/batch\n",
      "Epoch 10/20  Iteration 1676/3560 Training loss: 1.3620 13.7983 sec/batch\n",
      "Epoch 10/20  Iteration 1677/3560 Training loss: 1.3618 13.7035 sec/batch\n",
      "Epoch 10/20  Iteration 1678/3560 Training loss: 1.3618 13.7577 sec/batch\n",
      "Epoch 10/20  Iteration 1679/3560 Training loss: 1.3617 13.7024 sec/batch\n",
      "Epoch 10/20  Iteration 1680/3560 Training loss: 1.3615 13.7665 sec/batch\n",
      "Epoch 10/20  Iteration 1681/3560 Training loss: 1.3608 13.7223 sec/batch\n",
      "Epoch 10/20  Iteration 1682/3560 Training loss: 1.3608 13.8495 sec/batch\n",
      "Epoch 10/20  Iteration 1683/3560 Training loss: 1.3603 14.0112 sec/batch\n",
      "Epoch 10/20  Iteration 1684/3560 Training loss: 1.3601 14.0605 sec/batch\n",
      "Epoch 10/20  Iteration 1685/3560 Training loss: 1.3595 13.6695 sec/batch\n",
      "Epoch 10/20  Iteration 1686/3560 Training loss: 1.3594 13.6789 sec/batch\n",
      "Epoch 10/20  Iteration 1687/3560 Training loss: 1.3590 13.7112 sec/batch\n",
      "Epoch 10/20  Iteration 1688/3560 Training loss: 1.3587 13.8445 sec/batch\n",
      "Epoch 10/20  Iteration 1689/3560 Training loss: 1.3584 13.7449 sec/batch\n",
      "Epoch 10/20  Iteration 1690/3560 Training loss: 1.3582 13.7768 sec/batch\n",
      "Epoch 10/20  Iteration 1691/3560 Training loss: 1.3578 13.7194 sec/batch\n",
      "Epoch 10/20  Iteration 1692/3560 Training loss: 1.3578 13.8560 sec/batch\n",
      "Epoch 10/20  Iteration 1693/3560 Training loss: 1.3575 14.6353 sec/batch\n",
      "Epoch 10/20  Iteration 1694/3560 Training loss: 1.3573 13.9378 sec/batch\n",
      "Epoch 10/20  Iteration 1695/3560 Training loss: 1.3569 13.9035 sec/batch\n",
      "Epoch 10/20  Iteration 1696/3560 Training loss: 1.3564 13.8752 sec/batch\n",
      "Epoch 10/20  Iteration 1697/3560 Training loss: 1.3562 13.8438 sec/batch\n",
      "Epoch 10/20  Iteration 1698/3560 Training loss: 1.3562 14.1018 sec/batch\n",
      "Epoch 10/20  Iteration 1699/3560 Training loss: 1.3561 13.8808 sec/batch\n",
      "Epoch 10/20  Iteration 1700/3560 Training loss: 1.3557 13.8179 sec/batch\n",
      "Epoch 10/20  Iteration 1701/3560 Training loss: 1.3553 13.8162 sec/batch\n",
      "Epoch 10/20  Iteration 1702/3560 Training loss: 1.3548 13.8417 sec/batch\n",
      "Epoch 10/20  Iteration 1703/3560 Training loss: 1.3547 13.9324 sec/batch\n",
      "Epoch 10/20  Iteration 1704/3560 Training loss: 1.3545 13.8771 sec/batch\n",
      "Epoch 10/20  Iteration 1705/3560 Training loss: 1.3544 14.3613 sec/batch\n",
      "Epoch 10/20  Iteration 1706/3560 Training loss: 1.3543 13.8332 sec/batch\n",
      "Epoch 10/20  Iteration 1707/3560 Training loss: 1.3541 13.8397 sec/batch\n",
      "Epoch 10/20  Iteration 1708/3560 Training loss: 1.3541 13.8717 sec/batch\n",
      "Epoch 10/20  Iteration 1709/3560 Training loss: 1.3540 13.7990 sec/batch\n",
      "Epoch 10/20  Iteration 1710/3560 Training loss: 1.3540 13.8138 sec/batch\n",
      "Epoch 10/20  Iteration 1711/3560 Training loss: 1.3537 13.8616 sec/batch\n",
      "Epoch 10/20  Iteration 1712/3560 Training loss: 1.3537 13.8624 sec/batch\n",
      "Epoch 10/20  Iteration 1713/3560 Training loss: 1.3535 13.7853 sec/batch\n",
      "Epoch 10/20  Iteration 1714/3560 Training loss: 1.3533 13.7635 sec/batch\n",
      "Epoch 10/20  Iteration 1715/3560 Training loss: 1.3532 13.8736 sec/batch\n",
      "Epoch 10/20  Iteration 1716/3560 Training loss: 1.3530 13.8144 sec/batch\n",
      "Epoch 10/20  Iteration 1717/3560 Training loss: 1.3527 13.9234 sec/batch\n",
      "Epoch 10/20  Iteration 1718/3560 Training loss: 1.3523 13.7996 sec/batch\n",
      "Epoch 10/20  Iteration 1719/3560 Training loss: 1.3522 13.8571 sec/batch\n",
      "Epoch 10/20  Iteration 1720/3560 Training loss: 1.3521 13.8102 sec/batch\n",
      "Epoch 10/20  Iteration 1721/3560 Training loss: 1.3520 13.7450 sec/batch\n",
      "Epoch 10/20  Iteration 1722/3560 Training loss: 1.3519 13.8469 sec/batch\n",
      "Epoch 10/20  Iteration 1723/3560 Training loss: 1.3518 13.7726 sec/batch\n",
      "Epoch 10/20  Iteration 1724/3560 Training loss: 1.3513 13.8059 sec/batch\n",
      "Epoch 10/20  Iteration 1725/3560 Training loss: 1.3509 13.8971 sec/batch\n",
      "Epoch 10/20  Iteration 1726/3560 Training loss: 1.3509 13.7789 sec/batch\n",
      "Epoch 10/20  Iteration 1727/3560 Training loss: 1.3508 14.8961 sec/batch\n",
      "Epoch 10/20  Iteration 1728/3560 Training loss: 1.3502 13.8626 sec/batch\n",
      "Epoch 10/20  Iteration 1729/3560 Training loss: 1.3502 13.7658 sec/batch\n",
      "Epoch 10/20  Iteration 1730/3560 Training loss: 1.3501 13.7377 sec/batch\n",
      "Epoch 10/20  Iteration 1731/3560 Training loss: 1.3499 13.8097 sec/batch\n",
      "Epoch 10/20  Iteration 1732/3560 Training loss: 1.3495 13.8349 sec/batch\n",
      "Epoch 10/20  Iteration 1733/3560 Training loss: 1.3491 13.8330 sec/batch\n",
      "Epoch 10/20  Iteration 1734/3560 Training loss: 1.3488 13.9247 sec/batch\n",
      "Epoch 10/20  Iteration 1735/3560 Training loss: 1.3489 13.8577 sec/batch\n",
      "Epoch 10/20  Iteration 1736/3560 Training loss: 1.3489 13.7513 sec/batch\n",
      "Epoch 10/20  Iteration 1737/3560 Training loss: 1.3489 13.6958 sec/batch\n",
      "Epoch 10/20  Iteration 1738/3560 Training loss: 1.3489 13.7286 sec/batch\n",
      "Epoch 10/20  Iteration 1739/3560 Training loss: 1.3491 13.7818 sec/batch\n",
      "Epoch 10/20  Iteration 1740/3560 Training loss: 1.3491 13.8531 sec/batch\n",
      "Epoch 10/20  Iteration 1741/3560 Training loss: 1.3491 13.9571 sec/batch\n",
      "Epoch 10/20  Iteration 1742/3560 Training loss: 1.3490 13.8743 sec/batch\n",
      "Epoch 10/20  Iteration 1743/3560 Training loss: 1.3493 13.7866 sec/batch\n",
      "Epoch 10/20  Iteration 1744/3560 Training loss: 1.3493 13.8473 sec/batch\n",
      "Epoch 10/20  Iteration 1745/3560 Training loss: 1.3492 13.8298 sec/batch\n",
      "Epoch 10/20  Iteration 1746/3560 Training loss: 1.3493 13.8391 sec/batch\n",
      "Epoch 10/20  Iteration 1747/3560 Training loss: 1.3491 13.8102 sec/batch\n",
      "Epoch 10/20  Iteration 1748/3560 Training loss: 1.3492 14.0998 sec/batch\n",
      "Epoch 10/20  Iteration 1749/3560 Training loss: 1.3492 13.8015 sec/batch\n",
      "Epoch 10/20  Iteration 1750/3560 Training loss: 1.3494 13.8661 sec/batch\n",
      "Epoch 10/20  Iteration 1751/3560 Training loss: 1.3495 13.8674 sec/batch\n",
      "Epoch 10/20  Iteration 1752/3560 Training loss: 1.3493 13.8043 sec/batch\n",
      "Epoch 10/20  Iteration 1753/3560 Training loss: 1.3490 13.8143 sec/batch\n",
      "Epoch 10/20  Iteration 1754/3560 Training loss: 1.3487 13.8371 sec/batch\n",
      "Epoch 10/20  Iteration 1755/3560 Training loss: 1.3488 13.7820 sec/batch\n",
      "Epoch 10/20  Iteration 1756/3560 Training loss: 1.3486 13.8068 sec/batch\n",
      "Epoch 10/20  Iteration 1757/3560 Training loss: 1.3485 13.8457 sec/batch\n",
      "Epoch 10/20  Iteration 1758/3560 Training loss: 1.3485 13.7985 sec/batch\n",
      "Epoch 10/20  Iteration 1759/3560 Training loss: 1.3485 13.8261 sec/batch\n",
      "Epoch 10/20  Iteration 1760/3560 Training loss: 1.3484 13.9792 sec/batch\n",
      "Epoch 10/20  Iteration 1761/3560 Training loss: 1.3481 13.7911 sec/batch\n",
      "Epoch 10/20  Iteration 1762/3560 Training loss: 1.3481 13.8547 sec/batch\n",
      "Epoch 10/20  Iteration 1763/3560 Training loss: 1.3484 13.8654 sec/batch\n",
      "Epoch 10/20  Iteration 1764/3560 Training loss: 1.3484 13.8326 sec/batch\n",
      "Epoch 10/20  Iteration 1765/3560 Training loss: 1.3482 13.8769 sec/batch\n",
      "Epoch 10/20  Iteration 1766/3560 Training loss: 1.3482 13.8891 sec/batch\n",
      "Epoch 10/20  Iteration 1767/3560 Training loss: 1.3481 13.8202 sec/batch\n",
      "Epoch 10/20  Iteration 1768/3560 Training loss: 1.3480 13.8212 sec/batch\n",
      "Epoch 10/20  Iteration 1769/3560 Training loss: 1.3480 13.8105 sec/batch\n",
      "Epoch 10/20  Iteration 1770/3560 Training loss: 1.3484 14.8606 sec/batch\n",
      "Epoch 10/20  Iteration 1771/3560 Training loss: 1.3484 13.8765 sec/batch\n",
      "Epoch 10/20  Iteration 1772/3560 Training loss: 1.3483 13.8082 sec/batch\n",
      "Epoch 10/20  Iteration 1773/3560 Training loss: 1.3482 13.8086 sec/batch\n",
      "Epoch 10/20  Iteration 1774/3560 Training loss: 1.3480 13.8059 sec/batch\n",
      "Epoch 10/20  Iteration 1775/3560 Training loss: 1.3481 13.8341 sec/batch\n",
      "Epoch 10/20  Iteration 1776/3560 Training loss: 1.3481 13.7860 sec/batch\n",
      "Epoch 10/20  Iteration 1777/3560 Training loss: 1.3482 13.8712 sec/batch\n",
      "Epoch 10/20  Iteration 1778/3560 Training loss: 1.3480 13.8370 sec/batch\n",
      "Epoch 10/20  Iteration 1779/3560 Training loss: 1.3478 13.7668 sec/batch\n",
      "Epoch 10/20  Iteration 1780/3560 Training loss: 1.3479 13.7774 sec/batch\n",
      "Epoch 11/20  Iteration 1781/3560 Training loss: 1.4575 13.8127 sec/batch\n",
      "Epoch 11/20  Iteration 1782/3560 Training loss: 1.4133 13.7779 sec/batch\n",
      "Epoch 11/20  Iteration 1783/3560 Training loss: 1.3929 13.7595 sec/batch\n",
      "Epoch 11/20  Iteration 1784/3560 Training loss: 1.3842 13.7970 sec/batch\n",
      "Epoch 11/20  Iteration 1785/3560 Training loss: 1.3720 13.8287 sec/batch\n",
      "Epoch 11/20  Iteration 1786/3560 Training loss: 1.3590 13.8442 sec/batch\n",
      "Epoch 11/20  Iteration 1787/3560 Training loss: 1.3572 13.8195 sec/batch\n",
      "Epoch 11/20  Iteration 1788/3560 Training loss: 1.3553 13.8330 sec/batch\n",
      "Epoch 11/20  Iteration 1789/3560 Training loss: 1.3537 13.9405 sec/batch\n",
      "Epoch 11/20  Iteration 1790/3560 Training loss: 1.3524 13.7792 sec/batch\n",
      "Epoch 11/20  Iteration 1791/3560 Training loss: 1.3481 13.8274 sec/batch\n",
      "Epoch 11/20  Iteration 1792/3560 Training loss: 1.3459 14.4520 sec/batch\n",
      "Epoch 11/20  Iteration 1793/3560 Training loss: 1.3450 13.7764 sec/batch\n",
      "Epoch 11/20  Iteration 1794/3560 Training loss: 1.3459 13.8571 sec/batch\n",
      "Epoch 11/20  Iteration 1795/3560 Training loss: 1.3446 13.8751 sec/batch\n",
      "Epoch 11/20  Iteration 1796/3560 Training loss: 1.3421 13.7906 sec/batch\n",
      "Epoch 11/20  Iteration 1797/3560 Training loss: 1.3420 13.8404 sec/batch\n",
      "Epoch 11/20  Iteration 1798/3560 Training loss: 1.3428 13.7546 sec/batch\n",
      "Epoch 11/20  Iteration 1799/3560 Training loss: 1.3424 13.8045 sec/batch\n",
      "Epoch 11/20  Iteration 1800/3560 Training loss: 1.3434 13.7740 sec/batch\n",
      "Validation loss: 1.23649 Saving checkpoint!\n",
      "Epoch 11/20  Iteration 1801/3560 Training loss: 1.3500 13.7564 sec/batch\n",
      "Epoch 11/20  Iteration 1802/3560 Training loss: 1.3503 13.8648 sec/batch\n",
      "Epoch 11/20  Iteration 1803/3560 Training loss: 1.3493 13.8311 sec/batch\n",
      "Epoch 11/20  Iteration 1804/3560 Training loss: 1.3488 13.7992 sec/batch\n",
      "Epoch 11/20  Iteration 1805/3560 Training loss: 1.3483 13.7675 sec/batch\n",
      "Epoch 11/20  Iteration 1806/3560 Training loss: 1.3465 15.2527 sec/batch\n",
      "Epoch 11/20  Iteration 1807/3560 Training loss: 1.3451 13.8141 sec/batch\n",
      "Epoch 11/20  Iteration 1808/3560 Training loss: 1.3455 13.8576 sec/batch\n",
      "Epoch 11/20  Iteration 1809/3560 Training loss: 1.3453 13.8838 sec/batch\n",
      "Epoch 11/20  Iteration 1810/3560 Training loss: 1.3452 13.8009 sec/batch\n",
      "Epoch 11/20  Iteration 1811/3560 Training loss: 1.3443 13.8066 sec/batch\n",
      "Epoch 11/20  Iteration 1812/3560 Training loss: 1.3429 13.8340 sec/batch\n",
      "Epoch 11/20  Iteration 1813/3560 Training loss: 1.3429 13.8158 sec/batch\n",
      "Epoch 11/20  Iteration 1814/3560 Training loss: 1.3428 13.8755 sec/batch\n",
      "Epoch 11/20  Iteration 1815/3560 Training loss: 1.3422 13.8525 sec/batch\n",
      "Epoch 11/20  Iteration 1816/3560 Training loss: 1.3418 13.7841 sec/batch\n",
      "Epoch 11/20  Iteration 1817/3560 Training loss: 1.3407 13.8807 sec/batch\n",
      "Epoch 11/20  Iteration 1818/3560 Training loss: 1.3395 13.7807 sec/batch\n",
      "Epoch 11/20  Iteration 1819/3560 Training loss: 1.3382 13.7912 sec/batch\n",
      "Epoch 11/20  Iteration 1820/3560 Training loss: 1.3376 13.8734 sec/batch\n",
      "Epoch 11/20  Iteration 1821/3560 Training loss: 1.3370 13.8583 sec/batch\n",
      "Epoch 11/20  Iteration 1822/3560 Training loss: 1.3376 13.9199 sec/batch\n",
      "Epoch 11/20  Iteration 1823/3560 Training loss: 1.3370 13.8719 sec/batch\n",
      "Epoch 11/20  Iteration 1824/3560 Training loss: 1.3361 13.8477 sec/batch\n",
      "Epoch 11/20  Iteration 1825/3560 Training loss: 1.3361 13.8166 sec/batch\n",
      "Epoch 11/20  Iteration 1826/3560 Training loss: 1.3352 13.8655 sec/batch\n",
      "Epoch 11/20  Iteration 1827/3560 Training loss: 1.3346 14.1642 sec/batch\n",
      "Epoch 11/20  Iteration 1828/3560 Training loss: 1.3340 14.1870 sec/batch\n",
      "Epoch 11/20  Iteration 1829/3560 Training loss: 1.3337 13.8240 sec/batch\n",
      "Epoch 11/20  Iteration 1830/3560 Training loss: 1.3337 13.9024 sec/batch\n",
      "Epoch 11/20  Iteration 1831/3560 Training loss: 1.3332 13.8094 sec/batch\n",
      "Epoch 11/20  Iteration 1832/3560 Training loss: 1.3335 13.8533 sec/batch\n",
      "Epoch 11/20  Iteration 1833/3560 Training loss: 1.3333 14.0482 sec/batch\n",
      "Epoch 11/20  Iteration 1834/3560 Training loss: 1.3333 13.7934 sec/batch\n",
      "Epoch 11/20  Iteration 1835/3560 Training loss: 1.3328 13.8240 sec/batch\n",
      "Epoch 11/20  Iteration 1836/3560 Training loss: 1.3326 13.8726 sec/batch\n",
      "Epoch 11/20  Iteration 1837/3560 Training loss: 1.3330 13.8185 sec/batch\n",
      "Epoch 11/20  Iteration 1838/3560 Training loss: 1.3326 13.8382 sec/batch\n",
      "Epoch 11/20  Iteration 1839/3560 Training loss: 1.3321 13.9426 sec/batch\n",
      "Epoch 11/20  Iteration 1840/3560 Training loss: 1.3325 13.7888 sec/batch\n",
      "Epoch 11/20  Iteration 1841/3560 Training loss: 1.3324 13.8180 sec/batch\n",
      "Epoch 11/20  Iteration 1842/3560 Training loss: 1.3332 13.8456 sec/batch\n",
      "Epoch 11/20  Iteration 1843/3560 Training loss: 1.3334 13.8043 sec/batch\n",
      "Epoch 11/20  Iteration 1844/3560 Training loss: 1.3333 13.8004 sec/batch\n",
      "Epoch 11/20  Iteration 1845/3560 Training loss: 1.3331 13.8252 sec/batch\n",
      "Epoch 11/20  Iteration 1846/3560 Training loss: 1.3331 13.8249 sec/batch\n",
      "Epoch 11/20  Iteration 1847/3560 Training loss: 1.3333 13.8856 sec/batch\n",
      "Epoch 11/20  Iteration 1848/3560 Training loss: 1.3329 13.9281 sec/batch\n",
      "Epoch 11/20  Iteration 1849/3560 Training loss: 1.3328 14.4798 sec/batch\n",
      "Epoch 11/20  Iteration 1850/3560 Training loss: 1.3326 13.8981 sec/batch\n",
      "Epoch 11/20  Iteration 1851/3560 Training loss: 1.3331 13.8688 sec/batch\n",
      "Epoch 11/20  Iteration 1852/3560 Training loss: 1.3332 13.8476 sec/batch\n",
      "Epoch 11/20  Iteration 1853/3560 Training loss: 1.3336 13.8410 sec/batch\n",
      "Epoch 11/20  Iteration 1854/3560 Training loss: 1.3333 13.8803 sec/batch\n",
      "Epoch 11/20  Iteration 1855/3560 Training loss: 1.3332 13.7858 sec/batch\n",
      "Epoch 11/20  Iteration 1856/3560 Training loss: 1.3333 13.8632 sec/batch\n",
      "Epoch 11/20  Iteration 1857/3560 Training loss: 1.3329 13.7767 sec/batch\n",
      "Epoch 11/20  Iteration 1858/3560 Training loss: 1.3327 13.8668 sec/batch\n",
      "Epoch 11/20  Iteration 1859/3560 Training loss: 1.3319 13.8292 sec/batch\n",
      "Epoch 11/20  Iteration 1860/3560 Training loss: 1.3318 13.8248 sec/batch\n",
      "Epoch 11/20  Iteration 1861/3560 Training loss: 1.3313 13.7916 sec/batch\n",
      "Epoch 11/20  Iteration 1862/3560 Training loss: 1.3312 13.8670 sec/batch\n",
      "Epoch 11/20  Iteration 1863/3560 Training loss: 1.3307 13.7880 sec/batch\n",
      "Epoch 11/20  Iteration 1864/3560 Training loss: 1.3305 13.7926 sec/batch\n",
      "Epoch 11/20  Iteration 1865/3560 Training loss: 1.3302 13.9073 sec/batch\n",
      "Epoch 11/20  Iteration 1866/3560 Training loss: 1.3300 13.8282 sec/batch\n",
      "Epoch 11/20  Iteration 1867/3560 Training loss: 1.3296 13.8882 sec/batch\n",
      "Epoch 11/20  Iteration 1868/3560 Training loss: 1.3293 13.7981 sec/batch\n",
      "Epoch 11/20  Iteration 1869/3560 Training loss: 1.3289 13.7697 sec/batch\n",
      "Epoch 11/20  Iteration 1870/3560 Training loss: 1.3289 13.8394 sec/batch\n",
      "Epoch 11/20  Iteration 1871/3560 Training loss: 1.3287 14.3455 sec/batch\n",
      "Epoch 11/20  Iteration 1872/3560 Training loss: 1.3285 13.8597 sec/batch\n",
      "Epoch 11/20  Iteration 1873/3560 Training loss: 1.3280 13.8893 sec/batch\n",
      "Epoch 11/20  Iteration 1874/3560 Training loss: 1.3276 13.8270 sec/batch\n",
      "Epoch 11/20  Iteration 1875/3560 Training loss: 1.3274 13.8867 sec/batch\n",
      "Epoch 11/20  Iteration 1876/3560 Training loss: 1.3272 13.8771 sec/batch\n",
      "Epoch 11/20  Iteration 1877/3560 Training loss: 1.3271 13.8796 sec/batch\n",
      "Epoch 11/20  Iteration 1878/3560 Training loss: 1.3268 13.8739 sec/batch\n",
      "Epoch 11/20  Iteration 1879/3560 Training loss: 1.3264 13.8517 sec/batch\n",
      "Epoch 11/20  Iteration 1880/3560 Training loss: 1.3259 13.8430 sec/batch\n",
      "Epoch 11/20  Iteration 1881/3560 Training loss: 1.3258 13.9687 sec/batch\n",
      "Epoch 11/20  Iteration 1882/3560 Training loss: 1.3256 13.9448 sec/batch\n",
      "Epoch 11/20  Iteration 1883/3560 Training loss: 1.3255 13.8575 sec/batch\n",
      "Epoch 11/20  Iteration 1884/3560 Training loss: 1.3253 13.8998 sec/batch\n",
      "Epoch 11/20  Iteration 1885/3560 Training loss: 1.3251 13.8582 sec/batch\n",
      "Epoch 11/20  Iteration 1886/3560 Training loss: 1.3249 13.8051 sec/batch\n",
      "Epoch 11/20  Iteration 1887/3560 Training loss: 1.3247 13.8201 sec/batch\n",
      "Epoch 11/20  Iteration 1888/3560 Training loss: 1.3247 13.8734 sec/batch\n",
      "Epoch 11/20  Iteration 1889/3560 Training loss: 1.3245 13.8734 sec/batch\n",
      "Epoch 11/20  Iteration 1890/3560 Training loss: 1.3245 13.9007 sec/batch\n",
      "Epoch 11/20  Iteration 1891/3560 Training loss: 1.3243 13.9377 sec/batch\n",
      "Epoch 11/20  Iteration 1892/3560 Training loss: 1.3241 14.9004 sec/batch\n",
      "Epoch 11/20  Iteration 1893/3560 Training loss: 1.3240 13.8553 sec/batch\n",
      "Epoch 11/20  Iteration 1894/3560 Training loss: 1.3239 13.8695 sec/batch\n",
      "Epoch 11/20  Iteration 1895/3560 Training loss: 1.3235 13.8438 sec/batch\n",
      "Epoch 11/20  Iteration 1896/3560 Training loss: 1.3233 13.7965 sec/batch\n",
      "Epoch 11/20  Iteration 1897/3560 Training loss: 1.3232 13.7861 sec/batch\n",
      "Epoch 11/20  Iteration 1898/3560 Training loss: 1.3231 13.8928 sec/batch\n",
      "Epoch 11/20  Iteration 1899/3560 Training loss: 1.3229 13.8324 sec/batch\n",
      "Epoch 11/20  Iteration 1900/3560 Training loss: 1.3228 13.7694 sec/batch\n",
      "Epoch 11/20  Iteration 1901/3560 Training loss: 1.3227 13.9122 sec/batch\n",
      "Epoch 11/20  Iteration 1902/3560 Training loss: 1.3222 13.8472 sec/batch\n",
      "Epoch 11/20  Iteration 1903/3560 Training loss: 1.3218 13.8353 sec/batch\n",
      "Epoch 11/20  Iteration 1904/3560 Training loss: 1.3216 13.8166 sec/batch\n",
      "Epoch 11/20  Iteration 1905/3560 Training loss: 1.3216 13.8065 sec/batch\n",
      "Epoch 11/20  Iteration 1906/3560 Training loss: 1.3211 13.9243 sec/batch\n",
      "Epoch 11/20  Iteration 1907/3560 Training loss: 1.3211 13.8348 sec/batch\n",
      "Epoch 11/20  Iteration 1908/3560 Training loss: 1.3210 13.8929 sec/batch\n",
      "Epoch 11/20  Iteration 1909/3560 Training loss: 1.3208 13.7977 sec/batch\n",
      "Epoch 11/20  Iteration 1910/3560 Training loss: 1.3204 13.8816 sec/batch\n",
      "Epoch 11/20  Iteration 1911/3560 Training loss: 1.3201 13.8209 sec/batch\n",
      "Epoch 11/20  Iteration 1912/3560 Training loss: 1.3199 13.9032 sec/batch\n",
      "Epoch 11/20  Iteration 1913/3560 Training loss: 1.3199 13.8713 sec/batch\n",
      "Epoch 11/20  Iteration 1914/3560 Training loss: 1.3199 14.1667 sec/batch\n",
      "Epoch 11/20  Iteration 1915/3560 Training loss: 1.3198 13.9384 sec/batch\n",
      "Epoch 11/20  Iteration 1916/3560 Training loss: 1.3198 13.8794 sec/batch\n",
      "Epoch 11/20  Iteration 1917/3560 Training loss: 1.3201 13.9501 sec/batch\n",
      "Epoch 11/20  Iteration 1918/3560 Training loss: 1.3202 13.8446 sec/batch\n",
      "Epoch 11/20  Iteration 1919/3560 Training loss: 1.3202 13.8672 sec/batch\n",
      "Epoch 11/20  Iteration 1920/3560 Training loss: 1.3201 13.7479 sec/batch\n",
      "Epoch 11/20  Iteration 1921/3560 Training loss: 1.3205 13.7931 sec/batch\n",
      "Epoch 11/20  Iteration 1922/3560 Training loss: 1.3205 13.8409 sec/batch\n",
      "Epoch 11/20  Iteration 1923/3560 Training loss: 1.3204 13.8688 sec/batch\n",
      "Epoch 11/20  Iteration 1924/3560 Training loss: 1.3206 13.8797 sec/batch\n",
      "Epoch 11/20  Iteration 1925/3560 Training loss: 1.3204 13.8898 sec/batch\n",
      "Epoch 11/20  Iteration 1926/3560 Training loss: 1.3206 13.7837 sec/batch\n",
      "Epoch 11/20  Iteration 1927/3560 Training loss: 1.3206 13.8450 sec/batch\n",
      "Epoch 11/20  Iteration 1928/3560 Training loss: 1.3208 13.9368 sec/batch\n",
      "Epoch 11/20  Iteration 1929/3560 Training loss: 1.3208 13.8249 sec/batch\n",
      "Epoch 11/20  Iteration 1930/3560 Training loss: 1.3206 13.7871 sec/batch\n",
      "Epoch 11/20  Iteration 1931/3560 Training loss: 1.3203 13.9002 sec/batch\n",
      "Epoch 11/20  Iteration 1932/3560 Training loss: 1.3201 13.8254 sec/batch\n",
      "Epoch 11/20  Iteration 1933/3560 Training loss: 1.3201 13.8966 sec/batch\n",
      "Epoch 11/20  Iteration 1934/3560 Training loss: 1.3200 13.9025 sec/batch\n",
      "Epoch 11/20  Iteration 1935/3560 Training loss: 1.3200 13.8710 sec/batch\n",
      "Epoch 11/20  Iteration 1936/3560 Training loss: 1.3199 14.6293 sec/batch\n",
      "Epoch 11/20  Iteration 1937/3560 Training loss: 1.3199 13.8293 sec/batch\n",
      "Epoch 11/20  Iteration 1938/3560 Training loss: 1.3198 13.8645 sec/batch\n",
      "Epoch 11/20  Iteration 1939/3560 Training loss: 1.3195 13.7967 sec/batch\n",
      "Epoch 11/20  Iteration 1940/3560 Training loss: 1.3195 13.8286 sec/batch\n",
      "Epoch 11/20  Iteration 1941/3560 Training loss: 1.3197 13.7777 sec/batch\n",
      "Epoch 11/20  Iteration 1942/3560 Training loss: 1.3197 13.8884 sec/batch\n",
      "Epoch 11/20  Iteration 1943/3560 Training loss: 1.3196 13.9583 sec/batch\n",
      "Epoch 11/20  Iteration 1944/3560 Training loss: 1.3196 13.9141 sec/batch\n",
      "Epoch 11/20  Iteration 1945/3560 Training loss: 1.3195 13.9005 sec/batch\n",
      "Epoch 11/20  Iteration 1946/3560 Training loss: 1.3194 13.8532 sec/batch\n",
      "Epoch 11/20  Iteration 1947/3560 Training loss: 1.3195 13.8328 sec/batch\n",
      "Epoch 11/20  Iteration 1948/3560 Training loss: 1.3200 13.8307 sec/batch\n",
      "Epoch 11/20  Iteration 1949/3560 Training loss: 1.3200 13.8485 sec/batch\n",
      "Epoch 11/20  Iteration 1950/3560 Training loss: 1.3200 13.8378 sec/batch\n",
      "Epoch 11/20  Iteration 1951/3560 Training loss: 1.3200 13.9577 sec/batch\n",
      "Epoch 11/20  Iteration 1952/3560 Training loss: 1.3198 13.8290 sec/batch\n",
      "Epoch 11/20  Iteration 1953/3560 Training loss: 1.3199 13.7280 sec/batch\n",
      "Epoch 11/20  Iteration 1954/3560 Training loss: 1.3199 13.8477 sec/batch\n",
      "Epoch 11/20  Iteration 1955/3560 Training loss: 1.3199 13.7935 sec/batch\n",
      "Epoch 11/20  Iteration 1956/3560 Training loss: 1.3197 13.8083 sec/batch\n",
      "Epoch 11/20  Iteration 1957/3560 Training loss: 1.3196 14.0772 sec/batch\n",
      "Epoch 11/20  Iteration 1958/3560 Training loss: 1.3197 13.8229 sec/batch\n",
      "Epoch 12/20  Iteration 1959/3560 Training loss: 1.4467 14.1034 sec/batch\n",
      "Epoch 12/20  Iteration 1960/3560 Training loss: 1.3935 13.9526 sec/batch\n",
      "Epoch 12/20  Iteration 1961/3560 Training loss: 1.3682 13.9271 sec/batch\n",
      "Epoch 12/20  Iteration 1962/3560 Training loss: 1.3552 13.8151 sec/batch\n",
      "Epoch 12/20  Iteration 1963/3560 Training loss: 1.3407 13.8648 sec/batch\n",
      "Epoch 12/20  Iteration 1964/3560 Training loss: 1.3274 13.8791 sec/batch\n",
      "Epoch 12/20  Iteration 1965/3560 Training loss: 1.3268 13.8388 sec/batch\n",
      "Epoch 12/20  Iteration 1966/3560 Training loss: 1.3239 13.8396 sec/batch\n",
      "Epoch 12/20  Iteration 1967/3560 Training loss: 1.3227 13.8297 sec/batch\n",
      "Epoch 12/20  Iteration 1968/3560 Training loss: 1.3210 13.9447 sec/batch\n",
      "Epoch 12/20  Iteration 1969/3560 Training loss: 1.3167 13.8086 sec/batch\n",
      "Epoch 12/20  Iteration 1970/3560 Training loss: 1.3157 13.8516 sec/batch\n",
      "Epoch 12/20  Iteration 1971/3560 Training loss: 1.3147 14.0920 sec/batch\n",
      "Epoch 12/20  Iteration 1972/3560 Training loss: 1.3157 13.9406 sec/batch\n",
      "Epoch 12/20  Iteration 1973/3560 Training loss: 1.3145 13.8132 sec/batch\n",
      "Epoch 12/20  Iteration 1974/3560 Training loss: 1.3127 13.8394 sec/batch\n",
      "Epoch 12/20  Iteration 1975/3560 Training loss: 1.3125 13.8397 sec/batch\n",
      "Epoch 12/20  Iteration 1976/3560 Training loss: 1.3139 13.8933 sec/batch\n",
      "Epoch 12/20  Iteration 1977/3560 Training loss: 1.3137 13.8849 sec/batch\n",
      "Epoch 12/20  Iteration 1978/3560 Training loss: 1.3148 13.8039 sec/batch\n",
      "Epoch 12/20  Iteration 1979/3560 Training loss: 1.3141 15.0136 sec/batch\n",
      "Epoch 12/20  Iteration 1980/3560 Training loss: 1.3144 13.8911 sec/batch\n",
      "Epoch 12/20  Iteration 1981/3560 Training loss: 1.3133 13.8116 sec/batch\n",
      "Epoch 12/20  Iteration 1982/3560 Training loss: 1.3134 13.8406 sec/batch\n",
      "Epoch 12/20  Iteration 1983/3560 Training loss: 1.3132 13.8360 sec/batch\n",
      "Epoch 12/20  Iteration 1984/3560 Training loss: 1.3115 13.8258 sec/batch\n",
      "Epoch 12/20  Iteration 1985/3560 Training loss: 1.3103 13.8527 sec/batch\n",
      "Epoch 12/20  Iteration 1986/3560 Training loss: 1.3107 13.8609 sec/batch\n",
      "Epoch 12/20  Iteration 1987/3560 Training loss: 1.3107 13.8732 sec/batch\n",
      "Epoch 12/20  Iteration 1988/3560 Training loss: 1.3108 13.8446 sec/batch\n",
      "Epoch 12/20  Iteration 1989/3560 Training loss: 1.3102 13.8200 sec/batch\n",
      "Epoch 12/20  Iteration 1990/3560 Training loss: 1.3091 13.8120 sec/batch\n",
      "Epoch 12/20  Iteration 1991/3560 Training loss: 1.3090 13.7613 sec/batch\n",
      "Epoch 12/20  Iteration 1992/3560 Training loss: 1.3089 13.8367 sec/batch\n",
      "Epoch 12/20  Iteration 1993/3560 Training loss: 1.3085 13.8291 sec/batch\n",
      "Epoch 12/20  Iteration 1994/3560 Training loss: 1.3082 13.9593 sec/batch\n",
      "Epoch 12/20  Iteration 1995/3560 Training loss: 1.3072 13.8409 sec/batch\n",
      "Epoch 12/20  Iteration 1996/3560 Training loss: 1.3059 13.8305 sec/batch\n",
      "Epoch 12/20  Iteration 1997/3560 Training loss: 1.3048 13.8412 sec/batch\n",
      "Epoch 12/20  Iteration 1998/3560 Training loss: 1.3043 13.8144 sec/batch\n",
      "Epoch 12/20  Iteration 1999/3560 Training loss: 1.3036 13.8602 sec/batch\n",
      "Epoch 12/20  Iteration 2000/3560 Training loss: 1.3041 14.2220 sec/batch\n",
      "Validation loss: 1.20788 Saving checkpoint!\n",
      "Epoch 12/20  Iteration 2001/3560 Training loss: 1.3078 13.7642 sec/batch\n",
      "Epoch 12/20  Iteration 2002/3560 Training loss: 1.3077 13.8265 sec/batch\n",
      "Epoch 12/20  Iteration 2003/3560 Training loss: 1.3081 13.8323 sec/batch\n",
      "Epoch 12/20  Iteration 2004/3560 Training loss: 1.3075 13.8599 sec/batch\n",
      "Epoch 12/20  Iteration 2005/3560 Training loss: 1.3070 13.8550 sec/batch\n",
      "Epoch 12/20  Iteration 2006/3560 Training loss: 1.3066 13.8366 sec/batch\n",
      "Epoch 12/20  Iteration 2007/3560 Training loss: 1.3068 13.9235 sec/batch\n",
      "Epoch 12/20  Iteration 2008/3560 Training loss: 1.3069 13.8376 sec/batch\n",
      "Epoch 12/20  Iteration 2009/3560 Training loss: 1.3064 13.8367 sec/batch\n",
      "Epoch 12/20  Iteration 2010/3560 Training loss: 1.3072 13.8087 sec/batch\n",
      "Epoch 12/20  Iteration 2011/3560 Training loss: 1.3071 13.8496 sec/batch\n",
      "Epoch 12/20  Iteration 2012/3560 Training loss: 1.3074 13.8236 sec/batch\n",
      "Epoch 12/20  Iteration 2013/3560 Training loss: 1.3072 13.8569 sec/batch\n",
      "Epoch 12/20  Iteration 2014/3560 Training loss: 1.3073 13.7603 sec/batch\n",
      "Epoch 12/20  Iteration 2015/3560 Training loss: 1.3076 14.4240 sec/batch\n",
      "Epoch 12/20  Iteration 2016/3560 Training loss: 1.3072 13.8393 sec/batch\n",
      "Epoch 12/20  Iteration 2017/3560 Training loss: 1.3067 13.7793 sec/batch\n",
      "Epoch 12/20  Iteration 2018/3560 Training loss: 1.3073 13.8620 sec/batch\n",
      "Epoch 12/20  Iteration 2019/3560 Training loss: 1.3071 13.8314 sec/batch\n",
      "Epoch 12/20  Iteration 2020/3560 Training loss: 1.3079 13.8331 sec/batch\n",
      "Epoch 12/20  Iteration 2021/3560 Training loss: 1.3084 13.8309 sec/batch\n",
      "Epoch 12/20  Iteration 2022/3560 Training loss: 1.3086 13.9001 sec/batch\n",
      "Epoch 12/20  Iteration 2023/3560 Training loss: 1.3084 13.7983 sec/batch\n",
      "Epoch 12/20  Iteration 2024/3560 Training loss: 1.3084 13.8221 sec/batch\n",
      "Epoch 12/20  Iteration 2025/3560 Training loss: 1.3085 13.8450 sec/batch\n",
      "Epoch 12/20  Iteration 2026/3560 Training loss: 1.3081 13.7730 sec/batch\n",
      "Epoch 12/20  Iteration 2027/3560 Training loss: 1.3080 13.7948 sec/batch\n",
      "Epoch 12/20  Iteration 2028/3560 Training loss: 1.3079 13.7990 sec/batch\n",
      "Epoch 12/20  Iteration 2029/3560 Training loss: 1.3083 13.7362 sec/batch\n",
      "Epoch 12/20  Iteration 2030/3560 Training loss: 1.3085 13.8474 sec/batch\n",
      "Epoch 12/20  Iteration 2031/3560 Training loss: 1.3089 13.7851 sec/batch\n",
      "Epoch 12/20  Iteration 2032/3560 Training loss: 1.3085 13.8169 sec/batch\n",
      "Epoch 12/20  Iteration 2033/3560 Training loss: 1.3083 13.8157 sec/batch\n",
      "Epoch 12/20  Iteration 2034/3560 Training loss: 1.3084 13.8386 sec/batch\n",
      "Epoch 12/20  Iteration 2035/3560 Training loss: 1.3082 13.8259 sec/batch\n",
      "Epoch 12/20  Iteration 2036/3560 Training loss: 1.3080 14.3295 sec/batch\n",
      "Epoch 12/20  Iteration 2037/3560 Training loss: 1.3073 13.8857 sec/batch\n",
      "Epoch 12/20  Iteration 2038/3560 Training loss: 1.3072 13.8435 sec/batch\n",
      "Epoch 12/20  Iteration 2039/3560 Training loss: 1.3067 13.8491 sec/batch\n",
      "Epoch 12/20  Iteration 2040/3560 Training loss: 1.3066 13.8570 sec/batch\n",
      "Epoch 12/20  Iteration 2041/3560 Training loss: 1.3061 13.8064 sec/batch\n",
      "Epoch 12/20  Iteration 2042/3560 Training loss: 1.3060 13.7748 sec/batch\n",
      "Epoch 12/20  Iteration 2043/3560 Training loss: 1.3056 13.8529 sec/batch\n",
      "Epoch 12/20  Iteration 2044/3560 Training loss: 1.3055 13.8199 sec/batch\n",
      "Epoch 12/20  Iteration 2045/3560 Training loss: 1.3053 13.7687 sec/batch\n",
      "Epoch 12/20  Iteration 2046/3560 Training loss: 1.3050 13.7788 sec/batch\n",
      "Epoch 12/20  Iteration 2047/3560 Training loss: 1.3045 13.7849 sec/batch\n",
      "Epoch 12/20  Iteration 2048/3560 Training loss: 1.3045 13.7618 sec/batch\n",
      "Epoch 12/20  Iteration 2049/3560 Training loss: 1.3042 13.8566 sec/batch\n",
      "Epoch 12/20  Iteration 2050/3560 Training loss: 1.3040 13.9438 sec/batch\n",
      "Epoch 12/20  Iteration 2051/3560 Training loss: 1.3035 13.8511 sec/batch\n",
      "Epoch 12/20  Iteration 2052/3560 Training loss: 1.3030 13.8096 sec/batch\n",
      "Epoch 12/20  Iteration 2053/3560 Training loss: 1.3027 13.8161 sec/batch\n",
      "Epoch 12/20  Iteration 2054/3560 Training loss: 1.3027 13.8254 sec/batch\n",
      "Epoch 12/20  Iteration 2055/3560 Training loss: 1.3026 13.8218 sec/batch\n",
      "Epoch 12/20  Iteration 2056/3560 Training loss: 1.3022 13.9747 sec/batch\n",
      "Epoch 12/20  Iteration 2057/3560 Training loss: 1.3017 13.8790 sec/batch\n",
      "Epoch 12/20  Iteration 2058/3560 Training loss: 1.3013 15.0658 sec/batch\n",
      "Epoch 12/20  Iteration 2059/3560 Training loss: 1.3012 13.7764 sec/batch\n",
      "Epoch 12/20  Iteration 2060/3560 Training loss: 1.3010 13.8798 sec/batch\n",
      "Epoch 12/20  Iteration 2061/3560 Training loss: 1.3009 13.7814 sec/batch\n",
      "Epoch 12/20  Iteration 2062/3560 Training loss: 1.3008 13.7998 sec/batch\n",
      "Epoch 12/20  Iteration 2063/3560 Training loss: 1.3005 13.8608 sec/batch\n",
      "Epoch 12/20  Iteration 2064/3560 Training loss: 1.3004 13.7735 sec/batch\n",
      "Epoch 12/20  Iteration 2065/3560 Training loss: 1.3004 13.8245 sec/batch\n",
      "Epoch 12/20  Iteration 2066/3560 Training loss: 1.3004 13.8884 sec/batch\n",
      "Epoch 12/20  Iteration 2067/3560 Training loss: 1.3002 13.8308 sec/batch\n",
      "Epoch 12/20  Iteration 2068/3560 Training loss: 1.3003 13.7948 sec/batch\n",
      "Epoch 12/20  Iteration 2069/3560 Training loss: 1.3000 13.8526 sec/batch\n",
      "Epoch 12/20  Iteration 2070/3560 Training loss: 1.2998 13.8075 sec/batch\n",
      "Epoch 12/20  Iteration 2071/3560 Training loss: 1.2997 13.9106 sec/batch\n",
      "Epoch 12/20  Iteration 2072/3560 Training loss: 1.2995 13.7512 sec/batch\n",
      "Epoch 12/20  Iteration 2073/3560 Training loss: 1.2992 13.7714 sec/batch\n",
      "Epoch 12/20  Iteration 2074/3560 Training loss: 1.2989 13.9128 sec/batch\n",
      "Epoch 12/20  Iteration 2075/3560 Training loss: 1.2987 13.8581 sec/batch\n",
      "Epoch 12/20  Iteration 2076/3560 Training loss: 1.2987 13.8628 sec/batch\n",
      "Epoch 12/20  Iteration 2077/3560 Training loss: 1.2985 13.8597 sec/batch\n",
      "Epoch 12/20  Iteration 2078/3560 Training loss: 1.2984 13.7658 sec/batch\n",
      "Epoch 12/20  Iteration 2079/3560 Training loss: 1.2983 13.7730 sec/batch\n",
      "Epoch 12/20  Iteration 2080/3560 Training loss: 1.2979 14.1687 sec/batch\n",
      "Epoch 12/20  Iteration 2081/3560 Training loss: 1.2975 13.8715 sec/batch\n",
      "Epoch 12/20  Iteration 2082/3560 Training loss: 1.2974 13.8042 sec/batch\n",
      "Epoch 12/20  Iteration 2083/3560 Training loss: 1.2973 13.8317 sec/batch\n",
      "Epoch 12/20  Iteration 2084/3560 Training loss: 1.2968 13.8517 sec/batch\n",
      "Epoch 12/20  Iteration 2085/3560 Training loss: 1.2967 13.8931 sec/batch\n",
      "Epoch 12/20  Iteration 2086/3560 Training loss: 1.2967 13.9134 sec/batch\n",
      "Epoch 12/20  Iteration 2087/3560 Training loss: 1.2965 13.8119 sec/batch\n",
      "Epoch 12/20  Iteration 2088/3560 Training loss: 1.2961 13.7402 sec/batch\n",
      "Epoch 12/20  Iteration 2089/3560 Training loss: 1.2957 13.8734 sec/batch\n",
      "Epoch 12/20  Iteration 2090/3560 Training loss: 1.2954 14.1357 sec/batch\n",
      "Epoch 12/20  Iteration 2091/3560 Training loss: 1.2955 13.8636 sec/batch\n",
      "Epoch 12/20  Iteration 2092/3560 Training loss: 1.2955 13.8875 sec/batch\n",
      "Epoch 12/20  Iteration 2093/3560 Training loss: 1.2955 13.8867 sec/batch\n",
      "Epoch 12/20  Iteration 2094/3560 Training loss: 1.2955 13.8069 sec/batch\n",
      "Epoch 12/20  Iteration 2095/3560 Training loss: 1.2956 13.7956 sec/batch\n",
      "Epoch 12/20  Iteration 2096/3560 Training loss: 1.2958 13.8231 sec/batch\n",
      "Epoch 12/20  Iteration 2097/3560 Training loss: 1.2958 13.9556 sec/batch\n",
      "Epoch 12/20  Iteration 2098/3560 Training loss: 1.2957 13.8287 sec/batch\n",
      "Epoch 12/20  Iteration 2099/3560 Training loss: 1.2960 13.8331 sec/batch\n",
      "Epoch 12/20  Iteration 2100/3560 Training loss: 1.2960 13.8771 sec/batch\n",
      "Epoch 12/20  Iteration 2101/3560 Training loss: 1.2960 14.7976 sec/batch\n",
      "Epoch 12/20  Iteration 2102/3560 Training loss: 1.2961 13.8746 sec/batch\n",
      "Epoch 12/20  Iteration 2103/3560 Training loss: 1.2959 13.8490 sec/batch\n",
      "Epoch 12/20  Iteration 2104/3560 Training loss: 1.2960 13.8636 sec/batch\n",
      "Epoch 12/20  Iteration 2105/3560 Training loss: 1.2960 13.7505 sec/batch\n",
      "Epoch 12/20  Iteration 2106/3560 Training loss: 1.2962 13.7679 sec/batch\n",
      "Epoch 12/20  Iteration 2107/3560 Training loss: 1.2963 13.8154 sec/batch\n",
      "Epoch 12/20  Iteration 2108/3560 Training loss: 1.2962 13.9583 sec/batch\n",
      "Epoch 12/20  Iteration 2109/3560 Training loss: 1.2959 13.8301 sec/batch\n",
      "Epoch 12/20  Iteration 2110/3560 Training loss: 1.2957 13.8439 sec/batch\n",
      "Epoch 12/20  Iteration 2111/3560 Training loss: 1.2957 13.7604 sec/batch\n",
      "Epoch 12/20  Iteration 2112/3560 Training loss: 1.2956 13.8652 sec/batch\n",
      "Epoch 12/20  Iteration 2113/3560 Training loss: 1.2955 13.8471 sec/batch\n",
      "Epoch 12/20  Iteration 2114/3560 Training loss: 1.2954 13.7884 sec/batch\n",
      "Epoch 12/20  Iteration 2115/3560 Training loss: 1.2954 13.8449 sec/batch\n",
      "Epoch 12/20  Iteration 2116/3560 Training loss: 1.2954 13.8276 sec/batch\n",
      "Epoch 12/20  Iteration 2117/3560 Training loss: 1.2951 13.9462 sec/batch\n",
      "Epoch 12/20  Iteration 2118/3560 Training loss: 1.2951 13.7974 sec/batch\n",
      "Epoch 12/20  Iteration 2119/3560 Training loss: 1.2953 13.7980 sec/batch\n",
      "Epoch 12/20  Iteration 2120/3560 Training loss: 1.2953 13.7945 sec/batch\n",
      "Epoch 12/20  Iteration 2121/3560 Training loss: 1.2953 13.8629 sec/batch\n",
      "Epoch 12/20  Iteration 2122/3560 Training loss: 1.2952 13.8554 sec/batch\n",
      "Epoch 12/20  Iteration 2123/3560 Training loss: 1.2952 14.1630 sec/batch\n",
      "Epoch 12/20  Iteration 2124/3560 Training loss: 1.2951 13.8582 sec/batch\n",
      "Epoch 12/20  Iteration 2125/3560 Training loss: 1.2952 13.8461 sec/batch\n",
      "Epoch 12/20  Iteration 2126/3560 Training loss: 1.2955 13.8153 sec/batch\n",
      "Epoch 12/20  Iteration 2127/3560 Training loss: 1.2955 13.7555 sec/batch\n",
      "Epoch 12/20  Iteration 2128/3560 Training loss: 1.2955 13.8234 sec/batch\n",
      "Epoch 12/20  Iteration 2129/3560 Training loss: 1.2955 13.7702 sec/batch\n",
      "Epoch 12/20  Iteration 2130/3560 Training loss: 1.2953 13.7668 sec/batch\n",
      "Epoch 12/20  Iteration 2131/3560 Training loss: 1.2954 13.7835 sec/batch\n",
      "Epoch 12/20  Iteration 2132/3560 Training loss: 1.2954 13.8507 sec/batch\n",
      "Epoch 12/20  Iteration 2133/3560 Training loss: 1.2954 13.7973 sec/batch\n",
      "Epoch 12/20  Iteration 2134/3560 Training loss: 1.2952 13.9304 sec/batch\n",
      "Epoch 12/20  Iteration 2135/3560 Training loss: 1.2950 13.8511 sec/batch\n",
      "Epoch 12/20  Iteration 2136/3560 Training loss: 1.2951 13.8203 sec/batch\n",
      "Epoch 13/20  Iteration 2137/3560 Training loss: 1.4146 13.9089 sec/batch\n",
      "Epoch 13/20  Iteration 2138/3560 Training loss: 1.3653 13.8543 sec/batch\n",
      "Epoch 13/20  Iteration 2139/3560 Training loss: 1.3446 13.8131 sec/batch\n",
      "Epoch 13/20  Iteration 2140/3560 Training loss: 1.3336 13.8118 sec/batch\n",
      "Epoch 13/20  Iteration 2141/3560 Training loss: 1.3211 13.7919 sec/batch\n",
      "Epoch 13/20  Iteration 2142/3560 Training loss: 1.3079 13.8227 sec/batch\n",
      "Epoch 13/20  Iteration 2143/3560 Training loss: 1.3058 13.9108 sec/batch\n",
      "Epoch 13/20  Iteration 2144/3560 Training loss: 1.3018 14.2349 sec/batch\n",
      "Epoch 13/20  Iteration 2145/3560 Training loss: 1.3013 14.7163 sec/batch\n",
      "Epoch 13/20  Iteration 2146/3560 Training loss: 1.2990 13.8691 sec/batch\n",
      "Epoch 13/20  Iteration 2147/3560 Training loss: 1.2946 13.8822 sec/batch\n",
      "Epoch 13/20  Iteration 2148/3560 Training loss: 1.2936 13.8295 sec/batch\n",
      "Epoch 13/20  Iteration 2149/3560 Training loss: 1.2925 13.8688 sec/batch\n",
      "Epoch 13/20  Iteration 2150/3560 Training loss: 1.2928 13.7757 sec/batch\n",
      "Epoch 13/20  Iteration 2151/3560 Training loss: 1.2911 13.8467 sec/batch\n",
      "Epoch 13/20  Iteration 2152/3560 Training loss: 1.2891 13.8783 sec/batch\n",
      "Epoch 13/20  Iteration 2153/3560 Training loss: 1.2891 13.8497 sec/batch\n",
      "Epoch 13/20  Iteration 2154/3560 Training loss: 1.2901 13.8112 sec/batch\n",
      "Epoch 13/20  Iteration 2155/3560 Training loss: 1.2896 13.8575 sec/batch\n",
      "Epoch 13/20  Iteration 2156/3560 Training loss: 1.2908 13.8337 sec/batch\n",
      "Epoch 13/20  Iteration 2157/3560 Training loss: 1.2901 13.8641 sec/batch\n",
      "Epoch 13/20  Iteration 2158/3560 Training loss: 1.2900 13.8860 sec/batch\n",
      "Epoch 13/20  Iteration 2159/3560 Training loss: 1.2890 13.7847 sec/batch\n",
      "Epoch 13/20  Iteration 2160/3560 Training loss: 1.2891 13.9734 sec/batch\n",
      "Epoch 13/20  Iteration 2161/3560 Training loss: 1.2889 13.8267 sec/batch\n",
      "Epoch 13/20  Iteration 2162/3560 Training loss: 1.2871 13.9324 sec/batch\n",
      "Epoch 13/20  Iteration 2163/3560 Training loss: 1.2855 13.8219 sec/batch\n",
      "Epoch 13/20  Iteration 2164/3560 Training loss: 1.2858 13.8816 sec/batch\n",
      "Epoch 13/20  Iteration 2165/3560 Training loss: 1.2858 13.8250 sec/batch\n",
      "Epoch 13/20  Iteration 2166/3560 Training loss: 1.2859 14.1993 sec/batch\n",
      "Epoch 13/20  Iteration 2167/3560 Training loss: 1.2855 13.8115 sec/batch\n",
      "Epoch 13/20  Iteration 2168/3560 Training loss: 1.2844 13.8464 sec/batch\n",
      "Epoch 13/20  Iteration 2169/3560 Training loss: 1.2845 13.9897 sec/batch\n",
      "Epoch 13/20  Iteration 2170/3560 Training loss: 1.2847 13.7902 sec/batch\n",
      "Epoch 13/20  Iteration 2171/3560 Training loss: 1.2842 13.8807 sec/batch\n",
      "Epoch 13/20  Iteration 2172/3560 Training loss: 1.2841 13.8655 sec/batch\n",
      "Epoch 13/20  Iteration 2173/3560 Training loss: 1.2832 13.8833 sec/batch\n",
      "Epoch 13/20  Iteration 2174/3560 Training loss: 1.2818 13.8040 sec/batch\n",
      "Epoch 13/20  Iteration 2175/3560 Training loss: 1.2803 13.8755 sec/batch\n",
      "Epoch 13/20  Iteration 2176/3560 Training loss: 1.2800 13.8591 sec/batch\n",
      "Epoch 13/20  Iteration 2177/3560 Training loss: 1.2793 13.9079 sec/batch\n",
      "Epoch 13/20  Iteration 2178/3560 Training loss: 1.2800 13.8452 sec/batch\n",
      "Epoch 13/20  Iteration 2179/3560 Training loss: 1.2798 13.7918 sec/batch\n",
      "Epoch 13/20  Iteration 2180/3560 Training loss: 1.2791 13.8655 sec/batch\n",
      "Epoch 13/20  Iteration 2181/3560 Training loss: 1.2793 13.8542 sec/batch\n",
      "Epoch 13/20  Iteration 2182/3560 Training loss: 1.2787 13.8407 sec/batch\n",
      "Epoch 13/20  Iteration 2183/3560 Training loss: 1.2781 13.7982 sec/batch\n",
      "Epoch 13/20  Iteration 2184/3560 Training loss: 1.2778 13.8681 sec/batch\n",
      "Epoch 13/20  Iteration 2185/3560 Training loss: 1.2777 13.8140 sec/batch\n",
      "Epoch 13/20  Iteration 2186/3560 Training loss: 1.2779 13.9737 sec/batch\n",
      "Epoch 13/20  Iteration 2187/3560 Training loss: 1.2775 13.8335 sec/batch\n",
      "Epoch 13/20  Iteration 2188/3560 Training loss: 1.2780 14.4262 sec/batch\n",
      "Epoch 13/20  Iteration 2189/3560 Training loss: 1.2777 13.8887 sec/batch\n",
      "Epoch 13/20  Iteration 2190/3560 Training loss: 1.2781 13.7492 sec/batch\n",
      "Epoch 13/20  Iteration 2191/3560 Training loss: 1.2777 13.7936 sec/batch\n",
      "Epoch 13/20  Iteration 2192/3560 Training loss: 1.2777 13.8695 sec/batch\n",
      "Epoch 13/20  Iteration 2193/3560 Training loss: 1.2781 13.8198 sec/batch\n",
      "Epoch 13/20  Iteration 2194/3560 Training loss: 1.2778 13.8569 sec/batch\n",
      "Epoch 13/20  Iteration 2195/3560 Training loss: 1.2773 13.9039 sec/batch\n",
      "Epoch 13/20  Iteration 2196/3560 Training loss: 1.2778 13.8275 sec/batch\n",
      "Epoch 13/20  Iteration 2197/3560 Training loss: 1.2777 13.8744 sec/batch\n",
      "Epoch 13/20  Iteration 2198/3560 Training loss: 1.2787 13.9915 sec/batch\n",
      "Epoch 13/20  Iteration 2199/3560 Training loss: 1.2791 14.0257 sec/batch\n",
      "Epoch 13/20  Iteration 2200/3560 Training loss: 1.2793 13.9999 sec/batch\n",
      "Validation loss: 1.18952 Saving checkpoint!\n",
      "Epoch 13/20  Iteration 2201/3560 Training loss: 1.2820 13.7872 sec/batch\n",
      "Epoch 13/20  Iteration 2202/3560 Training loss: 1.2824 14.4344 sec/batch\n",
      "Epoch 13/20  Iteration 2203/3560 Training loss: 1.2826 14.0077 sec/batch\n",
      "Epoch 13/20  Iteration 2204/3560 Training loss: 1.2822 14.0720 sec/batch\n",
      "Epoch 13/20  Iteration 2205/3560 Training loss: 1.2824 13.9807 sec/batch\n",
      "Epoch 13/20  Iteration 2206/3560 Training loss: 1.2823 14.0393 sec/batch\n",
      "Epoch 13/20  Iteration 2207/3560 Training loss: 1.2829 14.0128 sec/batch\n",
      "Epoch 13/20  Iteration 2208/3560 Training loss: 1.2832 14.0064 sec/batch\n",
      "Epoch 13/20  Iteration 2209/3560 Training loss: 1.2836 13.9454 sec/batch\n",
      "Epoch 13/20  Iteration 2210/3560 Training loss: 1.2833 13.9671 sec/batch\n",
      "Epoch 13/20  Iteration 2211/3560 Training loss: 1.2832 14.3746 sec/batch\n",
      "Epoch 13/20  Iteration 2212/3560 Training loss: 1.2834 13.9328 sec/batch\n",
      "Epoch 13/20  Iteration 2213/3560 Training loss: 1.2833 14.0516 sec/batch\n",
      "Epoch 13/20  Iteration 2214/3560 Training loss: 1.2832 13.9163 sec/batch\n",
      "Epoch 13/20  Iteration 2215/3560 Training loss: 1.2826 13.9849 sec/batch\n",
      "Epoch 13/20  Iteration 2216/3560 Training loss: 1.2826 13.9791 sec/batch\n",
      "Epoch 13/20  Iteration 2217/3560 Training loss: 1.2821 13.9472 sec/batch\n",
      "Epoch 13/20  Iteration 2218/3560 Training loss: 1.2820 13.9552 sec/batch\n",
      "Epoch 13/20  Iteration 2219/3560 Training loss: 1.2816 13.9209 sec/batch\n",
      "Epoch 13/20  Iteration 2220/3560 Training loss: 1.2815 13.9406 sec/batch\n",
      "Epoch 13/20  Iteration 2221/3560 Training loss: 1.2812 13.9774 sec/batch\n",
      "Epoch 13/20  Iteration 2222/3560 Training loss: 1.2810 14.0350 sec/batch\n",
      "Epoch 13/20  Iteration 2223/3560 Training loss: 1.2807 15.1062 sec/batch\n",
      "Epoch 13/20  Iteration 2224/3560 Training loss: 1.2804 15.9074 sec/batch\n",
      "Epoch 13/20  Iteration 2225/3560 Training loss: 1.2800 13.9443 sec/batch\n",
      "Epoch 13/20  Iteration 2226/3560 Training loss: 1.2800 14.0132 sec/batch\n",
      "Epoch 13/20  Iteration 2227/3560 Training loss: 1.2797 13.9919 sec/batch\n",
      "Epoch 13/20  Iteration 2228/3560 Training loss: 1.2796 14.0205 sec/batch\n",
      "Epoch 13/20  Iteration 2229/3560 Training loss: 1.2793 13.9895 sec/batch\n",
      "Epoch 13/20  Iteration 2230/3560 Training loss: 1.2789 13.9809 sec/batch\n",
      "Epoch 13/20  Iteration 2231/3560 Training loss: 1.2787 14.1211 sec/batch\n",
      "Epoch 13/20  Iteration 2232/3560 Training loss: 1.2788 13.8992 sec/batch\n",
      "Epoch 13/20  Iteration 2233/3560 Training loss: 1.2787 13.9418 sec/batch\n",
      "Epoch 13/20  Iteration 2234/3560 Training loss: 1.2784 13.8882 sec/batch\n",
      "Epoch 13/20  Iteration 2235/3560 Training loss: 1.2780 13.8778 sec/batch\n",
      "Epoch 13/20  Iteration 2236/3560 Training loss: 1.2777 14.0423 sec/batch\n",
      "Epoch 13/20  Iteration 2237/3560 Training loss: 1.2776 13.9632 sec/batch\n",
      "Epoch 13/20  Iteration 2238/3560 Training loss: 1.2775 13.9556 sec/batch\n",
      "Epoch 13/20  Iteration 2239/3560 Training loss: 1.2774 13.9924 sec/batch\n",
      "Epoch 13/20  Iteration 2240/3560 Training loss: 1.2773 14.0150 sec/batch\n",
      "Epoch 13/20  Iteration 2241/3560 Training loss: 1.2772 13.9534 sec/batch\n",
      "Epoch 13/20  Iteration 2242/3560 Training loss: 1.2770 13.9342 sec/batch\n",
      "Epoch 13/20  Iteration 2243/3560 Training loss: 1.2770 13.9562 sec/batch\n",
      "Epoch 13/20  Iteration 2244/3560 Training loss: 1.2769 14.0831 sec/batch\n",
      "Epoch 13/20  Iteration 2245/3560 Training loss: 1.2767 14.4466 sec/batch\n",
      "Epoch 13/20  Iteration 2246/3560 Training loss: 1.2769 13.9898 sec/batch\n",
      "Epoch 13/20  Iteration 2247/3560 Training loss: 1.2766 13.9313 sec/batch\n",
      "Epoch 13/20  Iteration 2248/3560 Training loss: 1.2765 13.9870 sec/batch\n",
      "Epoch 13/20  Iteration 2249/3560 Training loss: 1.2765 13.9297 sec/batch\n",
      "Epoch 13/20  Iteration 2250/3560 Training loss: 1.2763 13.8849 sec/batch\n",
      "Epoch 13/20  Iteration 2251/3560 Training loss: 1.2759 13.9829 sec/batch\n",
      "Epoch 13/20  Iteration 2252/3560 Training loss: 1.2757 13.9190 sec/batch\n",
      "Epoch 13/20  Iteration 2253/3560 Training loss: 1.2756 13.9992 sec/batch\n",
      "Epoch 13/20  Iteration 2254/3560 Training loss: 1.2756 13.9290 sec/batch\n",
      "Epoch 13/20  Iteration 2255/3560 Training loss: 1.2755 13.9621 sec/batch\n",
      "Epoch 13/20  Iteration 2256/3560 Training loss: 1.2755 13.9864 sec/batch\n",
      "Epoch 13/20  Iteration 2257/3560 Training loss: 1.2753 13.8107 sec/batch\n",
      "Epoch 13/20  Iteration 2258/3560 Training loss: 1.2749 14.0758 sec/batch\n",
      "Epoch 13/20  Iteration 2259/3560 Training loss: 1.2745 13.9711 sec/batch\n",
      "Epoch 13/20  Iteration 2260/3560 Training loss: 1.2744 13.9813 sec/batch\n",
      "Epoch 13/20  Iteration 2261/3560 Training loss: 1.2743 13.9720 sec/batch\n",
      "Epoch 13/20  Iteration 2262/3560 Training loss: 1.2739 14.0073 sec/batch\n",
      "Epoch 13/20  Iteration 2263/3560 Training loss: 1.2739 13.9504 sec/batch\n",
      "Epoch 13/20  Iteration 2264/3560 Training loss: 1.2739 14.0597 sec/batch\n",
      "Epoch 13/20  Iteration 2265/3560 Training loss: 1.2737 14.0388 sec/batch\n",
      "Epoch 13/20  Iteration 2266/3560 Training loss: 1.2734 14.5772 sec/batch\n",
      "Epoch 13/20  Iteration 2267/3560 Training loss: 1.2730 13.9827 sec/batch\n",
      "Epoch 13/20  Iteration 2268/3560 Training loss: 1.2727 13.9562 sec/batch\n",
      "Epoch 13/20  Iteration 2269/3560 Training loss: 1.2728 13.9480 sec/batch\n",
      "Epoch 13/20  Iteration 2270/3560 Training loss: 1.2728 13.9425 sec/batch\n",
      "Epoch 13/20  Iteration 2271/3560 Training loss: 1.2728 13.9554 sec/batch\n",
      "Epoch 13/20  Iteration 2272/3560 Training loss: 1.2728 14.0205 sec/batch\n",
      "Epoch 13/20  Iteration 2273/3560 Training loss: 1.2730 14.0673 sec/batch\n",
      "Epoch 13/20  Iteration 2274/3560 Training loss: 1.2731 13.9571 sec/batch\n",
      "Epoch 13/20  Iteration 2275/3560 Training loss: 1.2731 13.9768 sec/batch\n",
      "Epoch 13/20  Iteration 2276/3560 Training loss: 1.2730 13.8842 sec/batch\n",
      "Epoch 13/20  Iteration 2277/3560 Training loss: 1.2734 13.9258 sec/batch\n",
      "Epoch 13/20  Iteration 2278/3560 Training loss: 1.2734 13.9381 sec/batch\n",
      "Epoch 13/20  Iteration 2279/3560 Training loss: 1.2733 13.9381 sec/batch\n",
      "Epoch 13/20  Iteration 2280/3560 Training loss: 1.2735 14.0241 sec/batch\n",
      "Epoch 13/20  Iteration 2281/3560 Training loss: 1.2733 13.9465 sec/batch\n",
      "Epoch 13/20  Iteration 2282/3560 Training loss: 1.2734 13.9134 sec/batch\n",
      "Epoch 13/20  Iteration 2283/3560 Training loss: 1.2734 13.9172 sec/batch\n",
      "Epoch 13/20  Iteration 2284/3560 Training loss: 1.2736 14.0462 sec/batch\n",
      "Epoch 13/20  Iteration 2285/3560 Training loss: 1.2737 13.9165 sec/batch\n",
      "Epoch 13/20  Iteration 2286/3560 Training loss: 1.2735 13.9431 sec/batch\n",
      "Epoch 13/20  Iteration 2287/3560 Training loss: 1.2732 14.6112 sec/batch\n",
      "Epoch 13/20  Iteration 2288/3560 Training loss: 1.2730 14.0374 sec/batch\n",
      "Epoch 13/20  Iteration 2289/3560 Training loss: 1.2730 13.9846 sec/batch\n",
      "Epoch 13/20  Iteration 2290/3560 Training loss: 1.2730 13.9190 sec/batch\n",
      "Epoch 13/20  Iteration 2291/3560 Training loss: 1.2729 13.9225 sec/batch\n",
      "Epoch 13/20  Iteration 2292/3560 Training loss: 1.2729 13.9991 sec/batch\n",
      "Epoch 13/20  Iteration 2293/3560 Training loss: 1.2729 13.8673 sec/batch\n",
      "Epoch 13/20  Iteration 2294/3560 Training loss: 1.2728 14.0041 sec/batch\n",
      "Epoch 13/20  Iteration 2295/3560 Training loss: 1.2725 13.9950 sec/batch\n",
      "Epoch 13/20  Iteration 2296/3560 Training loss: 1.2726 14.0097 sec/batch\n",
      "Epoch 13/20  Iteration 2297/3560 Training loss: 1.2728 13.9143 sec/batch\n",
      "Epoch 13/20  Iteration 2298/3560 Training loss: 1.2729 13.9006 sec/batch\n",
      "Epoch 13/20  Iteration 2299/3560 Training loss: 1.2728 13.9264 sec/batch\n",
      "Epoch 13/20  Iteration 2300/3560 Training loss: 1.2727 14.0034 sec/batch\n",
      "Epoch 13/20  Iteration 2301/3560 Training loss: 1.2727 13.9789 sec/batch\n",
      "Epoch 13/20  Iteration 2302/3560 Training loss: 1.2725 13.9349 sec/batch\n",
      "Epoch 13/20  Iteration 2303/3560 Training loss: 1.2727 13.8530 sec/batch\n",
      "Epoch 13/20  Iteration 2304/3560 Training loss: 1.2731 13.9816 sec/batch\n",
      "Epoch 13/20  Iteration 2305/3560 Training loss: 1.2730 14.0645 sec/batch\n",
      "Epoch 13/20  Iteration 2306/3560 Training loss: 1.2730 13.9435 sec/batch\n",
      "Epoch 13/20  Iteration 2307/3560 Training loss: 1.2730 13.9695 sec/batch\n",
      "Epoch 13/20  Iteration 2308/3560 Training loss: 1.2728 13.9853 sec/batch\n",
      "Epoch 13/20  Iteration 2309/3560 Training loss: 1.2730 15.0805 sec/batch\n",
      "Epoch 13/20  Iteration 2310/3560 Training loss: 1.2730 14.0068 sec/batch\n",
      "Epoch 13/20  Iteration 2311/3560 Training loss: 1.2730 13.9303 sec/batch\n",
      "Epoch 13/20  Iteration 2312/3560 Training loss: 1.2728 13.8727 sec/batch\n",
      "Epoch 13/20  Iteration 2313/3560 Training loss: 1.2727 13.9186 sec/batch\n",
      "Epoch 13/20  Iteration 2314/3560 Training loss: 1.2728 14.0789 sec/batch\n",
      "Epoch 14/20  Iteration 2315/3560 Training loss: 1.3848 13.9604 sec/batch\n",
      "Epoch 14/20  Iteration 2316/3560 Training loss: 1.3357 13.9391 sec/batch\n",
      "Epoch 14/20  Iteration 2317/3560 Training loss: 1.3165 13.8917 sec/batch\n",
      "Epoch 14/20  Iteration 2318/3560 Training loss: 1.3079 13.8912 sec/batch\n",
      "Epoch 14/20  Iteration 2319/3560 Training loss: 1.2969 13.9715 sec/batch\n",
      "Epoch 14/20  Iteration 2320/3560 Training loss: 1.2852 13.9783 sec/batch\n",
      "Epoch 14/20  Iteration 2321/3560 Training loss: 1.2842 13.9339 sec/batch\n",
      "Epoch 14/20  Iteration 2322/3560 Training loss: 1.2802 14.0328 sec/batch\n",
      "Epoch 14/20  Iteration 2323/3560 Training loss: 1.2795 13.9842 sec/batch\n",
      "Epoch 14/20  Iteration 2324/3560 Training loss: 1.2786 14.1115 sec/batch\n",
      "Epoch 14/20  Iteration 2325/3560 Training loss: 1.2756 13.9804 sec/batch\n",
      "Epoch 14/20  Iteration 2326/3560 Training loss: 1.2744 13.9431 sec/batch\n",
      "Epoch 14/20  Iteration 2327/3560 Training loss: 1.2736 13.9225 sec/batch\n",
      "Epoch 14/20  Iteration 2328/3560 Training loss: 1.2739 14.0066 sec/batch\n",
      "Epoch 14/20  Iteration 2329/3560 Training loss: 1.2723 13.8897 sec/batch\n",
      "Epoch 14/20  Iteration 2330/3560 Training loss: 1.2699 14.4760 sec/batch\n",
      "Epoch 14/20  Iteration 2331/3560 Training loss: 1.2699 13.9614 sec/batch\n",
      "Epoch 14/20  Iteration 2332/3560 Training loss: 1.2706 13.9600 sec/batch\n",
      "Epoch 14/20  Iteration 2333/3560 Training loss: 1.2701 14.0835 sec/batch\n",
      "Epoch 14/20  Iteration 2334/3560 Training loss: 1.2714 13.9460 sec/batch\n",
      "Epoch 14/20  Iteration 2335/3560 Training loss: 1.2706 13.9593 sec/batch\n",
      "Epoch 14/20  Iteration 2336/3560 Training loss: 1.2707 13.8920 sec/batch\n",
      "Epoch 14/20  Iteration 2337/3560 Training loss: 1.2696 14.0125 sec/batch\n",
      "Epoch 14/20  Iteration 2338/3560 Training loss: 1.2697 13.9796 sec/batch\n",
      "Epoch 14/20  Iteration 2339/3560 Training loss: 1.2692 13.9819 sec/batch\n",
      "Epoch 14/20  Iteration 2340/3560 Training loss: 1.2670 13.9629 sec/batch\n",
      "Epoch 14/20  Iteration 2341/3560 Training loss: 1.2658 13.9373 sec/batch\n",
      "Epoch 14/20  Iteration 2342/3560 Training loss: 1.2664 13.9466 sec/batch\n",
      "Epoch 14/20  Iteration 2343/3560 Training loss: 1.2662 13.9440 sec/batch\n",
      "Epoch 14/20  Iteration 2344/3560 Training loss: 1.2663 14.0874 sec/batch\n",
      "Epoch 14/20  Iteration 2345/3560 Training loss: 1.2655 13.8979 sec/batch\n",
      "Epoch 14/20  Iteration 2346/3560 Training loss: 1.2646 14.3065 sec/batch\n",
      "Epoch 14/20  Iteration 2347/3560 Training loss: 1.2646 13.9246 sec/batch\n",
      "Epoch 14/20  Iteration 2348/3560 Training loss: 1.2648 13.9713 sec/batch\n",
      "Epoch 14/20  Iteration 2349/3560 Training loss: 1.2645 13.8911 sec/batch\n",
      "Epoch 14/20  Iteration 2350/3560 Training loss: 1.2643 13.9240 sec/batch\n",
      "Epoch 14/20  Iteration 2351/3560 Training loss: 1.2636 14.0692 sec/batch\n",
      "Epoch 14/20  Iteration 2352/3560 Training loss: 1.2624 14.6155 sec/batch\n",
      "Epoch 14/20  Iteration 2353/3560 Training loss: 1.2610 13.9585 sec/batch\n",
      "Epoch 14/20  Iteration 2354/3560 Training loss: 1.2605 13.9831 sec/batch\n",
      "Epoch 14/20  Iteration 2355/3560 Training loss: 1.2600 13.8343 sec/batch\n",
      "Epoch 14/20  Iteration 2356/3560 Training loss: 1.2607 13.9584 sec/batch\n",
      "Epoch 14/20  Iteration 2357/3560 Training loss: 1.2604 13.9902 sec/batch\n",
      "Epoch 14/20  Iteration 2358/3560 Training loss: 1.2597 13.8734 sec/batch\n",
      "Epoch 14/20  Iteration 2359/3560 Training loss: 1.2599 14.0731 sec/batch\n",
      "Epoch 14/20  Iteration 2360/3560 Training loss: 1.2590 13.9384 sec/batch\n",
      "Epoch 14/20  Iteration 2361/3560 Training loss: 1.2587 13.9699 sec/batch\n",
      "Epoch 14/20  Iteration 2362/3560 Training loss: 1.2583 13.9949 sec/batch\n",
      "Epoch 14/20  Iteration 2363/3560 Training loss: 1.2582 13.9675 sec/batch\n",
      "Epoch 14/20  Iteration 2364/3560 Training loss: 1.2584 13.9471 sec/batch\n",
      "Epoch 14/20  Iteration 2365/3560 Training loss: 1.2578 13.9931 sec/batch\n",
      "Epoch 14/20  Iteration 2366/3560 Training loss: 1.2587 13.9759 sec/batch\n",
      "Epoch 14/20  Iteration 2367/3560 Training loss: 1.2585 14.2104 sec/batch\n",
      "Epoch 14/20  Iteration 2368/3560 Training loss: 1.2588 13.9243 sec/batch\n",
      "Epoch 14/20  Iteration 2369/3560 Training loss: 1.2586 13.9770 sec/batch\n",
      "Epoch 14/20  Iteration 2370/3560 Training loss: 1.2586 13.9474 sec/batch\n",
      "Epoch 14/20  Iteration 2371/3560 Training loss: 1.2590 13.9013 sec/batch\n",
      "Epoch 14/20  Iteration 2372/3560 Training loss: 1.2586 13.9040 sec/batch\n",
      "Epoch 14/20  Iteration 2373/3560 Training loss: 1.2580 14.3976 sec/batch\n",
      "Epoch 14/20  Iteration 2374/3560 Training loss: 1.2587 14.0865 sec/batch\n",
      "Epoch 14/20  Iteration 2375/3560 Training loss: 1.2587 13.9967 sec/batch\n",
      "Epoch 14/20  Iteration 2376/3560 Training loss: 1.2594 14.0619 sec/batch\n",
      "Epoch 14/20  Iteration 2377/3560 Training loss: 1.2598 13.9642 sec/batch\n",
      "Epoch 14/20  Iteration 2378/3560 Training loss: 1.2599 13.9708 sec/batch\n",
      "Epoch 14/20  Iteration 2379/3560 Training loss: 1.2599 14.0409 sec/batch\n",
      "Epoch 14/20  Iteration 2380/3560 Training loss: 1.2599 13.9885 sec/batch\n",
      "Epoch 14/20  Iteration 2381/3560 Training loss: 1.2600 13.9675 sec/batch\n",
      "Epoch 14/20  Iteration 2382/3560 Training loss: 1.2599 13.9555 sec/batch\n",
      "Epoch 14/20  Iteration 2383/3560 Training loss: 1.2599 13.9755 sec/batch\n",
      "Epoch 14/20  Iteration 2384/3560 Training loss: 1.2596 14.0258 sec/batch\n",
      "Epoch 14/20  Iteration 2385/3560 Training loss: 1.2601 13.9698 sec/batch\n",
      "Epoch 14/20  Iteration 2386/3560 Training loss: 1.2603 13.8881 sec/batch\n",
      "Epoch 14/20  Iteration 2387/3560 Training loss: 1.2607 14.0548 sec/batch\n",
      "Epoch 14/20  Iteration 2388/3560 Training loss: 1.2604 13.8382 sec/batch\n",
      "Epoch 14/20  Iteration 2389/3560 Training loss: 1.2602 14.0234 sec/batch\n",
      "Epoch 14/20  Iteration 2390/3560 Training loss: 1.2604 14.0276 sec/batch\n",
      "Epoch 14/20  Iteration 2391/3560 Training loss: 1.2602 13.9553 sec/batch\n",
      "Epoch 14/20  Iteration 2392/3560 Training loss: 1.2601 13.9524 sec/batch\n",
      "Epoch 14/20  Iteration 2393/3560 Training loss: 1.2594 14.0263 sec/batch\n",
      "Epoch 14/20  Iteration 2394/3560 Training loss: 1.2594 14.1439 sec/batch\n",
      "Epoch 14/20  Iteration 2395/3560 Training loss: 1.2589 14.8396 sec/batch\n",
      "Epoch 14/20  Iteration 2396/3560 Training loss: 1.2588 13.9471 sec/batch\n",
      "Epoch 14/20  Iteration 2397/3560 Training loss: 1.2584 13.9555 sec/batch\n",
      "Epoch 14/20  Iteration 2398/3560 Training loss: 1.2584 13.9378 sec/batch\n",
      "Epoch 14/20  Iteration 2399/3560 Training loss: 1.2580 14.1052 sec/batch\n",
      "Epoch 14/20  Iteration 2400/3560 Training loss: 1.2579 13.9024 sec/batch\n",
      "Validation loss: 1.17479 Saving checkpoint!\n",
      "Epoch 14/20  Iteration 2401/3560 Training loss: 1.2598 13.7786 sec/batch\n",
      "Epoch 14/20  Iteration 2402/3560 Training loss: 1.2597 13.9494 sec/batch\n",
      "Epoch 14/20  Iteration 2403/3560 Training loss: 1.2595 14.0251 sec/batch\n",
      "Epoch 14/20  Iteration 2404/3560 Training loss: 1.2596 14.0042 sec/batch\n",
      "Epoch 14/20  Iteration 2405/3560 Training loss: 1.2594 13.9863 sec/batch\n",
      "Epoch 14/20  Iteration 2406/3560 Training loss: 1.2594 13.9129 sec/batch\n",
      "Epoch 14/20  Iteration 2407/3560 Training loss: 1.2591 13.8872 sec/batch\n",
      "Epoch 14/20  Iteration 2408/3560 Training loss: 1.2589 14.0642 sec/batch\n",
      "Epoch 14/20  Iteration 2409/3560 Training loss: 1.2587 14.3442 sec/batch\n",
      "Epoch 14/20  Iteration 2410/3560 Training loss: 1.2588 13.9078 sec/batch\n",
      "Epoch 14/20  Iteration 2411/3560 Training loss: 1.2587 14.0220 sec/batch\n",
      "Epoch 14/20  Iteration 2412/3560 Training loss: 1.2584 13.9989 sec/batch\n",
      "Epoch 14/20  Iteration 2413/3560 Training loss: 1.2580 13.9952 sec/batch\n",
      "Epoch 14/20  Iteration 2414/3560 Training loss: 1.2577 13.9454 sec/batch\n",
      "Epoch 14/20  Iteration 2415/3560 Training loss: 1.2577 13.9969 sec/batch\n",
      "Epoch 14/20  Iteration 2416/3560 Training loss: 1.2576 13.9069 sec/batch\n",
      "Epoch 14/20  Iteration 2417/3560 Training loss: 1.2576 13.8933 sec/batch\n",
      "Epoch 14/20  Iteration 2418/3560 Training loss: 1.2575 14.1219 sec/batch\n",
      "Epoch 14/20  Iteration 2419/3560 Training loss: 1.2574 13.9784 sec/batch\n",
      "Epoch 14/20  Iteration 2420/3560 Training loss: 1.2572 14.0509 sec/batch\n",
      "Epoch 14/20  Iteration 2421/3560 Training loss: 1.2572 13.9202 sec/batch\n",
      "Epoch 14/20  Iteration 2422/3560 Training loss: 1.2572 14.0868 sec/batch\n",
      "Epoch 14/20  Iteration 2423/3560 Training loss: 1.2569 13.9551 sec/batch\n",
      "Epoch 14/20  Iteration 2424/3560 Training loss: 1.2570 13.8690 sec/batch\n",
      "Epoch 14/20  Iteration 2425/3560 Training loss: 1.2567 13.9145 sec/batch\n",
      "Epoch 14/20  Iteration 2426/3560 Training loss: 1.2567 14.0418 sec/batch\n",
      "Epoch 14/20  Iteration 2427/3560 Training loss: 1.2566 13.9733 sec/batch\n",
      "Epoch 14/20  Iteration 2428/3560 Training loss: 1.2564 13.9788 sec/batch\n",
      "Epoch 14/20  Iteration 2429/3560 Training loss: 1.2561 13.9226 sec/batch\n",
      "Epoch 14/20  Iteration 2430/3560 Training loss: 1.2558 14.7981 sec/batch\n",
      "Epoch 14/20  Iteration 2431/3560 Training loss: 1.2558 13.8881 sec/batch\n",
      "Epoch 14/20  Iteration 2432/3560 Training loss: 1.2558 13.8771 sec/batch\n",
      "Epoch 14/20  Iteration 2433/3560 Training loss: 1.2556 13.9453 sec/batch\n",
      "Epoch 14/20  Iteration 2434/3560 Training loss: 1.2556 14.0039 sec/batch\n",
      "Epoch 14/20  Iteration 2435/3560 Training loss: 1.2555 13.9318 sec/batch\n",
      "Epoch 14/20  Iteration 2436/3560 Training loss: 1.2552 13.9182 sec/batch\n",
      "Epoch 14/20  Iteration 2437/3560 Training loss: 1.2549 14.0456 sec/batch\n",
      "Epoch 14/20  Iteration 2438/3560 Training loss: 1.2548 13.9480 sec/batch\n",
      "Epoch 14/20  Iteration 2439/3560 Training loss: 1.2547 13.9601 sec/batch\n",
      "Epoch 14/20  Iteration 2440/3560 Training loss: 1.2543 13.9383 sec/batch\n",
      "Epoch 14/20  Iteration 2441/3560 Training loss: 1.2543 13.9437 sec/batch\n",
      "Epoch 14/20  Iteration 2442/3560 Training loss: 1.2543 14.0360 sec/batch\n",
      "Epoch 14/20  Iteration 2443/3560 Training loss: 1.2540 13.9848 sec/batch\n",
      "Epoch 14/20  Iteration 2444/3560 Training loss: 1.2537 13.9196 sec/batch\n",
      "Epoch 14/20  Iteration 2445/3560 Training loss: 1.2533 13.9494 sec/batch\n",
      "Epoch 14/20  Iteration 2446/3560 Training loss: 1.2531 14.0321 sec/batch\n",
      "Epoch 14/20  Iteration 2447/3560 Training loss: 1.2531 13.9938 sec/batch\n",
      "Epoch 14/20  Iteration 2448/3560 Training loss: 1.2531 13.8994 sec/batch\n",
      "Epoch 14/20  Iteration 2449/3560 Training loss: 1.2530 13.9151 sec/batch\n",
      "Epoch 14/20  Iteration 2450/3560 Training loss: 1.2530 14.0530 sec/batch\n",
      "Epoch 14/20  Iteration 2451/3560 Training loss: 1.2532 14.2169 sec/batch\n",
      "Epoch 14/20  Iteration 2452/3560 Training loss: 1.2533 14.0409 sec/batch\n",
      "Epoch 14/20  Iteration 2453/3560 Training loss: 1.2533 13.8760 sec/batch\n",
      "Epoch 14/20  Iteration 2454/3560 Training loss: 1.2533 14.1550 sec/batch\n",
      "Epoch 14/20  Iteration 2455/3560 Training loss: 1.2536 13.9418 sec/batch\n",
      "Epoch 14/20  Iteration 2456/3560 Training loss: 1.2537 13.9806 sec/batch\n",
      "Epoch 14/20  Iteration 2457/3560 Training loss: 1.2536 13.9552 sec/batch\n",
      "Epoch 14/20  Iteration 2458/3560 Training loss: 1.2538 13.9683 sec/batch\n",
      "Epoch 14/20  Iteration 2459/3560 Training loss: 1.2536 13.9600 sec/batch\n",
      "Epoch 14/20  Iteration 2460/3560 Training loss: 1.2538 13.8876 sec/batch\n",
      "Epoch 14/20  Iteration 2461/3560 Training loss: 1.2537 13.9642 sec/batch\n",
      "Epoch 14/20  Iteration 2462/3560 Training loss: 1.2539 13.9472 sec/batch\n",
      "Epoch 14/20  Iteration 2463/3560 Training loss: 1.2540 14.0421 sec/batch\n",
      "Epoch 14/20  Iteration 2464/3560 Training loss: 1.2539 13.9118 sec/batch\n",
      "Epoch 14/20  Iteration 2465/3560 Training loss: 1.2536 14.0200 sec/batch\n",
      "Epoch 14/20  Iteration 2466/3560 Training loss: 1.2535 14.0313 sec/batch\n",
      "Epoch 14/20  Iteration 2467/3560 Training loss: 1.2535 14.2812 sec/batch\n",
      "Epoch 14/20  Iteration 2468/3560 Training loss: 1.2534 13.8849 sec/batch\n",
      "Epoch 14/20  Iteration 2469/3560 Training loss: 1.2534 13.9750 sec/batch\n",
      "Epoch 14/20  Iteration 2470/3560 Training loss: 1.2533 14.0370 sec/batch\n",
      "Epoch 14/20  Iteration 2471/3560 Training loss: 1.2534 14.0012 sec/batch\n",
      "Epoch 14/20  Iteration 2472/3560 Training loss: 1.2533 13.9129 sec/batch\n",
      "Epoch 14/20  Iteration 2473/3560 Training loss: 1.2530 15.1637 sec/batch\n",
      "Epoch 14/20  Iteration 2474/3560 Training loss: 1.2531 14.0703 sec/batch\n",
      "Epoch 14/20  Iteration 2475/3560 Training loss: 1.2533 13.9614 sec/batch\n",
      "Epoch 14/20  Iteration 2476/3560 Training loss: 1.2533 13.9314 sec/batch\n",
      "Epoch 14/20  Iteration 2477/3560 Training loss: 1.2532 14.0005 sec/batch\n",
      "Epoch 14/20  Iteration 2478/3560 Training loss: 1.2532 14.0697 sec/batch\n",
      "Epoch 14/20  Iteration 2479/3560 Training loss: 1.2531 13.9140 sec/batch\n",
      "Epoch 14/20  Iteration 2480/3560 Training loss: 1.2530 13.9871 sec/batch\n",
      "Epoch 14/20  Iteration 2481/3560 Training loss: 1.2532 13.9928 sec/batch\n",
      "Epoch 14/20  Iteration 2482/3560 Training loss: 1.2535 13.9714 sec/batch\n",
      "Epoch 14/20  Iteration 2483/3560 Training loss: 1.2535 13.9307 sec/batch\n",
      "Epoch 14/20  Iteration 2484/3560 Training loss: 1.2536 13.9280 sec/batch\n",
      "Epoch 14/20  Iteration 2485/3560 Training loss: 1.2535 13.9740 sec/batch\n",
      "Epoch 14/20  Iteration 2486/3560 Training loss: 1.2534 13.9963 sec/batch\n",
      "Epoch 14/20  Iteration 2487/3560 Training loss: 1.2536 14.0222 sec/batch\n",
      "Epoch 14/20  Iteration 2488/3560 Training loss: 1.2536 14.0738 sec/batch\n",
      "Epoch 14/20  Iteration 2489/3560 Training loss: 1.2536 13.9914 sec/batch\n",
      "Epoch 14/20  Iteration 2490/3560 Training loss: 1.2535 13.9703 sec/batch\n",
      "Epoch 14/20  Iteration 2491/3560 Training loss: 1.2534 14.0648 sec/batch\n",
      "Epoch 14/20  Iteration 2492/3560 Training loss: 1.2535 13.9560 sec/batch\n",
      "Epoch 15/20  Iteration 2493/3560 Training loss: 1.3788 13.9564 sec/batch\n",
      "Epoch 15/20  Iteration 2494/3560 Training loss: 1.3241 14.3904 sec/batch\n",
      "Epoch 15/20  Iteration 2495/3560 Training loss: 1.3027 13.9908 sec/batch\n",
      "Epoch 15/20  Iteration 2496/3560 Training loss: 1.2951 14.1497 sec/batch\n",
      "Epoch 15/20  Iteration 2497/3560 Training loss: 1.2831 14.0963 sec/batch\n",
      "Epoch 15/20  Iteration 2498/3560 Training loss: 1.2701 13.9941 sec/batch\n",
      "Epoch 15/20  Iteration 2499/3560 Training loss: 1.2685 14.0224 sec/batch\n",
      "Epoch 15/20  Iteration 2500/3560 Training loss: 1.2648 14.0567 sec/batch\n",
      "Epoch 15/20  Iteration 2501/3560 Training loss: 1.2630 13.9967 sec/batch\n",
      "Epoch 15/20  Iteration 2502/3560 Training loss: 1.2615 13.9931 sec/batch\n",
      "Epoch 15/20  Iteration 2503/3560 Training loss: 1.2576 14.0193 sec/batch\n",
      "Epoch 15/20  Iteration 2504/3560 Training loss: 1.2573 14.0248 sec/batch\n",
      "Epoch 15/20  Iteration 2505/3560 Training loss: 1.2571 14.0088 sec/batch\n",
      "Epoch 15/20  Iteration 2506/3560 Training loss: 1.2575 14.0234 sec/batch\n",
      "Epoch 15/20  Iteration 2507/3560 Training loss: 1.2560 13.9545 sec/batch\n",
      "Epoch 15/20  Iteration 2508/3560 Training loss: 1.2533 14.0660 sec/batch\n",
      "Epoch 15/20  Iteration 2509/3560 Training loss: 1.2532 14.0970 sec/batch\n",
      "Epoch 15/20  Iteration 2510/3560 Training loss: 1.2543 13.9188 sec/batch\n",
      "Epoch 15/20  Iteration 2511/3560 Training loss: 1.2545 13.9472 sec/batch\n",
      "Epoch 15/20  Iteration 2512/3560 Training loss: 1.2553 14.1083 sec/batch\n",
      "Epoch 15/20  Iteration 2513/3560 Training loss: 1.2546 13.9670 sec/batch\n",
      "Epoch 15/20  Iteration 2514/3560 Training loss: 1.2547 14.0632 sec/batch\n",
      "Epoch 15/20  Iteration 2515/3560 Training loss: 1.2540 14.0729 sec/batch\n",
      "Epoch 15/20  Iteration 2516/3560 Training loss: 1.2539 14.5842 sec/batch\n",
      "Epoch 15/20  Iteration 2517/3560 Training loss: 1.2534 13.9934 sec/batch\n",
      "Epoch 15/20  Iteration 2518/3560 Training loss: 1.2517 14.0612 sec/batch\n",
      "Epoch 15/20  Iteration 2519/3560 Training loss: 1.2503 13.9604 sec/batch\n",
      "Epoch 15/20  Iteration 2520/3560 Training loss: 1.2506 13.9864 sec/batch\n",
      "Epoch 15/20  Iteration 2521/3560 Training loss: 1.2507 14.0444 sec/batch\n",
      "Epoch 15/20  Iteration 2522/3560 Training loss: 1.2508 13.9922 sec/batch\n",
      "Epoch 15/20  Iteration 2523/3560 Training loss: 1.2500 13.9564 sec/batch\n",
      "Epoch 15/20  Iteration 2524/3560 Training loss: 1.2491 13.9855 sec/batch\n",
      "Epoch 15/20  Iteration 2525/3560 Training loss: 1.2492 13.9652 sec/batch\n",
      "Epoch 15/20  Iteration 2526/3560 Training loss: 1.2493 13.9845 sec/batch\n",
      "Epoch 15/20  Iteration 2527/3560 Training loss: 1.2491 13.9483 sec/batch\n",
      "Epoch 15/20  Iteration 2528/3560 Training loss: 1.2491 13.9862 sec/batch\n",
      "Epoch 15/20  Iteration 2529/3560 Training loss: 1.2482 13.9991 sec/batch\n",
      "Epoch 15/20  Iteration 2530/3560 Training loss: 1.2468 14.0526 sec/batch\n",
      "Epoch 15/20  Iteration 2531/3560 Training loss: 1.2456 13.9547 sec/batch\n",
      "Epoch 15/20  Iteration 2532/3560 Training loss: 1.2450 13.9830 sec/batch\n",
      "Epoch 15/20  Iteration 2533/3560 Training loss: 1.2444 14.0166 sec/batch\n",
      "Epoch 15/20  Iteration 2534/3560 Training loss: 1.2451 14.0312 sec/batch\n",
      "Epoch 15/20  Iteration 2535/3560 Training loss: 1.2447 13.8272 sec/batch\n",
      "Epoch 15/20  Iteration 2536/3560 Training loss: 1.2441 14.0120 sec/batch\n",
      "Epoch 15/20  Iteration 2537/3560 Training loss: 1.2441 14.4523 sec/batch\n",
      "Epoch 15/20  Iteration 2538/3560 Training loss: 1.2433 13.9588 sec/batch\n",
      "Epoch 15/20  Iteration 2539/3560 Training loss: 1.2427 14.0733 sec/batch\n",
      "Epoch 15/20  Iteration 2540/3560 Training loss: 1.2425 14.0364 sec/batch\n",
      "Epoch 15/20  Iteration 2541/3560 Training loss: 1.2424 14.0647 sec/batch\n",
      "Epoch 15/20  Iteration 2542/3560 Training loss: 1.2426 13.9809 sec/batch\n",
      "Epoch 15/20  Iteration 2543/3560 Training loss: 1.2419 13.9736 sec/batch\n",
      "Epoch 15/20  Iteration 2544/3560 Training loss: 1.2426 14.0111 sec/batch\n",
      "Epoch 15/20  Iteration 2545/3560 Training loss: 1.2424 14.1188 sec/batch\n",
      "Epoch 15/20  Iteration 2546/3560 Training loss: 1.2426 13.9175 sec/batch\n",
      "Epoch 15/20  Iteration 2547/3560 Training loss: 1.2425 13.9101 sec/batch\n",
      "Epoch 15/20  Iteration 2548/3560 Training loss: 1.2424 14.0894 sec/batch\n",
      "Epoch 15/20  Iteration 2549/3560 Training loss: 1.2426 13.9652 sec/batch\n",
      "Epoch 15/20  Iteration 2550/3560 Training loss: 1.2422 13.9526 sec/batch\n",
      "Epoch 15/20  Iteration 2551/3560 Training loss: 1.2417 14.0528 sec/batch\n",
      "Epoch 15/20  Iteration 2552/3560 Training loss: 1.2423 13.9570 sec/batch\n",
      "Epoch 15/20  Iteration 2553/3560 Training loss: 1.2423 14.0019 sec/batch\n",
      "Epoch 15/20  Iteration 2554/3560 Training loss: 1.2431 14.0778 sec/batch\n",
      "Epoch 15/20  Iteration 2555/3560 Training loss: 1.2435 13.9931 sec/batch\n",
      "Epoch 15/20  Iteration 2556/3560 Training loss: 1.2436 13.9923 sec/batch\n",
      "Epoch 15/20  Iteration 2557/3560 Training loss: 1.2435 13.9688 sec/batch\n",
      "Epoch 15/20  Iteration 2558/3560 Training loss: 1.2437 14.9497 sec/batch\n",
      "Epoch 15/20  Iteration 2559/3560 Training loss: 1.2439 14.1937 sec/batch\n",
      "Epoch 15/20  Iteration 2560/3560 Training loss: 1.2437 14.0535 sec/batch\n",
      "Epoch 15/20  Iteration 2561/3560 Training loss: 1.2439 13.9856 sec/batch\n",
      "Epoch 15/20  Iteration 2562/3560 Training loss: 1.2438 13.9360 sec/batch\n",
      "Epoch 15/20  Iteration 2563/3560 Training loss: 1.2444 14.0761 sec/batch\n",
      "Epoch 15/20  Iteration 2564/3560 Training loss: 1.2447 13.9788 sec/batch\n",
      "Epoch 15/20  Iteration 2565/3560 Training loss: 1.2451 14.0748 sec/batch\n",
      "Epoch 15/20  Iteration 2566/3560 Training loss: 1.2447 13.9826 sec/batch\n",
      "Epoch 15/20  Iteration 2567/3560 Training loss: 1.2447 14.0169 sec/batch\n",
      "Epoch 15/20  Iteration 2568/3560 Training loss: 1.2449 13.9200 sec/batch\n",
      "Epoch 15/20  Iteration 2569/3560 Training loss: 1.2448 14.0581 sec/batch\n",
      "Epoch 15/20  Iteration 2570/3560 Training loss: 1.2446 14.0153 sec/batch\n",
      "Epoch 15/20  Iteration 2571/3560 Training loss: 1.2440 13.9240 sec/batch\n",
      "Epoch 15/20  Iteration 2572/3560 Training loss: 1.2440 14.0689 sec/batch\n",
      "Epoch 15/20  Iteration 2573/3560 Training loss: 1.2435 13.9716 sec/batch\n",
      "Epoch 15/20  Iteration 2574/3560 Training loss: 1.2433 14.0299 sec/batch\n",
      "Epoch 15/20  Iteration 2575/3560 Training loss: 1.2428 14.0276 sec/batch\n",
      "Epoch 15/20  Iteration 2576/3560 Training loss: 1.2427 14.0596 sec/batch\n",
      "Epoch 15/20  Iteration 2577/3560 Training loss: 1.2424 13.9373 sec/batch\n",
      "Epoch 15/20  Iteration 2578/3560 Training loss: 1.2422 14.0048 sec/batch\n",
      "Epoch 15/20  Iteration 2579/3560 Training loss: 1.2420 14.0429 sec/batch\n",
      "Epoch 15/20  Iteration 2580/3560 Training loss: 1.2417 14.2336 sec/batch\n",
      "Epoch 15/20  Iteration 2581/3560 Training loss: 1.2414 14.0582 sec/batch\n",
      "Epoch 15/20  Iteration 2582/3560 Training loss: 1.2413 14.0470 sec/batch\n",
      "Epoch 15/20  Iteration 2583/3560 Training loss: 1.2411 13.9706 sec/batch\n",
      "Epoch 15/20  Iteration 2584/3560 Training loss: 1.2410 14.0172 sec/batch\n",
      "Epoch 15/20  Iteration 2585/3560 Training loss: 1.2406 14.0372 sec/batch\n",
      "Epoch 15/20  Iteration 2586/3560 Training loss: 1.2402 14.0018 sec/batch\n",
      "Epoch 15/20  Iteration 2587/3560 Training loss: 1.2401 14.1128 sec/batch\n",
      "Epoch 15/20  Iteration 2588/3560 Training loss: 1.2401 13.9292 sec/batch\n",
      "Epoch 15/20  Iteration 2589/3560 Training loss: 1.2401 14.1178 sec/batch\n",
      "Epoch 15/20  Iteration 2590/3560 Training loss: 1.2398 13.9788 sec/batch\n",
      "Epoch 15/20  Iteration 2591/3560 Training loss: 1.2394 14.1600 sec/batch\n",
      "Epoch 15/20  Iteration 2592/3560 Training loss: 1.2390 14.0104 sec/batch\n",
      "Epoch 15/20  Iteration 2593/3560 Training loss: 1.2390 14.1343 sec/batch\n",
      "Epoch 15/20  Iteration 2594/3560 Training loss: 1.2389 13.9964 sec/batch\n",
      "Epoch 15/20  Iteration 2595/3560 Training loss: 1.2387 14.0235 sec/batch\n",
      "Epoch 15/20  Iteration 2596/3560 Training loss: 1.2386 13.9692 sec/batch\n",
      "Epoch 15/20  Iteration 2597/3560 Training loss: 1.2385 14.3795 sec/batch\n",
      "Epoch 15/20  Iteration 2598/3560 Training loss: 1.2384 13.9400 sec/batch\n",
      "Epoch 15/20  Iteration 2599/3560 Training loss: 1.2383 14.1326 sec/batch\n",
      "Epoch 15/20  Iteration 2600/3560 Training loss: 1.2382 13.9924 sec/batch\n",
      "Validation loss: 1.15845 Saving checkpoint!\n",
      "Epoch 15/20  Iteration 2601/3560 Training loss: 1.2399 13.7710 sec/batch\n",
      "Epoch 15/20  Iteration 2602/3560 Training loss: 1.2401 14.0857 sec/batch\n",
      "Epoch 15/20  Iteration 2603/3560 Training loss: 1.2400 13.9886 sec/batch\n",
      "Epoch 15/20  Iteration 2604/3560 Training loss: 1.2400 14.0764 sec/batch\n",
      "Epoch 15/20  Iteration 2605/3560 Training loss: 1.2399 13.9511 sec/batch\n",
      "Epoch 15/20  Iteration 2606/3560 Training loss: 1.2399 13.9623 sec/batch\n",
      "Epoch 15/20  Iteration 2607/3560 Training loss: 1.2396 14.0925 sec/batch\n",
      "Epoch 15/20  Iteration 2608/3560 Training loss: 1.2393 14.0191 sec/batch\n",
      "Epoch 15/20  Iteration 2609/3560 Training loss: 1.2394 14.1108 sec/batch\n",
      "Epoch 15/20  Iteration 2610/3560 Training loss: 1.2395 14.0333 sec/batch\n",
      "Epoch 15/20  Iteration 2611/3560 Training loss: 1.2395 14.0103 sec/batch\n",
      "Epoch 15/20  Iteration 2612/3560 Training loss: 1.2395 14.0521 sec/batch\n",
      "Epoch 15/20  Iteration 2613/3560 Training loss: 1.2393 14.0217 sec/batch\n",
      "Epoch 15/20  Iteration 2614/3560 Training loss: 1.2390 14.0182 sec/batch\n",
      "Epoch 15/20  Iteration 2615/3560 Training loss: 1.2387 14.3724 sec/batch\n",
      "Epoch 15/20  Iteration 2616/3560 Training loss: 1.2387 14.0472 sec/batch\n",
      "Epoch 15/20  Iteration 2617/3560 Training loss: 1.2385 13.9784 sec/batch\n",
      "Epoch 15/20  Iteration 2618/3560 Training loss: 1.2382 14.2136 sec/batch\n",
      "Epoch 15/20  Iteration 2619/3560 Training loss: 1.2381 14.0331 sec/batch\n",
      "Epoch 15/20  Iteration 2620/3560 Training loss: 1.2380 14.0491 sec/batch\n",
      "Epoch 15/20  Iteration 2621/3560 Training loss: 1.2378 13.9952 sec/batch\n",
      "Epoch 15/20  Iteration 2622/3560 Training loss: 1.2375 14.0972 sec/batch\n",
      "Epoch 15/20  Iteration 2623/3560 Training loss: 1.2372 13.9931 sec/batch\n",
      "Epoch 15/20  Iteration 2624/3560 Training loss: 1.2371 14.1199 sec/batch\n",
      "Epoch 15/20  Iteration 2625/3560 Training loss: 1.2371 14.1814 sec/batch\n",
      "Epoch 15/20  Iteration 2626/3560 Training loss: 1.2371 14.0803 sec/batch\n",
      "Epoch 15/20  Iteration 2627/3560 Training loss: 1.2371 13.9911 sec/batch\n",
      "Epoch 15/20  Iteration 2628/3560 Training loss: 1.2372 14.0230 sec/batch\n",
      "Epoch 15/20  Iteration 2629/3560 Training loss: 1.2374 13.9841 sec/batch\n",
      "Epoch 15/20  Iteration 2630/3560 Training loss: 1.2375 14.0754 sec/batch\n",
      "Epoch 15/20  Iteration 2631/3560 Training loss: 1.2375 14.0071 sec/batch\n",
      "Epoch 15/20  Iteration 2632/3560 Training loss: 1.2375 13.9976 sec/batch\n",
      "Epoch 15/20  Iteration 2633/3560 Training loss: 1.2379 13.9610 sec/batch\n",
      "Epoch 15/20  Iteration 2634/3560 Training loss: 1.2379 14.0221 sec/batch\n",
      "Epoch 15/20  Iteration 2635/3560 Training loss: 1.2377 14.0415 sec/batch\n",
      "Epoch 15/20  Iteration 2636/3560 Training loss: 1.2380 14.4315 sec/batch\n",
      "Epoch 15/20  Iteration 2637/3560 Training loss: 1.2378 14.9832 sec/batch\n",
      "Epoch 15/20  Iteration 2638/3560 Training loss: 1.2379 14.3432 sec/batch\n",
      "Epoch 15/20  Iteration 2639/3560 Training loss: 1.2379 14.3544 sec/batch\n",
      "Epoch 15/20  Iteration 2640/3560 Training loss: 1.2381 14.2687 sec/batch\n",
      "Epoch 15/20  Iteration 2641/3560 Training loss: 1.2382 14.3364 sec/batch\n",
      "Epoch 15/20  Iteration 2642/3560 Training loss: 1.2381 14.3951 sec/batch\n",
      "Epoch 15/20  Iteration 2643/3560 Training loss: 1.2378 14.4030 sec/batch\n",
      "Epoch 15/20  Iteration 2644/3560 Training loss: 1.2376 14.4118 sec/batch\n",
      "Epoch 15/20  Iteration 2645/3560 Training loss: 1.2376 14.2725 sec/batch\n",
      "Epoch 15/20  Iteration 2646/3560 Training loss: 1.2376 14.4267 sec/batch\n",
      "Epoch 15/20  Iteration 2647/3560 Training loss: 1.2375 14.3265 sec/batch\n",
      "Epoch 15/20  Iteration 2648/3560 Training loss: 1.2375 14.3735 sec/batch\n",
      "Epoch 15/20  Iteration 2649/3560 Training loss: 1.2375 14.2926 sec/batch\n",
      "Epoch 15/20  Iteration 2650/3560 Training loss: 1.2374 14.3601 sec/batch\n",
      "Epoch 15/20  Iteration 2651/3560 Training loss: 1.2372 14.2689 sec/batch\n",
      "Epoch 15/20  Iteration 2652/3560 Training loss: 1.2373 14.4607 sec/batch\n",
      "Epoch 15/20  Iteration 2653/3560 Training loss: 1.2374 14.3592 sec/batch\n",
      "Epoch 15/20  Iteration 2654/3560 Training loss: 1.2374 14.3892 sec/batch\n",
      "Epoch 15/20  Iteration 2655/3560 Training loss: 1.2374 14.2835 sec/batch\n",
      "Epoch 15/20  Iteration 2656/3560 Training loss: 1.2373 14.3502 sec/batch\n",
      "Epoch 15/20  Iteration 2657/3560 Training loss: 1.2373 14.6465 sec/batch\n",
      "Epoch 15/20  Iteration 2658/3560 Training loss: 1.2372 14.3942 sec/batch\n",
      "Epoch 15/20  Iteration 2659/3560 Training loss: 1.2374 14.3037 sec/batch\n",
      "Epoch 15/20  Iteration 2660/3560 Training loss: 1.2377 14.4508 sec/batch\n",
      "Epoch 15/20  Iteration 2661/3560 Training loss: 1.2377 14.3902 sec/batch\n",
      "Epoch 15/20  Iteration 2662/3560 Training loss: 1.2377 14.3409 sec/batch\n",
      "Epoch 15/20  Iteration 2663/3560 Training loss: 1.2377 14.2407 sec/batch\n",
      "Epoch 15/20  Iteration 2664/3560 Training loss: 1.2376 14.3882 sec/batch\n",
      "Epoch 15/20  Iteration 2665/3560 Training loss: 1.2377 14.2668 sec/batch\n",
      "Epoch 15/20  Iteration 2666/3560 Training loss: 1.2377 14.4283 sec/batch\n",
      "Epoch 15/20  Iteration 2667/3560 Training loss: 1.2377 14.3927 sec/batch\n",
      "Epoch 15/20  Iteration 2668/3560 Training loss: 1.2375 14.4035 sec/batch\n",
      "Epoch 15/20  Iteration 2669/3560 Training loss: 1.2374 14.3838 sec/batch\n",
      "Epoch 15/20  Iteration 2670/3560 Training loss: 1.2376 14.3645 sec/batch\n",
      "Epoch 16/20  Iteration 2671/3560 Training loss: 1.3543 14.3687 sec/batch\n",
      "Epoch 16/20  Iteration 2672/3560 Training loss: 1.3026 14.3579 sec/batch\n",
      "Epoch 16/20  Iteration 2673/3560 Training loss: 1.2835 14.3202 sec/batch\n",
      "Epoch 16/20  Iteration 2674/3560 Training loss: 1.2767 14.4477 sec/batch\n",
      "Epoch 16/20  Iteration 2675/3560 Training loss: 1.2620 14.3358 sec/batch\n",
      "Epoch 16/20  Iteration 2676/3560 Training loss: 1.2494 14.3130 sec/batch\n",
      "Epoch 16/20  Iteration 2677/3560 Training loss: 1.2491 14.4030 sec/batch\n",
      "Epoch 16/20  Iteration 2678/3560 Training loss: 1.2465 15.0404 sec/batch\n",
      "Epoch 16/20  Iteration 2679/3560 Training loss: 1.2451 14.3161 sec/batch\n",
      "Epoch 16/20  Iteration 2680/3560 Training loss: 1.2434 14.2971 sec/batch\n",
      "Epoch 16/20  Iteration 2681/3560 Training loss: 1.2403 14.2902 sec/batch\n",
      "Epoch 16/20  Iteration 2682/3560 Training loss: 1.2397 14.3302 sec/batch\n",
      "Epoch 16/20  Iteration 2683/3560 Training loss: 1.2400 14.2924 sec/batch\n",
      "Epoch 16/20  Iteration 2684/3560 Training loss: 1.2399 14.3596 sec/batch\n",
      "Epoch 16/20  Iteration 2685/3560 Training loss: 1.2381 14.3341 sec/batch\n",
      "Epoch 16/20  Iteration 2686/3560 Training loss: 1.2358 14.3871 sec/batch\n",
      "Epoch 16/20  Iteration 2687/3560 Training loss: 1.2357 14.3254 sec/batch\n",
      "Epoch 16/20  Iteration 2688/3560 Training loss: 1.2367 14.4150 sec/batch\n",
      "Epoch 16/20  Iteration 2689/3560 Training loss: 1.2362 14.2252 sec/batch\n",
      "Epoch 16/20  Iteration 2690/3560 Training loss: 1.2374 14.2991 sec/batch\n",
      "Epoch 16/20  Iteration 2691/3560 Training loss: 1.2370 14.3311 sec/batch\n",
      "Epoch 16/20  Iteration 2692/3560 Training loss: 1.2371 14.4139 sec/batch\n",
      "Epoch 16/20  Iteration 2693/3560 Training loss: 1.2361 14.3616 sec/batch\n",
      "Epoch 16/20  Iteration 2694/3560 Training loss: 1.2362 14.3230 sec/batch\n",
      "Epoch 16/20  Iteration 2695/3560 Training loss: 1.2355 14.2925 sec/batch\n",
      "Epoch 16/20  Iteration 2696/3560 Training loss: 1.2336 14.3647 sec/batch\n",
      "Epoch 16/20  Iteration 2697/3560 Training loss: 1.2327 14.2669 sec/batch\n",
      "Epoch 16/20  Iteration 2698/3560 Training loss: 1.2329 14.3261 sec/batch\n",
      "Epoch 16/20  Iteration 2699/3560 Training loss: 1.2329 14.7348 sec/batch\n",
      "Epoch 16/20  Iteration 2700/3560 Training loss: 1.2332 14.3903 sec/batch\n",
      "Epoch 16/20  Iteration 2701/3560 Training loss: 1.2324 14.3066 sec/batch\n",
      "Epoch 16/20  Iteration 2702/3560 Training loss: 1.2314 14.4638 sec/batch\n",
      "Epoch 16/20  Iteration 2703/3560 Training loss: 1.2314 14.3315 sec/batch\n",
      "Epoch 16/20  Iteration 2704/3560 Training loss: 1.2314 14.3795 sec/batch\n",
      "Epoch 16/20  Iteration 2705/3560 Training loss: 1.2310 14.3338 sec/batch\n",
      "Epoch 16/20  Iteration 2706/3560 Training loss: 1.2309 14.3146 sec/batch\n",
      "Epoch 16/20  Iteration 2707/3560 Training loss: 1.2301 14.2887 sec/batch\n",
      "Epoch 16/20  Iteration 2708/3560 Training loss: 1.2290 14.4320 sec/batch\n",
      "Epoch 16/20  Iteration 2709/3560 Training loss: 1.2278 14.2418 sec/batch\n",
      "Epoch 16/20  Iteration 2710/3560 Training loss: 1.2277 14.3956 sec/batch\n",
      "Epoch 16/20  Iteration 2711/3560 Training loss: 1.2269 14.3577 sec/batch\n",
      "Epoch 16/20  Iteration 2712/3560 Training loss: 1.2277 14.4945 sec/batch\n",
      "Epoch 16/20  Iteration 2713/3560 Training loss: 1.2272 14.3308 sec/batch\n",
      "Epoch 16/20  Iteration 2714/3560 Training loss: 1.2267 14.4028 sec/batch\n",
      "Epoch 16/20  Iteration 2715/3560 Training loss: 1.2269 14.2495 sec/batch\n",
      "Epoch 16/20  Iteration 2716/3560 Training loss: 1.2264 14.4273 sec/batch\n",
      "Epoch 16/20  Iteration 2717/3560 Training loss: 1.2259 14.3052 sec/batch\n",
      "Epoch 16/20  Iteration 2718/3560 Training loss: 1.2253 14.7540 sec/batch\n",
      "Epoch 16/20  Iteration 2719/3560 Training loss: 1.2254 14.3456 sec/batch\n",
      "Epoch 16/20  Iteration 2720/3560 Training loss: 1.2257 15.4831 sec/batch\n",
      "Epoch 16/20  Iteration 2721/3560 Training loss: 1.2253 14.3146 sec/batch\n",
      "Epoch 16/20  Iteration 2722/3560 Training loss: 1.2260 14.4451 sec/batch\n",
      "Epoch 16/20  Iteration 2723/3560 Training loss: 1.2259 14.3652 sec/batch\n",
      "Epoch 16/20  Iteration 2724/3560 Training loss: 1.2262 14.3065 sec/batch\n",
      "Epoch 16/20  Iteration 2725/3560 Training loss: 1.2260 14.3807 sec/batch\n",
      "Epoch 16/20  Iteration 2726/3560 Training loss: 1.2260 14.3859 sec/batch\n",
      "Epoch 16/20  Iteration 2727/3560 Training loss: 1.2262 14.3289 sec/batch\n",
      "Epoch 16/20  Iteration 2728/3560 Training loss: 1.2258 14.4167 sec/batch\n",
      "Epoch 16/20  Iteration 2729/3560 Training loss: 1.2252 14.3389 sec/batch\n",
      "Epoch 16/20  Iteration 2730/3560 Training loss: 1.2258 14.3921 sec/batch\n",
      "Epoch 16/20  Iteration 2731/3560 Training loss: 1.2258 14.3489 sec/batch\n",
      "Epoch 16/20  Iteration 2732/3560 Training loss: 1.2265 14.4452 sec/batch\n",
      "Epoch 16/20  Iteration 2733/3560 Training loss: 1.2270 14.3812 sec/batch\n",
      "Epoch 16/20  Iteration 2734/3560 Training loss: 1.2274 14.3955 sec/batch\n",
      "Epoch 16/20  Iteration 2735/3560 Training loss: 1.2273 14.3438 sec/batch\n",
      "Epoch 16/20  Iteration 2736/3560 Training loss: 1.2273 14.4160 sec/batch\n",
      "Epoch 16/20  Iteration 2737/3560 Training loss: 1.2274 14.2909 sec/batch\n",
      "Epoch 16/20  Iteration 2738/3560 Training loss: 1.2272 14.3733 sec/batch\n",
      "Epoch 16/20  Iteration 2739/3560 Training loss: 1.2273 14.3154 sec/batch\n",
      "Epoch 16/20  Iteration 2740/3560 Training loss: 1.2272 14.3369 sec/batch\n",
      "Epoch 16/20  Iteration 2741/3560 Training loss: 1.2278 14.7091 sec/batch\n",
      "Epoch 16/20  Iteration 2742/3560 Training loss: 1.2281 14.3629 sec/batch\n",
      "Epoch 16/20  Iteration 2743/3560 Training loss: 1.2284 14.3245 sec/batch\n",
      "Epoch 16/20  Iteration 2744/3560 Training loss: 1.2280 14.3845 sec/batch\n",
      "Epoch 16/20  Iteration 2745/3560 Training loss: 1.2280 14.8662 sec/batch\n",
      "Epoch 16/20  Iteration 2746/3560 Training loss: 1.2281 14.3628 sec/batch\n",
      "Epoch 16/20  Iteration 2747/3560 Training loss: 1.2280 14.3450 sec/batch\n",
      "Epoch 16/20  Iteration 2748/3560 Training loss: 1.2279 14.3911 sec/batch\n",
      "Epoch 16/20  Iteration 2749/3560 Training loss: 1.2271 14.2306 sec/batch\n",
      "Epoch 16/20  Iteration 2750/3560 Training loss: 1.2271 14.3416 sec/batch\n",
      "Epoch 16/20  Iteration 2751/3560 Training loss: 1.2267 14.3550 sec/batch\n",
      "Epoch 16/20  Iteration 2752/3560 Training loss: 1.2265 14.4477 sec/batch\n",
      "Epoch 16/20  Iteration 2753/3560 Training loss: 1.2261 14.3220 sec/batch\n",
      "Epoch 16/20  Iteration 2754/3560 Training loss: 1.2260 14.4206 sec/batch\n",
      "Epoch 16/20  Iteration 2755/3560 Training loss: 1.2258 14.3200 sec/batch\n",
      "Epoch 16/20  Iteration 2756/3560 Training loss: 1.2258 14.3901 sec/batch\n",
      "Epoch 16/20  Iteration 2757/3560 Training loss: 1.2255 14.3043 sec/batch\n",
      "Epoch 16/20  Iteration 2758/3560 Training loss: 1.2253 14.5992 sec/batch\n",
      "Epoch 16/20  Iteration 2759/3560 Training loss: 1.2249 14.2660 sec/batch\n",
      "Epoch 16/20  Iteration 2760/3560 Training loss: 1.2249 14.4448 sec/batch\n",
      "Epoch 16/20  Iteration 2761/3560 Training loss: 1.2247 14.2748 sec/batch\n",
      "Epoch 16/20  Iteration 2762/3560 Training loss: 1.2246 15.2188 sec/batch\n",
      "Epoch 16/20  Iteration 2763/3560 Training loss: 1.2242 14.2809 sec/batch\n",
      "Epoch 16/20  Iteration 2764/3560 Training loss: 1.2239 14.3347 sec/batch\n",
      "Epoch 16/20  Iteration 2765/3560 Training loss: 1.2237 14.2176 sec/batch\n",
      "Epoch 16/20  Iteration 2766/3560 Training loss: 1.2237 14.4365 sec/batch\n",
      "Epoch 16/20  Iteration 2767/3560 Training loss: 1.2237 14.3499 sec/batch\n",
      "Epoch 16/20  Iteration 2768/3560 Training loss: 1.2233 14.4426 sec/batch\n",
      "Epoch 16/20  Iteration 2769/3560 Training loss: 1.2230 14.3084 sec/batch\n",
      "Epoch 16/20  Iteration 2770/3560 Training loss: 1.2227 14.3388 sec/batch\n",
      "Epoch 16/20  Iteration 2771/3560 Training loss: 1.2228 14.3766 sec/batch\n",
      "Epoch 16/20  Iteration 2772/3560 Training loss: 1.2227 14.3907 sec/batch\n",
      "Epoch 16/20  Iteration 2773/3560 Training loss: 1.2227 14.2807 sec/batch\n",
      "Epoch 16/20  Iteration 2774/3560 Training loss: 1.2226 14.3779 sec/batch\n",
      "Epoch 16/20  Iteration 2775/3560 Training loss: 1.2225 14.3112 sec/batch\n",
      "Epoch 16/20  Iteration 2776/3560 Training loss: 1.2223 14.3730 sec/batch\n",
      "Epoch 16/20  Iteration 2777/3560 Training loss: 1.2223 14.4360 sec/batch\n",
      "Epoch 16/20  Iteration 2778/3560 Training loss: 1.2224 14.4136 sec/batch\n",
      "Epoch 16/20  Iteration 2779/3560 Training loss: 1.2222 14.3089 sec/batch\n",
      "Epoch 16/20  Iteration 2780/3560 Training loss: 1.2223 14.3764 sec/batch\n",
      "Epoch 16/20  Iteration 2781/3560 Training loss: 1.2221 14.2651 sec/batch\n",
      "Epoch 16/20  Iteration 2782/3560 Training loss: 1.2221 14.7349 sec/batch\n",
      "Epoch 16/20  Iteration 2783/3560 Training loss: 1.2220 14.3473 sec/batch\n",
      "Epoch 16/20  Iteration 2784/3560 Training loss: 1.2219 14.3979 sec/batch\n",
      "Epoch 16/20  Iteration 2785/3560 Training loss: 1.2216 14.5090 sec/batch\n",
      "Epoch 16/20  Iteration 2786/3560 Training loss: 1.2213 14.4212 sec/batch\n",
      "Epoch 16/20  Iteration 2787/3560 Training loss: 1.2213 14.3664 sec/batch\n",
      "Epoch 16/20  Iteration 2788/3560 Training loss: 1.2213 14.3469 sec/batch\n",
      "Epoch 16/20  Iteration 2789/3560 Training loss: 1.2213 14.3392 sec/batch\n",
      "Epoch 16/20  Iteration 2790/3560 Training loss: 1.2212 14.3814 sec/batch\n",
      "Epoch 16/20  Iteration 2791/3560 Training loss: 1.2212 14.2798 sec/batch\n",
      "Epoch 16/20  Iteration 2792/3560 Training loss: 1.2209 14.3524 sec/batch\n",
      "Epoch 16/20  Iteration 2793/3560 Training loss: 1.2206 14.4042 sec/batch\n",
      "Epoch 16/20  Iteration 2794/3560 Training loss: 1.2206 14.4367 sec/batch\n",
      "Epoch 16/20  Iteration 2795/3560 Training loss: 1.2205 14.3808 sec/batch\n",
      "Epoch 16/20  Iteration 2796/3560 Training loss: 1.2201 14.4349 sec/batch\n",
      "Epoch 16/20  Iteration 2797/3560 Training loss: 1.2201 14.4002 sec/batch\n",
      "Epoch 16/20  Iteration 2798/3560 Training loss: 1.2201 14.3893 sec/batch\n",
      "Epoch 16/20  Iteration 2799/3560 Training loss: 1.2200 14.3781 sec/batch\n",
      "Epoch 16/20  Iteration 2800/3560 Training loss: 1.2196 14.3771 sec/batch\n",
      "Validation loss: 1.1495 Saving checkpoint!\n",
      "Epoch 16/20  Iteration 2801/3560 Training loss: 1.2208 14.1236 sec/batch\n",
      "Epoch 16/20  Iteration 2802/3560 Training loss: 1.2207 14.4360 sec/batch\n",
      "Epoch 16/20  Iteration 2803/3560 Training loss: 1.2208 14.4563 sec/batch\n",
      "Epoch 16/20  Iteration 2804/3560 Training loss: 1.2208 14.3410 sec/batch\n",
      "Epoch 16/20  Iteration 2805/3560 Training loss: 1.2208 14.4281 sec/batch\n",
      "Epoch 16/20  Iteration 2806/3560 Training loss: 1.2208 14.2931 sec/batch\n",
      "Epoch 16/20  Iteration 2807/3560 Training loss: 1.2211 14.4079 sec/batch\n",
      "Epoch 16/20  Iteration 2808/3560 Training loss: 1.2212 14.4193 sec/batch\n",
      "Epoch 16/20  Iteration 2809/3560 Training loss: 1.2212 14.3736 sec/batch\n",
      "Epoch 16/20  Iteration 2810/3560 Training loss: 1.2212 14.5158 sec/batch\n",
      "Epoch 16/20  Iteration 2811/3560 Training loss: 1.2216 14.4554 sec/batch\n",
      "Epoch 16/20  Iteration 2812/3560 Training loss: 1.2217 14.3402 sec/batch\n",
      "Epoch 16/20  Iteration 2813/3560 Training loss: 1.2216 14.4403 sec/batch\n",
      "Epoch 16/20  Iteration 2814/3560 Training loss: 1.2218 14.4728 sec/batch\n",
      "Epoch 16/20  Iteration 2815/3560 Training loss: 1.2216 14.4285 sec/batch\n",
      "Epoch 16/20  Iteration 2816/3560 Training loss: 1.2218 14.4052 sec/batch\n",
      "Epoch 16/20  Iteration 2817/3560 Training loss: 1.2218 14.8432 sec/batch\n",
      "Epoch 16/20  Iteration 2818/3560 Training loss: 1.2221 14.5150 sec/batch\n",
      "Epoch 16/20  Iteration 2819/3560 Training loss: 1.2223 14.4415 sec/batch\n",
      "Epoch 16/20  Iteration 2820/3560 Training loss: 1.2222 14.3327 sec/batch\n",
      "Epoch 16/20  Iteration 2821/3560 Training loss: 1.2219 14.4758 sec/batch\n",
      "Epoch 16/20  Iteration 2822/3560 Training loss: 1.2217 14.4849 sec/batch\n",
      "Epoch 16/20  Iteration 2823/3560 Training loss: 1.2217 14.4546 sec/batch\n",
      "Epoch 16/20  Iteration 2824/3560 Training loss: 1.2217 14.4195 sec/batch\n",
      "Epoch 16/20  Iteration 2825/3560 Training loss: 1.2216 14.9253 sec/batch\n",
      "Epoch 16/20  Iteration 2826/3560 Training loss: 1.2216 14.5972 sec/batch\n",
      "Epoch 16/20  Iteration 2827/3560 Training loss: 1.2216 13.9795 sec/batch\n",
      "Epoch 16/20  Iteration 2828/3560 Training loss: 1.2216 14.0455 sec/batch\n",
      "Epoch 16/20  Iteration 2829/3560 Training loss: 1.2214 14.0128 sec/batch\n",
      "Epoch 16/20  Iteration 2830/3560 Training loss: 1.2214 14.0540 sec/batch\n",
      "Epoch 16/20  Iteration 2831/3560 Training loss: 1.2216 13.9585 sec/batch\n",
      "Epoch 16/20  Iteration 2832/3560 Training loss: 1.2216 14.0052 sec/batch\n",
      "Epoch 16/20  Iteration 2833/3560 Training loss: 1.2216 13.8999 sec/batch\n",
      "Epoch 16/20  Iteration 2834/3560 Training loss: 1.2216 14.3526 sec/batch\n",
      "Epoch 16/20  Iteration 2835/3560 Training loss: 1.2216 13.9538 sec/batch\n",
      "Epoch 16/20  Iteration 2836/3560 Training loss: 1.2215 14.1647 sec/batch\n",
      "Epoch 16/20  Iteration 2837/3560 Training loss: 1.2217 13.9926 sec/batch\n",
      "Epoch 16/20  Iteration 2838/3560 Training loss: 1.2220 14.8936 sec/batch\n",
      "Epoch 16/20  Iteration 2839/3560 Training loss: 1.2221 13.9878 sec/batch\n",
      "Epoch 16/20  Iteration 2840/3560 Training loss: 1.2221 14.0071 sec/batch\n",
      "Epoch 16/20  Iteration 2841/3560 Training loss: 1.2221 14.0070 sec/batch\n",
      "Epoch 16/20  Iteration 2842/3560 Training loss: 1.2220 13.9650 sec/batch\n",
      "Epoch 16/20  Iteration 2843/3560 Training loss: 1.2221 13.9615 sec/batch\n",
      "Epoch 16/20  Iteration 2844/3560 Training loss: 1.2221 13.9456 sec/batch\n",
      "Epoch 16/20  Iteration 2845/3560 Training loss: 1.2221 14.1259 sec/batch\n",
      "Epoch 16/20  Iteration 2846/3560 Training loss: 1.2219 13.9660 sec/batch\n",
      "Epoch 16/20  Iteration 2847/3560 Training loss: 1.2218 14.0131 sec/batch\n",
      "Epoch 16/20  Iteration 2848/3560 Training loss: 1.2219 14.0290 sec/batch\n",
      "Epoch 17/20  Iteration 2849/3560 Training loss: 1.3396 13.9622 sec/batch\n",
      "Epoch 17/20  Iteration 2850/3560 Training loss: 1.2902 13.9342 sec/batch\n",
      "Epoch 17/20  Iteration 2851/3560 Training loss: 1.2681 14.2224 sec/batch\n",
      "Epoch 17/20  Iteration 2852/3560 Training loss: 1.2595 14.3285 sec/batch\n",
      "Epoch 17/20  Iteration 2853/3560 Training loss: 1.2478 14.3713 sec/batch\n",
      "Epoch 17/20  Iteration 2854/3560 Training loss: 1.2351 14.3514 sec/batch\n",
      "Epoch 17/20  Iteration 2855/3560 Training loss: 1.2342 14.3770 sec/batch\n",
      "Epoch 17/20  Iteration 2856/3560 Training loss: 1.2309 14.2771 sec/batch\n",
      "Epoch 17/20  Iteration 2857/3560 Training loss: 1.2296 14.2704 sec/batch\n",
      "Epoch 17/20  Iteration 2858/3560 Training loss: 1.2290 14.3129 sec/batch\n",
      "Epoch 17/20  Iteration 2859/3560 Training loss: 1.2252 14.8264 sec/batch\n",
      "Epoch 17/20  Iteration 2860/3560 Training loss: 1.2239 14.3473 sec/batch\n",
      "Epoch 17/20  Iteration 2861/3560 Training loss: 1.2236 14.3002 sec/batch\n",
      "Epoch 17/20  Iteration 2862/3560 Training loss: 1.2240 14.2830 sec/batch\n",
      "Epoch 17/20  Iteration 2863/3560 Training loss: 1.2228 14.3054 sec/batch\n",
      "Epoch 17/20  Iteration 2864/3560 Training loss: 1.2212 14.3252 sec/batch\n",
      "Epoch 17/20  Iteration 2865/3560 Training loss: 1.2210 14.2484 sec/batch\n",
      "Epoch 17/20  Iteration 2866/3560 Training loss: 1.2219 14.2224 sec/batch\n",
      "Epoch 17/20  Iteration 2867/3560 Training loss: 1.2211 14.3159 sec/batch\n",
      "Epoch 17/20  Iteration 2868/3560 Training loss: 1.2223 14.2786 sec/batch\n",
      "Epoch 17/20  Iteration 2869/3560 Training loss: 1.2218 14.2469 sec/batch\n",
      "Epoch 17/20  Iteration 2870/3560 Training loss: 1.2218 14.4011 sec/batch\n",
      "Epoch 17/20  Iteration 2871/3560 Training loss: 1.2209 14.2308 sec/batch\n",
      "Epoch 17/20  Iteration 2872/3560 Training loss: 1.2212 14.3778 sec/batch\n",
      "Epoch 17/20  Iteration 2873/3560 Training loss: 1.2209 14.2381 sec/batch\n",
      "Epoch 17/20  Iteration 2874/3560 Training loss: 1.2190 14.1670 sec/batch\n",
      "Epoch 17/20  Iteration 2875/3560 Training loss: 1.2178 14.2620 sec/batch\n",
      "Epoch 17/20  Iteration 2876/3560 Training loss: 1.2182 14.2975 sec/batch\n",
      "Epoch 17/20  Iteration 2877/3560 Training loss: 1.2182 14.3176 sec/batch\n",
      "Epoch 17/20  Iteration 2878/3560 Training loss: 1.2185 14.3809 sec/batch\n",
      "Epoch 17/20  Iteration 2879/3560 Training loss: 1.2179 14.3225 sec/batch\n",
      "Epoch 17/20  Iteration 2880/3560 Training loss: 1.2169 15.3170 sec/batch\n",
      "Epoch 17/20  Iteration 2881/3560 Training loss: 1.2172 14.2927 sec/batch\n",
      "Epoch 17/20  Iteration 2882/3560 Training loss: 1.2172 14.3640 sec/batch\n",
      "Epoch 17/20  Iteration 2883/3560 Training loss: 1.2171 14.2130 sec/batch\n",
      "Epoch 17/20  Iteration 2884/3560 Training loss: 1.2168 14.5574 sec/batch\n",
      "Epoch 17/20  Iteration 2885/3560 Training loss: 1.2159 14.3437 sec/batch\n",
      "Epoch 17/20  Iteration 2886/3560 Training loss: 1.2147 14.3478 sec/batch\n",
      "Epoch 17/20  Iteration 2887/3560 Training loss: 1.2136 14.3682 sec/batch\n",
      "Epoch 17/20  Iteration 2888/3560 Training loss: 1.2134 14.2809 sec/batch\n",
      "Epoch 17/20  Iteration 2889/3560 Training loss: 1.2127 14.3382 sec/batch\n",
      "Epoch 17/20  Iteration 2890/3560 Training loss: 1.2136 14.2391 sec/batch\n",
      "Epoch 17/20  Iteration 2891/3560 Training loss: 1.2132 14.2579 sec/batch\n",
      "Epoch 17/20  Iteration 2892/3560 Training loss: 1.2126 14.2055 sec/batch\n",
      "Epoch 17/20  Iteration 2893/3560 Training loss: 1.2128 14.3671 sec/batch\n",
      "Epoch 17/20  Iteration 2894/3560 Training loss: 1.2120 14.2390 sec/batch\n",
      "Epoch 17/20  Iteration 2895/3560 Training loss: 1.2117 14.3862 sec/batch\n",
      "Epoch 17/20  Iteration 2896/3560 Training loss: 1.2115 14.2763 sec/batch\n",
      "Epoch 17/20  Iteration 2897/3560 Training loss: 1.2113 14.3624 sec/batch\n",
      "Epoch 17/20  Iteration 2898/3560 Training loss: 1.2114 14.2373 sec/batch\n",
      "Epoch 17/20  Iteration 2899/3560 Training loss: 1.2110 14.3277 sec/batch\n",
      "Epoch 17/20  Iteration 2900/3560 Training loss: 1.2115 14.2830 sec/batch\n",
      "Epoch 17/20  Iteration 2901/3560 Training loss: 1.2114 14.6941 sec/batch\n",
      "Epoch 17/20  Iteration 2902/3560 Training loss: 1.2116 14.3145 sec/batch\n",
      "Epoch 17/20  Iteration 2903/3560 Training loss: 1.2116 14.3529 sec/batch\n",
      "Epoch 17/20  Iteration 2904/3560 Training loss: 1.2116 14.2095 sec/batch\n",
      "Epoch 17/20  Iteration 2905/3560 Training loss: 1.2119 14.2850 sec/batch\n",
      "Epoch 17/20  Iteration 2906/3560 Training loss: 1.2118 14.3279 sec/batch\n",
      "Epoch 17/20  Iteration 2907/3560 Training loss: 1.2113 14.2957 sec/batch\n",
      "Epoch 17/20  Iteration 2908/3560 Training loss: 1.2119 14.2609 sec/batch\n",
      "Epoch 17/20  Iteration 2909/3560 Training loss: 1.2119 14.3405 sec/batch\n",
      "Epoch 17/20  Iteration 2910/3560 Training loss: 1.2126 14.2552 sec/batch\n",
      "Epoch 17/20  Iteration 2911/3560 Training loss: 1.2131 14.3010 sec/batch\n",
      "Epoch 17/20  Iteration 2912/3560 Training loss: 1.2132 14.3784 sec/batch\n",
      "Epoch 17/20  Iteration 2913/3560 Training loss: 1.2132 14.3369 sec/batch\n",
      "Epoch 17/20  Iteration 2914/3560 Training loss: 1.2134 14.3116 sec/batch\n",
      "Epoch 17/20  Iteration 2915/3560 Training loss: 1.2136 14.2044 sec/batch\n",
      "Epoch 17/20  Iteration 2916/3560 Training loss: 1.2135 14.3507 sec/batch\n",
      "Epoch 17/20  Iteration 2917/3560 Training loss: 1.2135 14.2301 sec/batch\n",
      "Epoch 17/20  Iteration 2918/3560 Training loss: 1.2133 14.3464 sec/batch\n",
      "Epoch 17/20  Iteration 2919/3560 Training loss: 1.2139 14.3078 sec/batch\n",
      "Epoch 17/20  Iteration 2920/3560 Training loss: 1.2143 14.3722 sec/batch\n",
      "Epoch 17/20  Iteration 2921/3560 Training loss: 1.2147 14.3344 sec/batch\n",
      "Epoch 17/20  Iteration 2922/3560 Training loss: 1.2142 15.1163 sec/batch\n",
      "Epoch 17/20  Iteration 2923/3560 Training loss: 1.2142 14.2923 sec/batch\n",
      "Epoch 17/20  Iteration 2924/3560 Training loss: 1.2143 14.3359 sec/batch\n",
      "Epoch 17/20  Iteration 2925/3560 Training loss: 1.2142 14.2391 sec/batch\n",
      "Epoch 17/20  Iteration 2926/3560 Training loss: 1.2141 14.3506 sec/batch\n",
      "Epoch 17/20  Iteration 2927/3560 Training loss: 1.2135 14.3008 sec/batch\n",
      "Epoch 17/20  Iteration 2928/3560 Training loss: 1.2134 14.3487 sec/batch\n",
      "Epoch 17/20  Iteration 2929/3560 Training loss: 1.2131 14.3341 sec/batch\n",
      "Epoch 17/20  Iteration 2930/3560 Training loss: 1.2130 14.2721 sec/batch\n",
      "Epoch 17/20  Iteration 2931/3560 Training loss: 1.2126 14.3787 sec/batch\n",
      "Epoch 17/20  Iteration 2932/3560 Training loss: 1.2126 14.3254 sec/batch\n",
      "Epoch 17/20  Iteration 2933/3560 Training loss: 1.2123 14.3152 sec/batch\n",
      "Epoch 17/20  Iteration 2934/3560 Training loss: 1.2122 14.3019 sec/batch\n",
      "Epoch 17/20  Iteration 2935/3560 Training loss: 1.2120 14.3734 sec/batch\n",
      "Epoch 17/20  Iteration 2936/3560 Training loss: 1.2118 14.2621 sec/batch\n",
      "Epoch 17/20  Iteration 2937/3560 Training loss: 1.2114 14.4017 sec/batch\n",
      "Epoch 17/20  Iteration 2938/3560 Training loss: 1.2115 14.2954 sec/batch\n",
      "Epoch 17/20  Iteration 2939/3560 Training loss: 1.2113 14.2416 sec/batch\n",
      "Epoch 17/20  Iteration 2940/3560 Training loss: 1.2111 14.3584 sec/batch\n",
      "Epoch 17/20  Iteration 2941/3560 Training loss: 1.2110 14.3016 sec/batch\n",
      "Epoch 17/20  Iteration 2942/3560 Training loss: 1.2106 14.3370 sec/batch\n",
      "Epoch 17/20  Iteration 2943/3560 Training loss: 1.2105 14.7076 sec/batch\n",
      "Epoch 17/20  Iteration 2944/3560 Training loss: 1.2104 14.2988 sec/batch\n",
      "Epoch 17/20  Iteration 2945/3560 Training loss: 1.2104 14.3108 sec/batch\n",
      "Epoch 17/20  Iteration 2946/3560 Training loss: 1.2101 14.2898 sec/batch\n",
      "Epoch 17/20  Iteration 2947/3560 Training loss: 1.2097 14.2489 sec/batch\n",
      "Epoch 17/20  Iteration 2948/3560 Training loss: 1.2094 14.3004 sec/batch\n",
      "Epoch 17/20  Iteration 2949/3560 Training loss: 1.2094 14.3104 sec/batch\n",
      "Epoch 17/20  Iteration 2950/3560 Training loss: 1.2093 14.2663 sec/batch\n",
      "Epoch 17/20  Iteration 2951/3560 Training loss: 1.2092 14.2422 sec/batch\n",
      "Epoch 17/20  Iteration 2952/3560 Training loss: 1.2090 14.2846 sec/batch\n",
      "Epoch 17/20  Iteration 2953/3560 Training loss: 1.2089 14.6323 sec/batch\n",
      "Epoch 17/20  Iteration 2954/3560 Training loss: 1.2088 14.2956 sec/batch\n",
      "Epoch 17/20  Iteration 2955/3560 Training loss: 1.2088 14.2664 sec/batch\n",
      "Epoch 17/20  Iteration 2956/3560 Training loss: 1.2090 14.3060 sec/batch\n",
      "Epoch 17/20  Iteration 2957/3560 Training loss: 1.2088 14.2328 sec/batch\n",
      "Epoch 17/20  Iteration 2958/3560 Training loss: 1.2090 14.2851 sec/batch\n",
      "Epoch 17/20  Iteration 2959/3560 Training loss: 1.2088 14.2844 sec/batch\n",
      "Epoch 17/20  Iteration 2960/3560 Training loss: 1.2088 14.3318 sec/batch\n",
      "Epoch 17/20  Iteration 2961/3560 Training loss: 1.2087 14.2537 sec/batch\n",
      "Epoch 17/20  Iteration 2962/3560 Training loss: 1.2086 14.3652 sec/batch\n",
      "Epoch 17/20  Iteration 2963/3560 Training loss: 1.2083 14.3674 sec/batch\n",
      "Epoch 17/20  Iteration 2964/3560 Training loss: 1.2081 15.4070 sec/batch\n",
      "Epoch 17/20  Iteration 2965/3560 Training loss: 1.2080 14.3160 sec/batch\n",
      "Epoch 17/20  Iteration 2966/3560 Training loss: 1.2081 14.2671 sec/batch\n",
      "Epoch 17/20  Iteration 2967/3560 Training loss: 1.2080 14.3279 sec/batch\n",
      "Epoch 17/20  Iteration 2968/3560 Training loss: 1.2081 14.3193 sec/batch\n",
      "Epoch 17/20  Iteration 2969/3560 Training loss: 1.2080 14.1592 sec/batch\n",
      "Epoch 17/20  Iteration 2970/3560 Training loss: 1.2077 14.3473 sec/batch\n",
      "Epoch 17/20  Iteration 2971/3560 Training loss: 1.2074 14.2848 sec/batch\n",
      "Epoch 17/20  Iteration 2972/3560 Training loss: 1.2074 14.3053 sec/batch\n",
      "Epoch 17/20  Iteration 2973/3560 Training loss: 1.2073 14.3010 sec/batch\n",
      "Epoch 17/20  Iteration 2974/3560 Training loss: 1.2069 14.2963 sec/batch\n",
      "Epoch 17/20  Iteration 2975/3560 Training loss: 1.2069 14.2552 sec/batch\n",
      "Epoch 17/20  Iteration 2976/3560 Training loss: 1.2069 14.5779 sec/batch\n",
      "Epoch 17/20  Iteration 2977/3560 Training loss: 1.2067 15.0020 sec/batch\n",
      "Epoch 17/20  Iteration 2978/3560 Training loss: 1.2064 13.9804 sec/batch\n",
      "Epoch 17/20  Iteration 2979/3560 Training loss: 1.2061 14.0121 sec/batch\n",
      "Epoch 17/20  Iteration 2980/3560 Training loss: 1.2061 13.9925 sec/batch\n",
      "Epoch 17/20  Iteration 2981/3560 Training loss: 1.2062 13.9441 sec/batch\n",
      "Epoch 17/20  Iteration 2982/3560 Training loss: 1.2062 13.9764 sec/batch\n",
      "Epoch 17/20  Iteration 2983/3560 Training loss: 1.2061 13.9438 sec/batch\n",
      "Epoch 17/20  Iteration 2984/3560 Training loss: 1.2061 14.1291 sec/batch\n",
      "Epoch 17/20  Iteration 2985/3560 Training loss: 1.2063 15.2844 sec/batch\n",
      "Epoch 17/20  Iteration 2986/3560 Training loss: 1.2064 13.9775 sec/batch\n",
      "Epoch 17/20  Iteration 2987/3560 Training loss: 1.2064 14.0658 sec/batch\n",
      "Epoch 17/20  Iteration 2988/3560 Training loss: 1.2064 13.8908 sec/batch\n",
      "Epoch 17/20  Iteration 2989/3560 Training loss: 1.2067 13.9641 sec/batch\n",
      "Epoch 17/20  Iteration 2990/3560 Training loss: 1.2068 14.0076 sec/batch\n",
      "Epoch 17/20  Iteration 2991/3560 Training loss: 1.2067 13.9108 sec/batch\n",
      "Epoch 17/20  Iteration 2992/3560 Training loss: 1.2069 13.9855 sec/batch\n",
      "Epoch 17/20  Iteration 2993/3560 Training loss: 1.2068 13.9737 sec/batch\n",
      "Epoch 17/20  Iteration 2994/3560 Training loss: 1.2070 13.8883 sec/batch\n",
      "Epoch 17/20  Iteration 2995/3560 Training loss: 1.2070 13.9083 sec/batch\n",
      "Epoch 17/20  Iteration 2996/3560 Training loss: 1.2072 14.0605 sec/batch\n",
      "Epoch 17/20  Iteration 2997/3560 Training loss: 1.2074 13.9448 sec/batch\n",
      "Epoch 17/20  Iteration 2998/3560 Training loss: 1.2073 14.0217 sec/batch\n",
      "Epoch 17/20  Iteration 2999/3560 Training loss: 1.2071 13.9674 sec/batch\n",
      "Epoch 17/20  Iteration 3000/3560 Training loss: 1.2069 13.9284 sec/batch\n",
      "Validation loss: 1.13895 Saving checkpoint!\n",
      "Epoch 17/20  Iteration 3001/3560 Training loss: 1.2083 13.5960 sec/batch\n",
      "Epoch 17/20  Iteration 3002/3560 Training loss: 1.2083 13.9653 sec/batch\n",
      "Epoch 17/20  Iteration 3003/3560 Training loss: 1.2084 13.9547 sec/batch\n",
      "Epoch 17/20  Iteration 3004/3560 Training loss: 1.2085 13.9961 sec/batch\n",
      "Epoch 17/20  Iteration 3005/3560 Training loss: 1.2085 13.9566 sec/batch\n",
      "Epoch 17/20  Iteration 3006/3560 Training loss: 1.2084 14.0863 sec/batch\n",
      "Epoch 17/20  Iteration 3007/3560 Training loss: 1.2083 13.9868 sec/batch\n",
      "Epoch 17/20  Iteration 3008/3560 Training loss: 1.2084 13.9651 sec/batch\n",
      "Epoch 17/20  Iteration 3009/3560 Training loss: 1.2086 13.9244 sec/batch\n",
      "Epoch 17/20  Iteration 3010/3560 Training loss: 1.2086 13.9571 sec/batch\n",
      "Epoch 17/20  Iteration 3011/3560 Training loss: 1.2086 14.1244 sec/batch\n",
      "Epoch 17/20  Iteration 3012/3560 Training loss: 1.2086 14.0146 sec/batch\n",
      "Epoch 17/20  Iteration 3013/3560 Training loss: 1.2086 14.0194 sec/batch\n",
      "Epoch 17/20  Iteration 3014/3560 Training loss: 1.2085 14.0561 sec/batch\n",
      "Epoch 17/20  Iteration 3015/3560 Training loss: 1.2087 14.0433 sec/batch\n",
      "Epoch 17/20  Iteration 3016/3560 Training loss: 1.2091 13.9522 sec/batch\n",
      "Epoch 17/20  Iteration 3017/3560 Training loss: 1.2091 14.0127 sec/batch\n",
      "Epoch 17/20  Iteration 3018/3560 Training loss: 1.2091 14.0461 sec/batch\n",
      "Epoch 17/20  Iteration 3019/3560 Training loss: 1.2091 13.8941 sec/batch\n",
      "Epoch 17/20  Iteration 3020/3560 Training loss: 1.2090 14.7227 sec/batch\n",
      "Epoch 17/20  Iteration 3021/3560 Training loss: 1.2092 14.0126 sec/batch\n",
      "Epoch 17/20  Iteration 3022/3560 Training loss: 1.2092 13.9251 sec/batch\n",
      "Epoch 17/20  Iteration 3023/3560 Training loss: 1.2093 14.0999 sec/batch\n",
      "Epoch 17/20  Iteration 3024/3560 Training loss: 1.2091 14.0126 sec/batch\n",
      "Epoch 17/20  Iteration 3025/3560 Training loss: 1.2090 13.9777 sec/batch\n",
      "Epoch 17/20  Iteration 3026/3560 Training loss: 1.2092 13.9437 sec/batch\n",
      "Epoch 18/20  Iteration 3027/3560 Training loss: 1.3178 13.9483 sec/batch\n",
      "Epoch 18/20  Iteration 3028/3560 Training loss: 1.2713 13.9748 sec/batch\n",
      "Epoch 18/20  Iteration 3029/3560 Training loss: 1.2519 14.0327 sec/batch\n",
      "Epoch 18/20  Iteration 3030/3560 Training loss: 1.2460 13.9845 sec/batch\n",
      "Epoch 18/20  Iteration 3031/3560 Training loss: 1.2326 13.9821 sec/batch\n",
      "Epoch 18/20  Iteration 3032/3560 Training loss: 1.2216 14.0002 sec/batch\n",
      "Epoch 18/20  Iteration 3033/3560 Training loss: 1.2215 13.9422 sec/batch\n",
      "Epoch 18/20  Iteration 3034/3560 Training loss: 1.2190 14.0506 sec/batch\n",
      "Epoch 18/20  Iteration 3035/3560 Training loss: 1.2176 13.9294 sec/batch\n",
      "Epoch 18/20  Iteration 3036/3560 Training loss: 1.2166 13.9999 sec/batch\n",
      "Epoch 18/20  Iteration 3037/3560 Training loss: 1.2143 13.9164 sec/batch\n",
      "Epoch 18/20  Iteration 3038/3560 Training loss: 1.2143 13.9721 sec/batch\n",
      "Epoch 18/20  Iteration 3039/3560 Training loss: 1.2141 13.8788 sec/batch\n",
      "Epoch 18/20  Iteration 3040/3560 Training loss: 1.2145 14.0521 sec/batch\n",
      "Epoch 18/20  Iteration 3041/3560 Training loss: 1.2127 14.3638 sec/batch\n",
      "Epoch 18/20  Iteration 3042/3560 Training loss: 1.2103 14.7296 sec/batch\n",
      "Epoch 18/20  Iteration 3043/3560 Training loss: 1.2102 13.9626 sec/batch\n",
      "Epoch 18/20  Iteration 3044/3560 Training loss: 1.2110 13.9444 sec/batch\n",
      "Epoch 18/20  Iteration 3045/3560 Training loss: 1.2105 13.9236 sec/batch\n",
      "Epoch 18/20  Iteration 3046/3560 Training loss: 1.2116 14.0454 sec/batch\n",
      "Epoch 18/20  Iteration 3047/3560 Training loss: 1.2111 13.9537 sec/batch\n",
      "Epoch 18/20  Iteration 3048/3560 Training loss: 1.2111 14.0632 sec/batch\n",
      "Epoch 18/20  Iteration 3049/3560 Training loss: 1.2106 13.9073 sec/batch\n",
      "Epoch 18/20  Iteration 3050/3560 Training loss: 1.2109 13.9021 sec/batch\n",
      "Epoch 18/20  Iteration 3051/3560 Training loss: 1.2103 13.9140 sec/batch\n",
      "Epoch 18/20  Iteration 3052/3560 Training loss: 1.2087 14.0063 sec/batch\n",
      "Epoch 18/20  Iteration 3053/3560 Training loss: 1.2075 13.9588 sec/batch\n",
      "Epoch 18/20  Iteration 3054/3560 Training loss: 1.2082 13.9773 sec/batch\n",
      "Epoch 18/20  Iteration 3055/3560 Training loss: 1.2081 14.0904 sec/batch\n",
      "Epoch 18/20  Iteration 3056/3560 Training loss: 1.2084 13.9450 sec/batch\n",
      "Epoch 18/20  Iteration 3057/3560 Training loss: 1.2076 14.0857 sec/batch\n",
      "Epoch 18/20  Iteration 3058/3560 Training loss: 1.2066 13.9868 sec/batch\n",
      "Epoch 18/20  Iteration 3059/3560 Training loss: 1.2064 14.0316 sec/batch\n",
      "Epoch 18/20  Iteration 3060/3560 Training loss: 1.2066 13.9491 sec/batch\n",
      "Epoch 18/20  Iteration 3061/3560 Training loss: 1.2060 13.9820 sec/batch\n",
      "Epoch 18/20  Iteration 3062/3560 Training loss: 1.2058 13.8848 sec/batch\n",
      "Epoch 18/20  Iteration 3063/3560 Training loss: 1.2050 14.3318 sec/batch\n",
      "Epoch 18/20  Iteration 3064/3560 Training loss: 1.2039 13.9534 sec/batch\n",
      "Epoch 18/20  Iteration 3065/3560 Training loss: 1.2026 14.0626 sec/batch\n",
      "Epoch 18/20  Iteration 3066/3560 Training loss: 1.2021 13.9513 sec/batch\n",
      "Epoch 18/20  Iteration 3067/3560 Training loss: 1.2013 13.9972 sec/batch\n",
      "Epoch 18/20  Iteration 3068/3560 Training loss: 1.2022 14.0261 sec/batch\n",
      "Epoch 18/20  Iteration 3069/3560 Training loss: 1.2019 13.9832 sec/batch\n",
      "Epoch 18/20  Iteration 3070/3560 Training loss: 1.2012 14.3144 sec/batch\n",
      "Epoch 18/20  Iteration 3071/3560 Training loss: 1.2012 13.9620 sec/batch\n",
      "Epoch 18/20  Iteration 3072/3560 Training loss: 1.2005 13.9916 sec/batch\n",
      "Epoch 18/20  Iteration 3073/3560 Training loss: 1.2001 14.0471 sec/batch\n",
      "Epoch 18/20  Iteration 3074/3560 Training loss: 1.1998 14.0572 sec/batch\n",
      "Epoch 18/20  Iteration 3075/3560 Training loss: 1.1997 13.9608 sec/batch\n",
      "Epoch 18/20  Iteration 3076/3560 Training loss: 1.1999 13.9662 sec/batch\n",
      "Epoch 18/20  Iteration 3077/3560 Training loss: 1.1993 13.8926 sec/batch\n",
      "Epoch 18/20  Iteration 3078/3560 Training loss: 1.1999 14.0389 sec/batch\n",
      "Epoch 18/20  Iteration 3079/3560 Training loss: 1.1998 13.9101 sec/batch\n",
      "Epoch 18/20  Iteration 3080/3560 Training loss: 1.2000 13.9969 sec/batch\n",
      "Epoch 18/20  Iteration 3081/3560 Training loss: 1.1998 14.0000 sec/batch\n",
      "Epoch 18/20  Iteration 3082/3560 Training loss: 1.1998 13.9480 sec/batch\n",
      "Epoch 18/20  Iteration 3083/3560 Training loss: 1.2001 14.0634 sec/batch\n",
      "Epoch 18/20  Iteration 3084/3560 Training loss: 1.1998 14.6968 sec/batch\n",
      "Epoch 18/20  Iteration 3085/3560 Training loss: 1.1994 13.9416 sec/batch\n",
      "Epoch 18/20  Iteration 3086/3560 Training loss: 1.2000 13.9683 sec/batch\n",
      "Epoch 18/20  Iteration 3087/3560 Training loss: 1.2000 13.9268 sec/batch\n",
      "Epoch 18/20  Iteration 3088/3560 Training loss: 1.2009 14.0188 sec/batch\n",
      "Epoch 18/20  Iteration 3089/3560 Training loss: 1.2012 13.9311 sec/batch\n",
      "Epoch 18/20  Iteration 3090/3560 Training loss: 1.2015 13.9850 sec/batch\n",
      "Epoch 18/20  Iteration 3091/3560 Training loss: 1.2014 14.0224 sec/batch\n",
      "Epoch 18/20  Iteration 3092/3560 Training loss: 1.2015 13.9264 sec/batch\n",
      "Epoch 18/20  Iteration 3093/3560 Training loss: 1.2017 13.9800 sec/batch\n",
      "Epoch 18/20  Iteration 3094/3560 Training loss: 1.2015 13.9930 sec/batch\n",
      "Epoch 18/20  Iteration 3095/3560 Training loss: 1.2015 14.0009 sec/batch\n",
      "Epoch 18/20  Iteration 3096/3560 Training loss: 1.2014 13.9372 sec/batch\n",
      "Epoch 18/20  Iteration 3097/3560 Training loss: 1.2019 14.0550 sec/batch\n",
      "Epoch 18/20  Iteration 3098/3560 Training loss: 1.2021 13.9470 sec/batch\n",
      "Epoch 18/20  Iteration 3099/3560 Training loss: 1.2026 13.8961 sec/batch\n",
      "Epoch 18/20  Iteration 3100/3560 Training loss: 1.2024 13.9885 sec/batch\n",
      "Epoch 18/20  Iteration 3101/3560 Training loss: 1.2023 13.9863 sec/batch\n",
      "Epoch 18/20  Iteration 3102/3560 Training loss: 1.2024 14.0593 sec/batch\n",
      "Epoch 18/20  Iteration 3103/3560 Training loss: 1.2023 13.9687 sec/batch\n",
      "Epoch 18/20  Iteration 3104/3560 Training loss: 1.2021 13.9129 sec/batch\n",
      "Epoch 18/20  Iteration 3105/3560 Training loss: 1.2014 13.9455 sec/batch\n",
      "Epoch 18/20  Iteration 3106/3560 Training loss: 1.2014 14.5337 sec/batch\n",
      "Epoch 18/20  Iteration 3107/3560 Training loss: 1.2010 14.0319 sec/batch\n",
      "Epoch 18/20  Iteration 3108/3560 Training loss: 1.2009 14.1240 sec/batch\n",
      "Epoch 18/20  Iteration 3109/3560 Training loss: 1.2006 14.0252 sec/batch\n",
      "Epoch 18/20  Iteration 3110/3560 Training loss: 1.2006 13.9241 sec/batch\n",
      "Epoch 18/20  Iteration 3111/3560 Training loss: 1.2004 13.9668 sec/batch\n",
      "Epoch 18/20  Iteration 3112/3560 Training loss: 1.2004 13.9757 sec/batch\n",
      "Epoch 18/20  Iteration 3113/3560 Training loss: 1.2001 14.0406 sec/batch\n",
      "Epoch 18/20  Iteration 3114/3560 Training loss: 1.1999 13.9733 sec/batch\n",
      "Epoch 18/20  Iteration 3115/3560 Training loss: 1.1996 14.0254 sec/batch\n",
      "Epoch 18/20  Iteration 3116/3560 Training loss: 1.1997 13.9658 sec/batch\n",
      "Epoch 18/20  Iteration 3117/3560 Training loss: 1.1995 14.0266 sec/batch\n",
      "Epoch 18/20  Iteration 3118/3560 Training loss: 1.1994 14.0075 sec/batch\n",
      "Epoch 18/20  Iteration 3119/3560 Training loss: 1.1991 14.0810 sec/batch\n",
      "Epoch 18/20  Iteration 3120/3560 Training loss: 1.1987 13.8888 sec/batch\n",
      "Epoch 18/20  Iteration 3121/3560 Training loss: 1.1985 13.9969 sec/batch\n",
      "Epoch 18/20  Iteration 3122/3560 Training loss: 1.1985 13.9601 sec/batch\n",
      "Epoch 18/20  Iteration 3123/3560 Training loss: 1.1986 14.0221 sec/batch\n",
      "Epoch 18/20  Iteration 3124/3560 Training loss: 1.1982 13.9631 sec/batch\n",
      "Epoch 18/20  Iteration 3125/3560 Training loss: 1.1979 14.0480 sec/batch\n",
      "Epoch 18/20  Iteration 3126/3560 Training loss: 1.1976 14.0523 sec/batch\n",
      "Epoch 18/20  Iteration 3127/3560 Training loss: 1.1976 15.2122 sec/batch\n",
      "Epoch 18/20  Iteration 3128/3560 Training loss: 1.1975 14.0170 sec/batch\n",
      "Epoch 18/20  Iteration 3129/3560 Training loss: 1.1975 14.0133 sec/batch\n",
      "Epoch 18/20  Iteration 3130/3560 Training loss: 1.1974 14.0259 sec/batch\n",
      "Epoch 18/20  Iteration 3131/3560 Training loss: 1.1973 13.9865 sec/batch\n",
      "Epoch 18/20  Iteration 3132/3560 Training loss: 1.1971 14.0104 sec/batch\n",
      "Epoch 18/20  Iteration 3133/3560 Training loss: 1.1971 13.9276 sec/batch\n",
      "Epoch 18/20  Iteration 3134/3560 Training loss: 1.1971 14.1282 sec/batch\n",
      "Epoch 18/20  Iteration 3135/3560 Training loss: 1.1969 13.8825 sec/batch\n",
      "Epoch 18/20  Iteration 3136/3560 Training loss: 1.1970 14.0850 sec/batch\n",
      "Epoch 18/20  Iteration 3137/3560 Training loss: 1.1968 13.9530 sec/batch\n",
      "Epoch 18/20  Iteration 3138/3560 Training loss: 1.1968 13.9957 sec/batch\n",
      "Epoch 18/20  Iteration 3139/3560 Training loss: 1.1967 13.9595 sec/batch\n",
      "Epoch 18/20  Iteration 3140/3560 Training loss: 1.1967 14.0249 sec/batch\n",
      "Epoch 18/20  Iteration 3141/3560 Training loss: 1.1964 14.0070 sec/batch\n",
      "Epoch 18/20  Iteration 3142/3560 Training loss: 1.1962 14.0902 sec/batch\n",
      "Epoch 18/20  Iteration 3143/3560 Training loss: 1.1962 14.0699 sec/batch\n",
      "Epoch 18/20  Iteration 3144/3560 Training loss: 1.1962 13.9797 sec/batch\n",
      "Epoch 18/20  Iteration 3145/3560 Training loss: 1.1961 13.9799 sec/batch\n",
      "Epoch 18/20  Iteration 3146/3560 Training loss: 1.1961 13.9278 sec/batch\n",
      "Epoch 18/20  Iteration 3147/3560 Training loss: 1.1960 14.1756 sec/batch\n",
      "Epoch 18/20  Iteration 3148/3560 Training loss: 1.1956 14.3487 sec/batch\n",
      "Epoch 18/20  Iteration 3149/3560 Training loss: 1.1953 13.9951 sec/batch\n",
      "Epoch 18/20  Iteration 3150/3560 Training loss: 1.1953 13.9715 sec/batch\n",
      "Epoch 18/20  Iteration 3151/3560 Training loss: 1.1952 14.1223 sec/batch\n",
      "Epoch 18/20  Iteration 3152/3560 Training loss: 1.1949 13.9598 sec/batch\n",
      "Epoch 18/20  Iteration 3153/3560 Training loss: 1.1948 14.0026 sec/batch\n",
      "Epoch 18/20  Iteration 3154/3560 Training loss: 1.1948 13.9946 sec/batch\n",
      "Epoch 18/20  Iteration 3155/3560 Training loss: 1.1946 14.1033 sec/batch\n",
      "Epoch 18/20  Iteration 3156/3560 Training loss: 1.1943 13.9222 sec/batch\n",
      "Epoch 18/20  Iteration 3157/3560 Training loss: 1.1940 14.0133 sec/batch\n",
      "Epoch 18/20  Iteration 3158/3560 Training loss: 1.1939 13.9858 sec/batch\n",
      "Epoch 18/20  Iteration 3159/3560 Training loss: 1.1940 14.0053 sec/batch\n",
      "Epoch 18/20  Iteration 3160/3560 Training loss: 1.1940 14.1431 sec/batch\n",
      "Epoch 18/20  Iteration 3161/3560 Training loss: 1.1940 13.9761 sec/batch\n",
      "Epoch 18/20  Iteration 3162/3560 Training loss: 1.1940 13.9723 sec/batch\n",
      "Epoch 18/20  Iteration 3163/3560 Training loss: 1.1942 13.9821 sec/batch\n",
      "Epoch 18/20  Iteration 3164/3560 Training loss: 1.1943 13.9993 sec/batch\n",
      "Epoch 18/20  Iteration 3165/3560 Training loss: 1.1943 13.9403 sec/batch\n",
      "Epoch 18/20  Iteration 3166/3560 Training loss: 1.1942 14.1294 sec/batch\n",
      "Epoch 18/20  Iteration 3167/3560 Training loss: 1.1945 14.0083 sec/batch\n",
      "Epoch 18/20  Iteration 3168/3560 Training loss: 1.1946 14.0094 sec/batch\n",
      "Epoch 18/20  Iteration 3169/3560 Training loss: 1.1946 14.0090 sec/batch\n",
      "Epoch 18/20  Iteration 3170/3560 Training loss: 1.1948 14.6224 sec/batch\n",
      "Epoch 18/20  Iteration 3171/3560 Training loss: 1.1947 13.9786 sec/batch\n",
      "Epoch 18/20  Iteration 3172/3560 Training loss: 1.1949 13.9670 sec/batch\n",
      "Epoch 18/20  Iteration 3173/3560 Training loss: 1.1949 14.0204 sec/batch\n",
      "Epoch 18/20  Iteration 3174/3560 Training loss: 1.1951 14.0281 sec/batch\n",
      "Epoch 18/20  Iteration 3175/3560 Training loss: 1.1953 14.0060 sec/batch\n",
      "Epoch 18/20  Iteration 3176/3560 Training loss: 1.1952 13.9644 sec/batch\n",
      "Epoch 18/20  Iteration 3177/3560 Training loss: 1.1949 14.1371 sec/batch\n",
      "Epoch 18/20  Iteration 3178/3560 Training loss: 1.1947 13.9740 sec/batch\n",
      "Epoch 18/20  Iteration 3179/3560 Training loss: 1.1947 14.0313 sec/batch\n",
      "Epoch 18/20  Iteration 3180/3560 Training loss: 1.1947 14.0432 sec/batch\n",
      "Epoch 18/20  Iteration 3181/3560 Training loss: 1.1947 13.9630 sec/batch\n",
      "Epoch 18/20  Iteration 3182/3560 Training loss: 1.1947 13.9612 sec/batch\n",
      "Epoch 18/20  Iteration 3183/3560 Training loss: 1.1947 14.0117 sec/batch\n",
      "Epoch 18/20  Iteration 3184/3560 Training loss: 1.1946 14.0165 sec/batch\n",
      "Epoch 18/20  Iteration 3185/3560 Training loss: 1.1944 14.0080 sec/batch\n",
      "Epoch 18/20  Iteration 3186/3560 Training loss: 1.1945 13.9049 sec/batch\n",
      "Epoch 18/20  Iteration 3187/3560 Training loss: 1.1946 14.0489 sec/batch\n",
      "Epoch 18/20  Iteration 3188/3560 Training loss: 1.1947 14.0384 sec/batch\n",
      "Epoch 18/20  Iteration 3189/3560 Training loss: 1.1946 13.9345 sec/batch\n",
      "Epoch 18/20  Iteration 3190/3560 Training loss: 1.1947 13.9552 sec/batch\n",
      "Epoch 18/20  Iteration 3191/3560 Training loss: 1.1947 14.5320 sec/batch\n",
      "Epoch 18/20  Iteration 3192/3560 Training loss: 1.1947 14.0352 sec/batch\n",
      "Epoch 18/20  Iteration 3193/3560 Training loss: 1.1948 13.9345 sec/batch\n",
      "Epoch 18/20  Iteration 3194/3560 Training loss: 1.1952 14.1219 sec/batch\n",
      "Epoch 18/20  Iteration 3195/3560 Training loss: 1.1953 14.0476 sec/batch\n",
      "Epoch 18/20  Iteration 3196/3560 Training loss: 1.1953 14.3803 sec/batch\n",
      "Epoch 18/20  Iteration 3197/3560 Training loss: 1.1953 14.0059 sec/batch\n",
      "Epoch 18/20  Iteration 3198/3560 Training loss: 1.1951 14.0249 sec/batch\n",
      "Epoch 18/20  Iteration 3199/3560 Training loss: 1.1953 14.0323 sec/batch\n",
      "Epoch 18/20  Iteration 3200/3560 Training loss: 1.1954 13.9484 sec/batch\n",
      "Validation loss: 1.12932 Saving checkpoint!\n",
      "Epoch 18/20  Iteration 3201/3560 Training loss: 1.1966 13.7029 sec/batch\n",
      "Epoch 18/20  Iteration 3202/3560 Training loss: 1.1965 14.0046 sec/batch\n",
      "Epoch 18/20  Iteration 3203/3560 Training loss: 1.1965 13.9131 sec/batch\n",
      "Epoch 18/20  Iteration 3204/3560 Training loss: 1.1967 14.0864 sec/batch\n",
      "Epoch 19/20  Iteration 3205/3560 Training loss: 1.3094 15.4099 sec/batch\n",
      "Epoch 19/20  Iteration 3206/3560 Training loss: 1.2612 14.0521 sec/batch\n",
      "Epoch 19/20  Iteration 3207/3560 Training loss: 1.2428 13.9412 sec/batch\n",
      "Epoch 19/20  Iteration 3208/3560 Training loss: 1.2368 13.9711 sec/batch\n",
      "Epoch 19/20  Iteration 3209/3560 Training loss: 1.2243 14.0337 sec/batch\n",
      "Epoch 19/20  Iteration 3210/3560 Training loss: 1.2126 14.0088 sec/batch\n",
      "Epoch 19/20  Iteration 3211/3560 Training loss: 1.2128 14.0037 sec/batch\n",
      "Epoch 19/20  Iteration 3212/3560 Training loss: 1.2110 14.0783 sec/batch\n",
      "Epoch 19/20  Iteration 3213/3560 Training loss: 1.2107 13.9957 sec/batch\n",
      "Epoch 19/20  Iteration 3214/3560 Training loss: 1.2079 13.9379 sec/batch\n",
      "Epoch 19/20  Iteration 3215/3560 Training loss: 1.2041 14.0107 sec/batch\n",
      "Epoch 19/20  Iteration 3216/3560 Training loss: 1.2043 14.0257 sec/batch\n",
      "Epoch 19/20  Iteration 3217/3560 Training loss: 1.2047 13.9850 sec/batch\n",
      "Epoch 19/20  Iteration 3218/3560 Training loss: 1.2049 14.0209 sec/batch\n",
      "Epoch 19/20  Iteration 3219/3560 Training loss: 1.2035 13.9614 sec/batch\n",
      "Epoch 19/20  Iteration 3220/3560 Training loss: 1.2016 13.9184 sec/batch\n",
      "Epoch 19/20  Iteration 3221/3560 Training loss: 1.2013 14.0724 sec/batch\n",
      "Epoch 19/20  Iteration 3222/3560 Training loss: 1.2024 13.9707 sec/batch\n",
      "Epoch 19/20  Iteration 3223/3560 Training loss: 1.2015 13.9718 sec/batch\n",
      "Epoch 19/20  Iteration 3224/3560 Training loss: 1.2026 14.0086 sec/batch\n",
      "Epoch 19/20  Iteration 3225/3560 Training loss: 1.2021 13.9979 sec/batch\n",
      "Epoch 19/20  Iteration 3226/3560 Training loss: 1.2018 13.9073 sec/batch\n",
      "Epoch 19/20  Iteration 3227/3560 Training loss: 1.2007 14.4588 sec/batch\n",
      "Epoch 19/20  Iteration 3228/3560 Training loss: 1.2006 13.9308 sec/batch\n",
      "Epoch 19/20  Iteration 3229/3560 Training loss: 1.2003 13.9802 sec/batch\n",
      "Epoch 19/20  Iteration 3230/3560 Training loss: 1.1987 14.0564 sec/batch\n",
      "Epoch 19/20  Iteration 3231/3560 Training loss: 1.1977 14.0104 sec/batch\n",
      "Epoch 19/20  Iteration 3232/3560 Training loss: 1.1982 13.9330 sec/batch\n",
      "Epoch 19/20  Iteration 3233/3560 Training loss: 1.1981 14.0371 sec/batch\n",
      "Epoch 19/20  Iteration 3234/3560 Training loss: 1.1983 14.0210 sec/batch\n",
      "Epoch 19/20  Iteration 3235/3560 Training loss: 1.1977 14.0173 sec/batch\n",
      "Epoch 19/20  Iteration 3236/3560 Training loss: 1.1968 14.0531 sec/batch\n",
      "Epoch 19/20  Iteration 3237/3560 Training loss: 1.1966 13.9979 sec/batch\n",
      "Epoch 19/20  Iteration 3238/3560 Training loss: 1.1965 13.9867 sec/batch\n",
      "Epoch 19/20  Iteration 3239/3560 Training loss: 1.1961 13.9188 sec/batch\n",
      "Epoch 19/20  Iteration 3240/3560 Training loss: 1.1958 14.0109 sec/batch\n",
      "Epoch 19/20  Iteration 3241/3560 Training loss: 1.1952 14.0136 sec/batch\n",
      "Epoch 19/20  Iteration 3242/3560 Training loss: 1.1940 14.0162 sec/batch\n",
      "Epoch 19/20  Iteration 3243/3560 Training loss: 1.1928 14.0409 sec/batch\n",
      "Epoch 19/20  Iteration 3244/3560 Training loss: 1.1926 13.9983 sec/batch\n",
      "Epoch 19/20  Iteration 3245/3560 Training loss: 1.1922 13.9997 sec/batch\n",
      "Epoch 19/20  Iteration 3246/3560 Training loss: 1.1929 14.0582 sec/batch\n",
      "Epoch 19/20  Iteration 3247/3560 Training loss: 1.1925 14.0265 sec/batch\n",
      "Epoch 19/20  Iteration 3248/3560 Training loss: 1.1919 14.6946 sec/batch\n",
      "Epoch 19/20  Iteration 3249/3560 Training loss: 1.1923 14.1218 sec/batch\n",
      "Epoch 19/20  Iteration 3250/3560 Training loss: 1.1917 13.9830 sec/batch\n",
      "Epoch 19/20  Iteration 3251/3560 Training loss: 1.1912 13.9565 sec/batch\n",
      "Epoch 19/20  Iteration 3252/3560 Training loss: 1.1908 13.9352 sec/batch\n",
      "Epoch 19/20  Iteration 3253/3560 Training loss: 1.1908 14.0125 sec/batch\n",
      "Epoch 19/20  Iteration 3254/3560 Training loss: 1.1910 14.0690 sec/batch\n",
      "Epoch 19/20  Iteration 3255/3560 Training loss: 1.1906 14.0294 sec/batch\n",
      "Epoch 19/20  Iteration 3256/3560 Training loss: 1.1912 13.9788 sec/batch\n",
      "Epoch 19/20  Iteration 3257/3560 Training loss: 1.1910 14.0607 sec/batch\n",
      "Epoch 19/20  Iteration 3258/3560 Training loss: 1.1910 13.9481 sec/batch\n",
      "Epoch 19/20  Iteration 3259/3560 Training loss: 1.1907 13.9842 sec/batch\n",
      "Epoch 19/20  Iteration 3260/3560 Training loss: 1.1908 14.0173 sec/batch\n",
      "Epoch 19/20  Iteration 3261/3560 Training loss: 1.1908 13.9918 sec/batch\n",
      "Epoch 19/20  Iteration 3262/3560 Training loss: 1.1906 14.0333 sec/batch\n",
      "Epoch 19/20  Iteration 3263/3560 Training loss: 1.1901 14.0624 sec/batch\n",
      "Epoch 19/20  Iteration 3264/3560 Training loss: 1.1906 14.0214 sec/batch\n",
      "Epoch 19/20  Iteration 3265/3560 Training loss: 1.1908 14.0490 sec/batch\n",
      "Epoch 19/20  Iteration 3266/3560 Training loss: 1.1915 14.0076 sec/batch\n",
      "Epoch 19/20  Iteration 3267/3560 Training loss: 1.1917 13.9949 sec/batch\n",
      "Epoch 19/20  Iteration 3268/3560 Training loss: 1.1919 14.0354 sec/batch\n",
      "Epoch 19/20  Iteration 3269/3560 Training loss: 1.1919 14.3794 sec/batch\n",
      "Epoch 19/20  Iteration 3270/3560 Training loss: 1.1921 14.0683 sec/batch\n",
      "Epoch 19/20  Iteration 3271/3560 Training loss: 1.1924 13.9536 sec/batch\n",
      "Epoch 19/20  Iteration 3272/3560 Training loss: 1.1922 13.9889 sec/batch\n",
      "Epoch 19/20  Iteration 3273/3560 Training loss: 1.1924 13.9032 sec/batch\n",
      "Epoch 19/20  Iteration 3274/3560 Training loss: 1.1924 14.0543 sec/batch\n",
      "Epoch 19/20  Iteration 3275/3560 Training loss: 1.1928 13.9734 sec/batch\n",
      "Epoch 19/20  Iteration 3276/3560 Training loss: 1.1931 14.1849 sec/batch\n",
      "Epoch 19/20  Iteration 3277/3560 Training loss: 1.1936 13.9735 sec/batch\n",
      "Epoch 19/20  Iteration 3278/3560 Training loss: 1.1932 13.9761 sec/batch\n",
      "Epoch 19/20  Iteration 3279/3560 Training loss: 1.1930 13.9552 sec/batch\n",
      "Epoch 19/20  Iteration 3280/3560 Training loss: 1.1932 14.0071 sec/batch\n",
      "Epoch 19/20  Iteration 3281/3560 Training loss: 1.1931 14.0637 sec/batch\n",
      "Epoch 19/20  Iteration 3282/3560 Training loss: 1.1930 13.9686 sec/batch\n",
      "Epoch 19/20  Iteration 3283/3560 Training loss: 1.1923 14.0373 sec/batch\n",
      "Epoch 19/20  Iteration 3284/3560 Training loss: 1.1924 14.0210 sec/batch\n",
      "Epoch 19/20  Iteration 3285/3560 Training loss: 1.1920 13.9630 sec/batch\n",
      "Epoch 19/20  Iteration 3286/3560 Training loss: 1.1919 13.9571 sec/batch\n",
      "Epoch 19/20  Iteration 3287/3560 Training loss: 1.1915 14.0343 sec/batch\n",
      "Epoch 19/20  Iteration 3288/3560 Training loss: 1.1915 14.0444 sec/batch\n",
      "Epoch 19/20  Iteration 3289/3560 Training loss: 1.1912 14.0728 sec/batch\n",
      "Epoch 19/20  Iteration 3290/3560 Training loss: 1.1910 13.9276 sec/batch\n",
      "Epoch 19/20  Iteration 3291/3560 Training loss: 1.1908 15.1852 sec/batch\n",
      "Epoch 19/20  Iteration 3292/3560 Training loss: 1.1905 14.0343 sec/batch\n",
      "Epoch 19/20  Iteration 3293/3560 Training loss: 1.1903 13.9434 sec/batch\n",
      "Epoch 19/20  Iteration 3294/3560 Training loss: 1.1903 13.9990 sec/batch\n",
      "Epoch 19/20  Iteration 3295/3560 Training loss: 1.1901 14.0234 sec/batch\n",
      "Epoch 19/20  Iteration 3296/3560 Training loss: 1.1900 13.9795 sec/batch\n",
      "Epoch 19/20  Iteration 3297/3560 Training loss: 1.1896 13.9948 sec/batch\n",
      "Epoch 19/20  Iteration 3298/3560 Training loss: 1.1893 14.0946 sec/batch\n",
      "Epoch 19/20  Iteration 3299/3560 Training loss: 1.1891 13.9925 sec/batch\n",
      "Epoch 19/20  Iteration 3300/3560 Training loss: 1.1892 13.9596 sec/batch\n",
      "Epoch 19/20  Iteration 3301/3560 Training loss: 1.1892 14.0006 sec/batch\n",
      "Epoch 19/20  Iteration 3302/3560 Training loss: 1.1888 13.9980 sec/batch\n",
      "Epoch 19/20  Iteration 3303/3560 Training loss: 1.1886 14.0172 sec/batch\n",
      "Epoch 19/20  Iteration 3304/3560 Training loss: 1.1883 14.0435 sec/batch\n",
      "Epoch 19/20  Iteration 3305/3560 Training loss: 1.1883 14.0025 sec/batch\n",
      "Epoch 19/20  Iteration 3306/3560 Training loss: 1.1882 14.0664 sec/batch\n",
      "Epoch 19/20  Iteration 3307/3560 Training loss: 1.1882 14.0290 sec/batch\n",
      "Epoch 19/20  Iteration 3308/3560 Training loss: 1.1880 14.0096 sec/batch\n",
      "Epoch 19/20  Iteration 3309/3560 Training loss: 1.1879 14.0048 sec/batch\n",
      "Epoch 19/20  Iteration 3310/3560 Training loss: 1.1877 14.0258 sec/batch\n",
      "Epoch 19/20  Iteration 3311/3560 Training loss: 1.1877 14.0125 sec/batch\n",
      "Epoch 19/20  Iteration 3312/3560 Training loss: 1.1877 14.9430 sec/batch\n",
      "Epoch 19/20  Iteration 3313/3560 Training loss: 1.1875 14.0568 sec/batch\n",
      "Epoch 19/20  Iteration 3314/3560 Training loss: 1.1875 13.9875 sec/batch\n",
      "Epoch 19/20  Iteration 3315/3560 Training loss: 1.1873 14.0912 sec/batch\n",
      "Epoch 19/20  Iteration 3316/3560 Training loss: 1.1873 13.9347 sec/batch\n",
      "Epoch 19/20  Iteration 3317/3560 Training loss: 1.1872 13.9900 sec/batch\n",
      "Epoch 19/20  Iteration 3318/3560 Training loss: 1.1871 13.9278 sec/batch\n",
      "Epoch 19/20  Iteration 3319/3560 Training loss: 1.1869 13.9909 sec/batch\n",
      "Epoch 19/20  Iteration 3320/3560 Training loss: 1.1865 14.2679 sec/batch\n",
      "Epoch 19/20  Iteration 3321/3560 Training loss: 1.1865 14.0464 sec/batch\n",
      "Epoch 19/20  Iteration 3322/3560 Training loss: 1.1865 13.9934 sec/batch\n",
      "Epoch 19/20  Iteration 3323/3560 Training loss: 1.1864 13.9951 sec/batch\n",
      "Epoch 19/20  Iteration 3324/3560 Training loss: 1.1864 13.9653 sec/batch\n",
      "Epoch 19/20  Iteration 3325/3560 Training loss: 1.1863 14.0021 sec/batch\n",
      "Epoch 19/20  Iteration 3326/3560 Training loss: 1.1860 13.9949 sec/batch\n",
      "Epoch 19/20  Iteration 3327/3560 Training loss: 1.1857 13.9562 sec/batch\n",
      "Epoch 19/20  Iteration 3328/3560 Training loss: 1.1856 13.9450 sec/batch\n",
      "Epoch 19/20  Iteration 3329/3560 Training loss: 1.1855 14.0423 sec/batch\n",
      "Epoch 19/20  Iteration 3330/3560 Training loss: 1.1851 13.9328 sec/batch\n",
      "Epoch 19/20  Iteration 3331/3560 Training loss: 1.1851 14.0147 sec/batch\n",
      "Epoch 19/20  Iteration 3332/3560 Training loss: 1.1851 14.0438 sec/batch\n",
      "Epoch 19/20  Iteration 3333/3560 Training loss: 1.1849 14.3523 sec/batch\n",
      "Epoch 19/20  Iteration 3334/3560 Training loss: 1.1845 14.0160 sec/batch\n",
      "Epoch 19/20  Iteration 3335/3560 Training loss: 1.1841 14.0219 sec/batch\n",
      "Epoch 19/20  Iteration 3336/3560 Training loss: 1.1840 13.9660 sec/batch\n",
      "Epoch 19/20  Iteration 3337/3560 Training loss: 1.1842 13.9487 sec/batch\n",
      "Epoch 19/20  Iteration 3338/3560 Training loss: 1.1842 14.0237 sec/batch\n",
      "Epoch 19/20  Iteration 3339/3560 Training loss: 1.1842 13.9406 sec/batch\n",
      "Epoch 19/20  Iteration 3340/3560 Training loss: 1.1841 14.0923 sec/batch\n",
      "Epoch 19/20  Iteration 3341/3560 Training loss: 1.1842 14.0017 sec/batch\n",
      "Epoch 19/20  Iteration 3342/3560 Training loss: 1.1843 13.9932 sec/batch\n",
      "Epoch 19/20  Iteration 3343/3560 Training loss: 1.1844 14.0198 sec/batch\n",
      "Epoch 19/20  Iteration 3344/3560 Training loss: 1.1843 14.0346 sec/batch\n",
      "Epoch 19/20  Iteration 3345/3560 Training loss: 1.1847 13.9919 sec/batch\n",
      "Epoch 19/20  Iteration 3346/3560 Training loss: 1.1847 13.9102 sec/batch\n",
      "Epoch 19/20  Iteration 3347/3560 Training loss: 1.1846 13.9788 sec/batch\n",
      "Epoch 19/20  Iteration 3348/3560 Training loss: 1.1849 13.9749 sec/batch\n",
      "Epoch 19/20  Iteration 3349/3560 Training loss: 1.1848 13.9775 sec/batch\n",
      "Epoch 19/20  Iteration 3350/3560 Training loss: 1.1850 13.9989 sec/batch\n",
      "Epoch 19/20  Iteration 3351/3560 Training loss: 1.1850 14.0002 sec/batch\n",
      "Epoch 19/20  Iteration 3352/3560 Training loss: 1.1852 14.0114 sec/batch\n",
      "Epoch 19/20  Iteration 3353/3560 Training loss: 1.1854 13.9969 sec/batch\n",
      "Epoch 19/20  Iteration 3354/3560 Training loss: 1.1853 13.9670 sec/batch\n",
      "Epoch 19/20  Iteration 3355/3560 Training loss: 1.1850 14.9435 sec/batch\n",
      "Epoch 19/20  Iteration 3356/3560 Training loss: 1.1848 14.0057 sec/batch\n",
      "Epoch 19/20  Iteration 3357/3560 Training loss: 1.1849 14.1583 sec/batch\n",
      "Epoch 19/20  Iteration 3358/3560 Training loss: 1.1848 14.0793 sec/batch\n",
      "Epoch 19/20  Iteration 3359/3560 Training loss: 1.1848 14.0374 sec/batch\n",
      "Epoch 19/20  Iteration 3360/3560 Training loss: 1.1848 13.9702 sec/batch\n",
      "Epoch 19/20  Iteration 3361/3560 Training loss: 1.1849 14.0156 sec/batch\n",
      "Epoch 19/20  Iteration 3362/3560 Training loss: 1.1847 13.9673 sec/batch\n",
      "Epoch 19/20  Iteration 3363/3560 Training loss: 1.1845 14.0154 sec/batch\n",
      "Epoch 19/20  Iteration 3364/3560 Training loss: 1.1846 13.9449 sec/batch\n",
      "Epoch 19/20  Iteration 3365/3560 Training loss: 1.1848 14.0188 sec/batch\n",
      "Epoch 19/20  Iteration 3366/3560 Training loss: 1.1849 13.9873 sec/batch\n",
      "Epoch 19/20  Iteration 3367/3560 Training loss: 1.1848 13.9092 sec/batch\n",
      "Epoch 19/20  Iteration 3368/3560 Training loss: 1.1848 13.9744 sec/batch\n",
      "Epoch 19/20  Iteration 3369/3560 Training loss: 1.1848 13.9334 sec/batch\n",
      "Epoch 19/20  Iteration 3370/3560 Training loss: 1.1847 13.9674 sec/batch\n",
      "Epoch 19/20  Iteration 3371/3560 Training loss: 1.1849 14.0335 sec/batch\n",
      "Epoch 19/20  Iteration 3372/3560 Training loss: 1.1852 14.0425 sec/batch\n",
      "Epoch 19/20  Iteration 3373/3560 Training loss: 1.1854 14.0103 sec/batch\n",
      "Epoch 19/20  Iteration 3374/3560 Training loss: 1.1854 13.9228 sec/batch\n",
      "Epoch 19/20  Iteration 3375/3560 Training loss: 1.1854 14.0676 sec/batch\n",
      "Epoch 19/20  Iteration 3376/3560 Training loss: 1.1853 15.0660 sec/batch\n",
      "Epoch 19/20  Iteration 3377/3560 Training loss: 1.1854 13.9903 sec/batch\n",
      "Epoch 19/20  Iteration 3378/3560 Training loss: 1.1855 14.0464 sec/batch\n",
      "Epoch 19/20  Iteration 3379/3560 Training loss: 1.1855 14.0272 sec/batch\n",
      "Epoch 19/20  Iteration 3380/3560 Training loss: 1.1854 13.9721 sec/batch\n",
      "Epoch 19/20  Iteration 3381/3560 Training loss: 1.1854 14.0077 sec/batch\n",
      "Epoch 19/20  Iteration 3382/3560 Training loss: 1.1855 13.9699 sec/batch\n",
      "Epoch 20/20  Iteration 3383/3560 Training loss: 1.3043 14.0346 sec/batch\n",
      "Epoch 20/20  Iteration 3384/3560 Training loss: 1.2495 13.9497 sec/batch\n",
      "Epoch 20/20  Iteration 3385/3560 Training loss: 1.2306 13.9505 sec/batch\n",
      "Epoch 20/20  Iteration 3386/3560 Training loss: 1.2233 13.9873 sec/batch\n",
      "Epoch 20/20  Iteration 3387/3560 Training loss: 1.2114 13.9674 sec/batch\n",
      "Epoch 20/20  Iteration 3388/3560 Training loss: 1.1989 13.9534 sec/batch\n",
      "Epoch 20/20  Iteration 3389/3560 Training loss: 1.1980 14.0541 sec/batch\n",
      "Epoch 20/20  Iteration 3390/3560 Training loss: 1.1945 14.0041 sec/batch\n",
      "Epoch 20/20  Iteration 3391/3560 Training loss: 1.1927 13.9816 sec/batch\n",
      "Epoch 20/20  Iteration 3392/3560 Training loss: 1.1916 14.0198 sec/batch\n",
      "Epoch 20/20  Iteration 3393/3560 Training loss: 1.1896 13.9327 sec/batch\n",
      "Epoch 20/20  Iteration 3394/3560 Training loss: 1.1892 14.0095 sec/batch\n",
      "Epoch 20/20  Iteration 3395/3560 Training loss: 1.1893 14.0451 sec/batch\n",
      "Epoch 20/20  Iteration 3396/3560 Training loss: 1.1899 13.9754 sec/batch\n",
      "Epoch 20/20  Iteration 3397/3560 Training loss: 1.1883 14.0552 sec/batch\n",
      "Epoch 20/20  Iteration 3398/3560 Training loss: 1.1858 14.4126 sec/batch\n",
      "Epoch 20/20  Iteration 3399/3560 Training loss: 1.1862 13.9394 sec/batch\n",
      "Epoch 20/20  Iteration 3400/3560 Training loss: 1.1871 14.1367 sec/batch\n",
      "Validation loss: 1.12449 Saving checkpoint!\n",
      "Epoch 20/20  Iteration 3401/3560 Training loss: 1.1967 13.7131 sec/batch\n",
      "Epoch 20/20  Iteration 3402/3560 Training loss: 1.1978 14.1082 sec/batch\n",
      "Epoch 20/20  Iteration 3403/3560 Training loss: 1.1969 13.9788 sec/batch\n",
      "Epoch 20/20  Iteration 3404/3560 Training loss: 1.1966 14.0525 sec/batch\n",
      "Epoch 20/20  Iteration 3405/3560 Training loss: 1.1952 14.2369 sec/batch\n",
      "Epoch 20/20  Iteration 3406/3560 Training loss: 1.1951 13.9809 sec/batch\n",
      "Epoch 20/20  Iteration 3407/3560 Training loss: 1.1951 13.9852 sec/batch\n",
      "Epoch 20/20  Iteration 3408/3560 Training loss: 1.1931 13.9589 sec/batch\n",
      "Epoch 20/20  Iteration 3409/3560 Training loss: 1.1917 13.9739 sec/batch\n",
      "Epoch 20/20  Iteration 3410/3560 Training loss: 1.1918 14.1692 sec/batch\n",
      "Epoch 20/20  Iteration 3411/3560 Training loss: 1.1920 14.0554 sec/batch\n",
      "Epoch 20/20  Iteration 3412/3560 Training loss: 1.1921 14.8118 sec/batch\n",
      "Epoch 20/20  Iteration 3413/3560 Training loss: 1.1911 14.1547 sec/batch\n",
      "Epoch 20/20  Iteration 3414/3560 Training loss: 1.1901 13.9807 sec/batch\n",
      "Epoch 20/20  Iteration 3415/3560 Training loss: 1.1902 13.9839 sec/batch\n",
      "Epoch 20/20  Iteration 3416/3560 Training loss: 1.1901 13.8906 sec/batch\n",
      "Epoch 20/20  Iteration 3417/3560 Training loss: 1.1893 14.0066 sec/batch\n",
      "Epoch 20/20  Iteration 3418/3560 Training loss: 1.1888 13.9434 sec/batch\n",
      "Epoch 20/20  Iteration 3419/3560 Training loss: 1.1877 14.0316 sec/batch\n",
      "Epoch 20/20  Iteration 3420/3560 Training loss: 1.1866 14.0565 sec/batch\n",
      "Epoch 20/20  Iteration 3421/3560 Training loss: 1.1852 13.9997 sec/batch\n",
      "Epoch 20/20  Iteration 3422/3560 Training loss: 1.1848 14.0099 sec/batch\n",
      "Epoch 20/20  Iteration 3423/3560 Training loss: 1.1840 13.9390 sec/batch\n",
      "Epoch 20/20  Iteration 3424/3560 Training loss: 1.1847 14.0376 sec/batch\n",
      "Epoch 20/20  Iteration 3425/3560 Training loss: 1.1844 13.9728 sec/batch\n",
      "Epoch 20/20  Iteration 3426/3560 Training loss: 1.1835 14.0512 sec/batch\n",
      "Epoch 20/20  Iteration 3427/3560 Training loss: 1.1836 14.0939 sec/batch\n",
      "Epoch 20/20  Iteration 3428/3560 Training loss: 1.1829 13.9679 sec/batch\n",
      "Epoch 20/20  Iteration 3429/3560 Training loss: 1.1825 14.0567 sec/batch\n",
      "Epoch 20/20  Iteration 3430/3560 Training loss: 1.1821 13.9781 sec/batch\n",
      "Epoch 20/20  Iteration 3431/3560 Training loss: 1.1821 14.0038 sec/batch\n",
      "Epoch 20/20  Iteration 3432/3560 Training loss: 1.1821 14.0096 sec/batch\n",
      "Epoch 20/20  Iteration 3433/3560 Training loss: 1.1817 14.4263 sec/batch\n",
      "Epoch 20/20  Iteration 3434/3560 Training loss: 1.1823 13.9907 sec/batch\n",
      "Epoch 20/20  Iteration 3435/3560 Training loss: 1.1820 13.9521 sec/batch\n",
      "Epoch 20/20  Iteration 3436/3560 Training loss: 1.1822 14.4402 sec/batch\n",
      "Epoch 20/20  Iteration 3437/3560 Training loss: 1.1821 14.1259 sec/batch\n",
      "Epoch 20/20  Iteration 3438/3560 Training loss: 1.1821 14.0040 sec/batch\n",
      "Epoch 20/20  Iteration 3439/3560 Training loss: 1.1824 13.9882 sec/batch\n",
      "Epoch 20/20  Iteration 3440/3560 Training loss: 1.1822 13.9890 sec/batch\n",
      "Epoch 20/20  Iteration 3441/3560 Training loss: 1.1816 14.0069 sec/batch\n",
      "Epoch 20/20  Iteration 3442/3560 Training loss: 1.1821 14.0350 sec/batch\n",
      "Epoch 20/20  Iteration 3443/3560 Training loss: 1.1820 14.0247 sec/batch\n",
      "Epoch 20/20  Iteration 3444/3560 Training loss: 1.1828 14.0495 sec/batch\n",
      "Epoch 20/20  Iteration 3445/3560 Training loss: 1.1831 13.9757 sec/batch\n",
      "Epoch 20/20  Iteration 3446/3560 Training loss: 1.1832 14.0920 sec/batch\n",
      "Epoch 20/20  Iteration 3447/3560 Training loss: 1.1831 14.0954 sec/batch\n",
      "Epoch 20/20  Iteration 3448/3560 Training loss: 1.1832 13.9544 sec/batch\n",
      "Epoch 20/20  Iteration 3449/3560 Training loss: 1.1834 14.0644 sec/batch\n",
      "Epoch 20/20  Iteration 3450/3560 Training loss: 1.1831 14.0984 sec/batch\n",
      "Epoch 20/20  Iteration 3451/3560 Training loss: 1.1833 14.1393 sec/batch\n",
      "Epoch 20/20  Iteration 3452/3560 Training loss: 1.1832 14.1030 sec/batch\n",
      "Epoch 20/20  Iteration 3453/3560 Training loss: 1.1838 14.1397 sec/batch\n",
      "Epoch 20/20  Iteration 3454/3560 Training loss: 1.1841 15.3012 sec/batch\n",
      "Epoch 20/20  Iteration 3455/3560 Training loss: 1.1845 14.1816 sec/batch\n",
      "Epoch 20/20  Iteration 3456/3560 Training loss: 1.1841 14.0947 sec/batch\n",
      "Epoch 20/20  Iteration 3457/3560 Training loss: 1.1840 14.1027 sec/batch\n",
      "Epoch 20/20  Iteration 3458/3560 Training loss: 1.1841 14.0326 sec/batch\n",
      "Epoch 20/20  Iteration 3459/3560 Training loss: 1.1839 14.0125 sec/batch\n",
      "Epoch 20/20  Iteration 3460/3560 Training loss: 1.1838 14.2062 sec/batch\n",
      "Epoch 20/20  Iteration 3461/3560 Training loss: 1.1832 14.1785 sec/batch\n",
      "Epoch 20/20  Iteration 3462/3560 Training loss: 1.1831 13.9826 sec/batch\n",
      "Epoch 20/20  Iteration 3463/3560 Training loss: 1.1827 14.0742 sec/batch\n",
      "Epoch 20/20  Iteration 3464/3560 Training loss: 1.1826 14.1214 sec/batch\n",
      "Epoch 20/20  Iteration 3465/3560 Training loss: 1.1822 14.0524 sec/batch\n",
      "Epoch 20/20  Iteration 3466/3560 Training loss: 1.1822 14.0819 sec/batch\n",
      "Epoch 20/20  Iteration 3467/3560 Training loss: 1.1820 14.0526 sec/batch\n",
      "Epoch 20/20  Iteration 3468/3560 Training loss: 1.1820 14.1088 sec/batch\n",
      "Epoch 20/20  Iteration 3469/3560 Training loss: 1.1818 14.0965 sec/batch\n",
      "Epoch 20/20  Iteration 3470/3560 Training loss: 1.1816 14.0433 sec/batch\n",
      "Epoch 20/20  Iteration 3471/3560 Training loss: 1.1814 14.0962 sec/batch\n",
      "Epoch 20/20  Iteration 3472/3560 Training loss: 1.1813 14.0303 sec/batch\n",
      "Epoch 20/20  Iteration 3473/3560 Training loss: 1.1811 14.0842 sec/batch\n",
      "Epoch 20/20  Iteration 3474/3560 Training loss: 1.1810 13.9955 sec/batch\n",
      "Epoch 20/20  Iteration 3475/3560 Training loss: 1.1806 14.0145 sec/batch\n",
      "Epoch 20/20  Iteration 3476/3560 Training loss: 1.1802 14.4430 sec/batch\n",
      "Epoch 20/20  Iteration 3477/3560 Training loss: 1.1800 14.0445 sec/batch\n",
      "Epoch 20/20  Iteration 3478/3560 Training loss: 1.1801 14.1031 sec/batch\n",
      "Epoch 20/20  Iteration 3479/3560 Training loss: 1.1800 14.0427 sec/batch\n",
      "Epoch 20/20  Iteration 3480/3560 Training loss: 1.1797 14.0068 sec/batch\n",
      "Epoch 20/20  Iteration 3481/3560 Training loss: 1.1793 14.0165 sec/batch\n",
      "Epoch 20/20  Iteration 3482/3560 Training loss: 1.1790 14.0855 sec/batch\n",
      "Epoch 20/20  Iteration 3483/3560 Training loss: 1.1790 14.0702 sec/batch\n",
      "Epoch 20/20  Iteration 3484/3560 Training loss: 1.1788 14.0916 sec/batch\n",
      "Epoch 20/20  Iteration 3485/3560 Training loss: 1.1788 14.0679 sec/batch\n",
      "Epoch 20/20  Iteration 3486/3560 Training loss: 1.1786 14.1036 sec/batch\n",
      "Epoch 20/20  Iteration 3487/3560 Training loss: 1.1784 14.2047 sec/batch\n",
      "Epoch 20/20  Iteration 3488/3560 Training loss: 1.1783 14.0238 sec/batch\n",
      "Epoch 20/20  Iteration 3489/3560 Training loss: 1.1782 14.0805 sec/batch\n",
      "Epoch 20/20  Iteration 3490/3560 Training loss: 1.1782 14.0818 sec/batch\n",
      "Epoch 20/20  Iteration 3491/3560 Training loss: 1.1780 14.0522 sec/batch\n",
      "Epoch 20/20  Iteration 3492/3560 Training loss: 1.1781 14.0340 sec/batch\n",
      "Epoch 20/20  Iteration 3493/3560 Training loss: 1.1779 14.1523 sec/batch\n",
      "Epoch 20/20  Iteration 3494/3560 Training loss: 1.1779 14.0868 sec/batch\n",
      "Epoch 20/20  Iteration 3495/3560 Training loss: 1.1778 14.1580 sec/batch\n",
      "Epoch 20/20  Iteration 3496/3560 Training loss: 1.1776 14.0877 sec/batch\n",
      "Epoch 20/20  Iteration 3497/3560 Training loss: 1.1773 14.5865 sec/batch\n",
      "Epoch 20/20  Iteration 3498/3560 Training loss: 1.1772 14.0779 sec/batch\n",
      "Epoch 20/20  Iteration 3499/3560 Training loss: 1.1771 14.1181 sec/batch\n",
      "Epoch 20/20  Iteration 3500/3560 Training loss: 1.1772 14.1290 sec/batch\n",
      "Epoch 20/20  Iteration 3501/3560 Training loss: 1.1771 14.0611 sec/batch\n",
      "Epoch 20/20  Iteration 3502/3560 Training loss: 1.1771 14.0714 sec/batch\n",
      "Epoch 20/20  Iteration 3503/3560 Training loss: 1.1770 14.0299 sec/batch\n",
      "Epoch 20/20  Iteration 3504/3560 Training loss: 1.1767 14.2075 sec/batch\n",
      "Epoch 20/20  Iteration 3505/3560 Training loss: 1.1764 14.0736 sec/batch\n",
      "Epoch 20/20  Iteration 3506/3560 Training loss: 1.1763 14.0659 sec/batch\n",
      "Epoch 20/20  Iteration 3507/3560 Training loss: 1.1762 14.0372 sec/batch\n",
      "Epoch 20/20  Iteration 3508/3560 Training loss: 1.1759 13.9950 sec/batch\n",
      "Epoch 20/20  Iteration 3509/3560 Training loss: 1.1759 14.0132 sec/batch\n",
      "Epoch 20/20  Iteration 3510/3560 Training loss: 1.1758 14.1034 sec/batch\n",
      "Epoch 20/20  Iteration 3511/3560 Training loss: 1.1757 14.0742 sec/batch\n",
      "Epoch 20/20  Iteration 3512/3560 Training loss: 1.1753 14.0416 sec/batch\n",
      "Epoch 20/20  Iteration 3513/3560 Training loss: 1.1749 14.0680 sec/batch\n",
      "Epoch 20/20  Iteration 3514/3560 Training loss: 1.1747 14.0618 sec/batch\n",
      "Epoch 20/20  Iteration 3515/3560 Training loss: 1.1748 14.0936 sec/batch\n",
      "Epoch 20/20  Iteration 3516/3560 Training loss: 1.1747 14.0297 sec/batch\n",
      "Epoch 20/20  Iteration 3517/3560 Training loss: 1.1747 14.0620 sec/batch\n",
      "Epoch 20/20  Iteration 3518/3560 Training loss: 1.1747 14.5439 sec/batch\n",
      "Epoch 20/20  Iteration 3519/3560 Training loss: 1.1749 14.1612 sec/batch\n",
      "Epoch 20/20  Iteration 3520/3560 Training loss: 1.1750 14.0845 sec/batch\n",
      "Epoch 20/20  Iteration 3521/3560 Training loss: 1.1750 14.0828 sec/batch\n",
      "Epoch 20/20  Iteration 3522/3560 Training loss: 1.1750 14.0398 sec/batch\n",
      "Epoch 20/20  Iteration 3523/3560 Training loss: 1.1753 14.0611 sec/batch\n",
      "Epoch 20/20  Iteration 3524/3560 Training loss: 1.1754 14.1073 sec/batch\n",
      "Epoch 20/20  Iteration 3525/3560 Training loss: 1.1754 14.0730 sec/batch\n",
      "Epoch 20/20  Iteration 3526/3560 Training loss: 1.1756 14.0499 sec/batch\n",
      "Epoch 20/20  Iteration 3527/3560 Training loss: 1.1754 14.0514 sec/batch\n",
      "Epoch 20/20  Iteration 3528/3560 Training loss: 1.1756 14.1106 sec/batch\n",
      "Epoch 20/20  Iteration 3529/3560 Training loss: 1.1756 14.1462 sec/batch\n",
      "Epoch 20/20  Iteration 3530/3560 Training loss: 1.1759 13.9994 sec/batch\n",
      "Epoch 20/20  Iteration 3531/3560 Training loss: 1.1760 14.0113 sec/batch\n",
      "Epoch 20/20  Iteration 3532/3560 Training loss: 1.1759 14.1618 sec/batch\n",
      "Epoch 20/20  Iteration 3533/3560 Training loss: 1.1757 14.0426 sec/batch\n",
      "Epoch 20/20  Iteration 3534/3560 Training loss: 1.1755 14.0492 sec/batch\n",
      "Epoch 20/20  Iteration 3535/3560 Training loss: 1.1756 14.0482 sec/batch\n",
      "Epoch 20/20  Iteration 3536/3560 Training loss: 1.1755 14.1112 sec/batch\n",
      "Epoch 20/20  Iteration 3537/3560 Training loss: 1.1755 14.1870 sec/batch\n",
      "Epoch 20/20  Iteration 3538/3560 Training loss: 1.1754 14.1548 sec/batch\n",
      "Epoch 20/20  Iteration 3539/3560 Training loss: 1.1754 15.0212 sec/batch\n",
      "Epoch 20/20  Iteration 3540/3560 Training loss: 1.1753 14.4699 sec/batch\n",
      "Epoch 20/20  Iteration 3541/3560 Training loss: 1.1751 14.0625 sec/batch\n",
      "Epoch 20/20  Iteration 3542/3560 Training loss: 1.1752 14.0360 sec/batch\n",
      "Epoch 20/20  Iteration 3543/3560 Training loss: 1.1754 14.0601 sec/batch\n",
      "Epoch 20/20  Iteration 3544/3560 Training loss: 1.1754 14.1171 sec/batch\n",
      "Epoch 20/20  Iteration 3545/3560 Training loss: 1.1754 14.0382 sec/batch\n",
      "Epoch 20/20  Iteration 3546/3560 Training loss: 1.1754 14.1181 sec/batch\n",
      "Epoch 20/20  Iteration 3547/3560 Training loss: 1.1753 14.1157 sec/batch\n",
      "Epoch 20/20  Iteration 3548/3560 Training loss: 1.1753 13.9859 sec/batch\n",
      "Epoch 20/20  Iteration 3549/3560 Training loss: 1.1755 14.1384 sec/batch\n",
      "Epoch 20/20  Iteration 3550/3560 Training loss: 1.1758 14.0701 sec/batch\n",
      "Epoch 20/20  Iteration 3551/3560 Training loss: 1.1759 14.0600 sec/batch\n",
      "Epoch 20/20  Iteration 3552/3560 Training loss: 1.1759 14.0750 sec/batch\n",
      "Epoch 20/20  Iteration 3553/3560 Training loss: 1.1758 14.0232 sec/batch\n",
      "Epoch 20/20  Iteration 3554/3560 Training loss: 1.1758 13.9658 sec/batch\n",
      "Epoch 20/20  Iteration 3555/3560 Training loss: 1.1759 14.2142 sec/batch\n",
      "Epoch 20/20  Iteration 3556/3560 Training loss: 1.1759 14.0329 sec/batch\n",
      "Epoch 20/20  Iteration 3557/3560 Training loss: 1.1760 14.0655 sec/batch\n",
      "Epoch 20/20  Iteration 3558/3560 Training loss: 1.1758 14.0275 sec/batch\n",
      "Epoch 20/20  Iteration 3559/3560 Training loss: 1.1758 14.0787 sec/batch\n",
      "Epoch 20/20  Iteration 3560/3560 Training loss: 1.1760 14.1029 sec/batch\n",
      "Validation loss: 1.1161 Saving checkpoint!\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "# Save every N iterations\n",
    "save_every_n = 200\n",
    "train_x, train_y, val_x, val_y = split_data(chars, batch_size, num_steps)\n",
    "\n",
    "model = build_rnn(len(vocab), \n",
    "                  batch_size=batch_size,\n",
    "                  num_steps=num_steps,\n",
    "                  learning_rate=learning_rate,\n",
    "                  lstm_size=lstm_size,\n",
    "                  num_layers=num_layers)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    \n",
    "    n_batches = int(train_x.shape[1]/num_steps)\n",
    "    iterations = n_batches * epochs\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for b, (x, y) in enumerate(get_batch([train_x, train_y], num_steps), 1):\n",
    "            iteration = e*n_batches + b\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.cost, model.final_state, model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            loss += batch_loss\n",
    "            end = time.time()\n",
    "            print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                  'Iteration {}/{}'.format(iteration, iterations),\n",
    "                  'Training loss: {:.4f}'.format(loss/b),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            \n",
    "            if (iteration%save_every_n == 0) or (iteration == iterations):\n",
    "                # Check performance, notice dropout has been set to 1\n",
    "                val_loss = []\n",
    "                new_state = sess.run(model.initial_state)\n",
    "                for x, y in get_batch([val_x, val_y], num_steps):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.targets: y,\n",
    "                            model.keep_prob: 1.,\n",
    "                            model.initial_state: new_state}\n",
    "                    batch_loss, new_state = sess.run([model.cost, model.final_state], feed_dict=feed)\n",
    "                    val_loss.append(batch_loss)\n",
    "\n",
    "                print('Validation loss:', np.mean(val_loss),\n",
    "                      'Saving checkpoint!')\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}_v{:.3f}.ckpt\".format(iteration, lstm_size, np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Saved checkpoints\n",
    "\n",
    "Read up on saving and loading checkpoints here: https://www.tensorflow.org/programmers_guide/variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/i3560_l512_v1.116.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i200_l512_v2.374.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i400_l512_v1.936.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i600_l512_v1.711.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i800_l512_v1.556.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1000_l512_v1.448.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1200_l512_v1.362.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1400_l512_v1.306.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1600_l512_v1.266.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1800_l512_v1.236.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2000_l512_v1.208.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2200_l512_v1.190.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2400_l512_v1.175.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2600_l512_v1.158.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2800_l512_v1.150.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3000_l512_v1.139.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3200_l512_v1.129.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3400_l512_v1.124.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3560_l512_v1.116.ckpt\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = build_rnn(vocab_size, lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here, pass in the path to a checkpoint and sample from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/____.ckpt\"\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [tensorflow]",
   "language": "python",
   "name": "Python [tensorflow]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
